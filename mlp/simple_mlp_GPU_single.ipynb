{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP, Multi-processing example. Distributed Sampler\n",
    "# https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLazy(nn.Module):\n",
    "\n",
    "    def __init__(self, nx, hidden_layer_dims, ny, gpu):\n",
    "        super(MLPLazy, self).__init__()\n",
    "        self.hidden_layer_dims = hidden_layer_dims\n",
    "        \n",
    "        linear_layers = []\n",
    "        last_dim = nx\n",
    "        for next_dim in hidden_layer_dims:\n",
    "            linear_layer = nn.Linear(last_dim, next_dim).cuda(gpu)\n",
    "            linear_layers.append(linear_layer)\n",
    "            last_dim = next_dim\n",
    "        # should push to ModuleList so that params stay on cuda\n",
    "        self.linear_layers = nn.ModuleList(linear_layers)\n",
    "        \n",
    "        self.scorer = nn.Linear(last_dim, ny)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        last_X = X\n",
    "        for i, linear_layer in enumerate(self.linear_layers):\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = linear_layer(last_X)\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = torch.relu(last_X)\n",
    "        # shape (m, ny)\n",
    "        z = self.scorer(last_X)\n",
    "        # shape (m, ny)\n",
    "        a = torch.softmax(z, dim=1)\n",
    "        return z, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model, train_loader, valid_loader, loss_criterion, optimizer, args, gpu):\n",
    "    '''\n",
    "    Train model and report losses on train and dev sets per epoch\n",
    "    '''\n",
    "    \n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'valid_losses': [],        \n",
    "        'valid_accuracy': [],\n",
    "    }\n",
    "    \n",
    "    for epoch_i in range(args.epochs):\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        sum_batch_losses = torch.tensor([0.], dtype=torch.float, device=gpu)\n",
    "        for batch_i, batch_data in enumerate(train_loader):\n",
    "            batch_X = batch_data['X'].cuda(gpu, non_blocking=True)\n",
    "            batch_y = batch_data['y'].cuda(gpu, non_blocking=True)\n",
    "            logits, activations = model(batch_X)\n",
    "            loss = loss_criterion(logits, batch_y) #return scalar\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_batch_losses += loss # don't call item yet to use pinned memory\n",
    "        num_batches = batch_i + 1.\n",
    "        # append tensor scalar\n",
    "        history['train_losses'].append(sum_batch_losses/num_batches)\n",
    "\n",
    "        # validate\n",
    "        val_sum_batch_losses, val_sum_batch_accuracies, val_num_batches = run_pred(model, valid_loader, loss_criterion, gpu)\n",
    "        history['valid_losses'].append(val_sum_batch_losses / val_num_batches)\n",
    "        history['valid_accuracy'].append(val_sum_batch_accuracies / val_num_batches)\n",
    "    \n",
    "    # Use pinned memory in dataloader for faster data transfer\n",
    "    # do not add synchronization point until training loop has ended\n",
    "    # https://discuss.pytorch.org/t/should-we-set-non-blocking-to-true/38234/4\n",
    "    # https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/\n",
    "    itemize = lambda x: [tensor_val.item() for tensor_val in x]\n",
    "    history['train_losses'] = itemize(history['train_losses'])\n",
    "    history['valid_losses'] = itemize(history['valid_losses'])\n",
    "    history['valid_accuracy'] = itemize(history['valid_accuracy'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_pred(model, test_loader, loss_criterion, gpu):\n",
    "    '''Propogate forward on dev or test set, report loss and accuracy.'''\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    sum_batch_losses = torch.tensor([0.], dtype=torch.float, device=gpu)\n",
    "    sum_batch_accuracies = torch.tensor([0.], dtype=torch.float, device=gpu)\n",
    "    for batch_i, batch_data in enumerate(test_loader):\n",
    "        batch_X = batch_data['X'].cuda(gpu, non_blocking=True)\n",
    "        batch_y = batch_data['y'].cuda(gpu, non_blocking=True)\n",
    "        logits, activations = model(batch_X)\n",
    "        loss = loss_criterion(logits, batch_y)\n",
    "        sum_batch_losses += loss\n",
    "        _, max_index = torch.max(logits, dim=1)\n",
    "        accuracy = torch.mean(max_index.eq(batch_y).type(torch.FloatTensor))\n",
    "        sum_batch_accuracies += accuracy\n",
    "    num_batches_computed = batch_i + 1.\n",
    "    return sum_batch_losses, sum_batch_accuracies, num_batches_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Toy dataset construction.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Path to the directory with data files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # shape (m, nx)\n",
    "        self.X = np.load(os.path.join(data_dir, 'features.npy'))\n",
    "        # shape (m, ny=1)\n",
    "        self.y = np.load(os.path.join(data_dir, 'labels.npy'))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        else:\n",
    "            X = torch.from_numpy(self.X[idx, :]).type(torch.FloatTensor)\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "            sample = {'X': X, 'y': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def main_train(gpu, args):\n",
    "    torch.manual_seed(args.seed)\n",
    "    \n",
    "    ################################################################\n",
    "    # load datasets\n",
    "    training_set = ToyDataset(data_dir=os.path.join(args.data_dir, args.dataset_dir, 'train'))\n",
    "    training_generator = torch.utils.data.DataLoader(dataset=training_set, \n",
    "                                                     batch_size=args.batch_size, \n",
    "                                                     shuffle=True, \n",
    "                                                     num_workers=0, \n",
    "                                                     pin_memory=True)\n",
    "    \n",
    "    validation_set = ToyDataset(data_dir=os.path.join(args.data_dir, args.dataset_dir, 'valid'))\n",
    "    validation_generator = torch.utils.data.DataLoader(dataset=validation_set, \n",
    "                                                       batch_size=args.batch_size, \n",
    "                                                       shuffle=True, \n",
    "                                                       num_workers=0, \n",
    "                                                       pin_memory=True)\n",
    "    \n",
    "    nx = training_set.X.shape[1]\n",
    "    ny = max(training_set.y) + 1\n",
    "    \n",
    "    print('Train set X shape:', training_set.X.shape)\n",
    "    print('Train set y shape:', training_set.y.shape)\n",
    "    print('Valid set X shape:', validation_set.X.shape)\n",
    "    print('Valid set y shape:', validation_set.y.shape)\n",
    "    ################################################################\n",
    "    \n",
    "    model = MLPLazy(nx, args.hidden_layer_dims, ny, gpu)\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model.to(device=gpu)\n",
    "    ################################################################\n",
    "    batch_size = args.batch_size\n",
    "    loss_criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    start = datetime.now()\n",
    "    history = run_train(model, training_generator, validation_generator, loss_criterion, optimizer, args, gpu)\n",
    "    \n",
    "    if gpu == 0:\n",
    "        print(\"Training complete in: \" + str(datetime.now() - start))\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "args.data_dir = '/datadrive'\n",
    "args.dataset_dir = 'toy_mlp_1'\n",
    "args.seed = 123\n",
    "args.batch_size = 1000\n",
    "# https://stackoverflow.com/questions/15753701/how-can-i-pass-a-list-as-a-command-line-argument-with-argparse\n",
    "args.hidden_layer_dims = [10, 10]\n",
    "args.lr = 0.01\n",
    "args.epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 27 04:29:49 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00006C84:00:00.0 Off |                    0 |\n",
      "| N/A   69C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 0000AA21:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuda0 = torch.device('cuda:0')\n",
    "cuda1 = torch.device('cuda:1') \n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set X shape: (9000, 10)\n",
      "Train set y shape: (9000,)\n",
      "Valid set X shape: (500, 10)\n",
      "Valid set y shape: (500,)\n",
      "Training complete in: 0:00:03.985048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_losses': [1.6582450866699219,\n",
       "  1.6510987281799316,\n",
       "  1.644874095916748,\n",
       "  1.6394009590148926,\n",
       "  1.634544849395752,\n",
       "  1.6302032470703125,\n",
       "  1.6262813806533813,\n",
       "  1.6227096319198608,\n",
       "  1.6194416284561157,\n",
       "  1.6164271831512451,\n",
       "  1.6136350631713867,\n",
       "  1.6110327243804932,\n",
       "  1.608588457107544,\n",
       "  1.6062806844711304,\n",
       "  1.6041074991226196,\n",
       "  1.602038025856018,\n",
       "  1.600063681602478,\n",
       "  1.5981789827346802,\n",
       "  1.596370816230774,\n",
       "  1.5946385860443115],\n",
       " 'valid_losses': [1.6564011573791504,\n",
       "  1.6499884128570557,\n",
       "  1.6443711519241333,\n",
       "  1.6393944025039673,\n",
       "  1.6349437236785889,\n",
       "  1.630926251411438,\n",
       "  1.6272802352905273,\n",
       "  1.6239466667175293,\n",
       "  1.6208895444869995,\n",
       "  1.6180613040924072,\n",
       "  1.6154333353042603,\n",
       "  1.6129724979400635,\n",
       "  1.610644817352295,\n",
       "  1.6084356307983398,\n",
       "  1.6063393354415894,\n",
       "  1.6043424606323242,\n",
       "  1.6024408340454102,\n",
       "  1.6006133556365967,\n",
       "  1.5988612174987793,\n",
       "  1.597172498703003],\n",
       " 'valid_accuracy': [0.1899999976158142,\n",
       "  0.1860000044107437,\n",
       "  0.18199999630451202,\n",
       "  0.18000000715255737,\n",
       "  0.17000000178813934,\n",
       "  0.17000000178813934,\n",
       "  0.17000000178813934,\n",
       "  0.1720000058412552,\n",
       "  0.1720000058412552,\n",
       "  0.17599999904632568,\n",
       "  0.18199999630451202,\n",
       "  0.18199999630451202,\n",
       "  0.18400000035762787,\n",
       "  0.1940000057220459,\n",
       "  0.19599999487400055,\n",
       "  0.20200000703334808,\n",
       "  0.20200000703334808,\n",
       "  0.20000000298023224,\n",
       "  0.1940000057220459,\n",
       "  0.19599999487400055]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_train(0, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 27 04:29:55 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00006C84:00:00.0 Off |                    0 |\n",
      "| N/A   69C    P0    62W / 149W |    281MiB / 11441MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 0000AA21:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P8    33W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    129700      C   ...nvs/playground/bin/python      278MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |  351232 B  |   87133 KB |   87133 KB |\n",
      "|       from large pool |       0 B  |       0 B  |       0 KB |       0 KB |\n",
      "|       from small pool |       0 B  |  351232 B  |   87133 KB |   87133 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |  351232 B  |   87133 KB |   87133 KB |\n",
      "|       from large pool |       0 B  |       0 B  |       0 KB |       0 KB |\n",
      "|       from small pool |       0 B  |  351232 B  |   87133 KB |   87133 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |    2047 KB |   89180 KB |   89180 KB |\n",
      "|       from large pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |       0 B  |    2047 KB |   89180 KB |   89180 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |      88    |    4726    |    4726    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |      88    |    4726    |    4726    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |      88    |    4726    |    4726    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |      88    |    4726    |    4726    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |      24    |    2218    |    2218    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |      24    |    2218    |    2218    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuda0 = torch.device('cuda:0')\n",
    "cuda1 = torch.device('cuda:1') \n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(cuda1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
