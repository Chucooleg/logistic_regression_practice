{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining TODOs for this notebook\n",
    "# custom Softmax and custom Cross Entropy loss as torch autograd functions\n",
    "# run the custom version of this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "args.data_dir = '/datadrive'\n",
    "args.seed = 123\n",
    "args.lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Off the shelf MLP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLazy(nn.Module):\n",
    "\n",
    "    def __init__(self, nx, hidden_layer_dims, ny):\n",
    "        super(MLPLazy, self).__init__()\n",
    "        self.hidden_layer_dims = hidden_layer_dims\n",
    "        \n",
    "        # note that this is only possible with CPU training\n",
    "        # push filled python list to nn.ModuleList if using GPU\n",
    "        self.linear_layers = [] \n",
    "        last_dim = nx\n",
    "        for next_dim in hidden_layer_dims:\n",
    "            linear_layer = nn.Linear(last_dim, next_dim)\n",
    "            self.linear_layers.append(linear_layer)\n",
    "            last_dim = next_dim\n",
    "        self.scorer = nn.Linear(last_dim, ny)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        last_X = X\n",
    "        for i, linear_layer in enumerate(self.linear_layers):\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = linear_layer(last_X)\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = torch.relu(last_X)\n",
    "        # shape (m, ny)\n",
    "        z = self.scorer(last_X)\n",
    "        # shape (m, ny)\n",
    "        a = torch.softmax(z, dim=1)\n",
    "        return z, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Main Train & Pred Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, loss_criterion, optimizer, args, epochs=20):\n",
    "    '''\n",
    "    Train model and report losses on train and dev sets per epoch\n",
    "    '''\n",
    "    \n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'valid_losses': [],        \n",
    "        'valid_accuracy': [],\n",
    "    }\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for batch_i, batch_data in enumerate(train_loader):\n",
    "            logits, activations = model(batch_data['X'])\n",
    "            loss = loss_criterion(logits, batch_data['y'])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        history['train_losses'].append(sum(batch_losses) / len(batch_losses))\n",
    "\n",
    "        # validate\n",
    "        batch_val_losses, batch_val_accuracies = pred(model, valid_loader, loss_criterion)\n",
    "        history['valid_losses'].append(sum(batch_val_losses) / len(batch_val_losses))\n",
    "        history['valid_accuracy'].append(sum(batch_val_accuracies) / len(batch_val_accuracies))\n",
    "        \n",
    "    return history\n",
    "\n",
    "# def write_param_history(model, history):\n",
    "#     weights = model.scorer.weight.clone().detach().numpy()\n",
    "#     bias = model.scorer.bias.data.clone().detach().numpy()\n",
    "#     history['weights'].append(weights)\n",
    "#     history['bias'].append(bias)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pred(model, test_loader, loss_criterion):\n",
    "    '''Propogate forward on dev or test set, report loss and accuracy.'''\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "    for batch_i, batch_data in enumerate(test_loader):\n",
    "        logits, activations = model(batch_data['X'])\n",
    "        loss = loss_criterion(logits, batch_data['y'])\n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        _, max_index = torch.max(logits, dim=1)\n",
    "        accuracy = torch.mean(max_index.eq(batch_data['y']).type(torch.FloatTensor))\n",
    "        batch_accuracies.append(accuracy.item())\n",
    "    \n",
    "    return batch_losses, batch_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dataloader\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "# Pytorch Data Collate (Further reading, not implemented here)\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Toy dataset construction.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Path to the directory with data files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # shape (m, nx)\n",
    "        self.X = np.load(os.path.join(data_dir, 'features.npy'))\n",
    "        # shape (m, ny=1)\n",
    "        self.y = np.load(os.path.join(data_dir, 'labels.npy'))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        else:\n",
    "            X = torch.from_numpy(self.X[idx, :]).type(torch.FloatTensor)\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "#             y = torch.from_numpy(self.y[idx, :]).type(torch.FloatTensor)\n",
    "            sample = {'X': X, 'y': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give permission to access /datadrive\n",
    "!sudo chmod -R 777 /datadrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training indices [2656  445 9505  332 4168 2364 6097    7 7752 4453]\n",
      "X shape (10000, 10)\n",
      "y shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# construct and save toydataset\n",
    "\n",
    "m_train, m_valid, m_test = 9000, 500, 500\n",
    "m_total = m_train + m_valid + m_test\n",
    "\n",
    "X, y = make_classification(n_samples=m_total, n_features=10, n_informative=10, n_redundant=0, n_repeated=0, n_classes=5, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=args.seed)\n",
    "# y = np.expand_dims(y, -1)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "permutation = np.random.permutation(m_total)\n",
    "print('First 10 training indices', permutation[:10])\n",
    "print('X shape', X.shape)\n",
    "print('y shape', y.shape)\n",
    "\n",
    "train_indices = permutation[0:m_train]\n",
    "valid_indices = permutation[m_train:m_train+m_valid]\n",
    "test_indices = permutation[m_train+m_valid:]\n",
    "\n",
    "dataset_dir = 'toy_mlp_1'\n",
    "os.makedirs(os.path.join(args.data_dir, dataset_dir, 'train'), mode = 0o777, exist_ok = True) \n",
    "os.makedirs(os.path.join(args.data_dir, dataset_dir, 'valid'), mode = 0o777, exist_ok = True) \n",
    "os.makedirs(os.path.join(args.data_dir, dataset_dir, 'test'), mode = 0o777, exist_ok = True) \n",
    "\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'train', 'features.npy'), X[train_indices])\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'train', 'labels.npy'), y[train_indices])\n",
    "\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'valid', 'features.npy'), X[valid_indices])\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'valid', 'labels.npy'), y[valid_indices])\n",
    "\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'test', 'features.npy'), X[test_indices])\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'test', 'labels.npy'), y[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check and set CUDA devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active CUDA Device: GPU 0\n",
      "Available devices  2\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 23:18:08 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00006C84:00:00.0 Off |                    0 |\n",
      "| N/A   45C    P8    26W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 0000AA21:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P8    33W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda devices\n",
    "cuda0 = torch.device('cuda:0') # same as cuda0 = torch.device('cuda:0') or cuda = torch.device('cuda')\n",
    "cuda1 = torch.device('cuda:1') # same as cuda1 = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and compare results on toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set X shape: (9000, 10)\n",
      "Train set y shape: (9000,)\n",
      "Valid set X shape: (500, 10)\n",
      "Valid set y shape: (500,)\n",
      "Test set X shape: (500, 10)\n",
      "Test set y shape: (500,)\n",
      "Num classes: 5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "dataset_dir = 'toy_mlp_1'\n",
    "\n",
    "training_set = ToyDataset(data_dir=os.path.join(args.data_dir, dataset_dir, 'train'))\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_set = ToyDataset(data_dir=os.path.join(args.data_dir, dataset_dir, 'valid'))\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, batch_size=batch_size)\n",
    "\n",
    "test_set = ToyDataset(data_dir=os.path.join(args.data_dir, dataset_dir, 'test'))\n",
    "test_generator = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "print('Train set X shape:', training_set.X.shape)\n",
    "print('Train set y shape:', training_set.y.shape)\n",
    "\n",
    "print('Valid set X shape:', validation_set.X.shape)\n",
    "print('Valid set y shape:', validation_set.y.shape)\n",
    "\n",
    "print('Test set X shape:', test_set.X.shape)\n",
    "print('Test set y shape:', test_set.y.shape)\n",
    "\n",
    "m = training_set.X.shape[0]\n",
    "nx = training_set.X.shape[1]\n",
    "ny = max(training_set.y) + 1\n",
    "\n",
    "print('Num classes:', ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# set off-the-shelf model, loss function and optimizer\n",
    "model = MLPLazy(nx, [10, 10], ny)\n",
    "# loss_criterion_lazy = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "loss_criterion_lazy = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer_lazy = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "history_off_the_shelf = train(model, training_generator, validation_generator, loss_criterion_lazy, optimizer_lazy, args, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classes=2 , layers=[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7078015372157097,\n",
       " 0.559940741956234,\n",
       " 0.5136001236736775,\n",
       " 0.49279844135046,\n",
       " 0.4808144173026085,\n",
       " 0.47350259937345984,\n",
       " 0.46876141712069513,\n",
       " 0.4649814983457327,\n",
       " 0.4627411755174398,\n",
       " 0.4607449018955231]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(history_off_the_shelf['valid_losses'][::10])\n",
    "print(history_off_the_shelf['train_losses'][::10])\n",
    "print(history_off_the_shelf['valid_accuracy'][::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7399412658479478,\n",
       " 0.5025229536824756,\n",
       " 0.42427631384796566,\n",
       " 0.3825305799643199,\n",
       " 0.35708342492580414,\n",
       " 0.340332708424992,\n",
       " 0.32642075336641735,\n",
       " 0.3165907309287124,\n",
       " 0.30814487404293484,\n",
       " 0.30177313917213017]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_off_the_shelf['train_losses'][::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49200001135468485,\n",
       " 0.7340000128746033,\n",
       " 0.7660000109672547,\n",
       " 0.7640000106394291,\n",
       " 0.7780000104010105,\n",
       " 0.7820000104606152,\n",
       " 0.7840000106394291,\n",
       " 0.7860000105202198,\n",
       " 0.7920000103116035,\n",
       " 0.7920000103116035]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_off_the_shelf['valid_accuracy'][::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classes=5 , layers=[10], 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.594672179222107, 1.4679929852485656, 1.444815957546234, 1.4348259568214417, 1.429522454738617, 1.4265439987182618, 1.42513347864151, 1.42394357919693, 1.4238737106323243, 1.4231544494628907]\n",
      "\n",
      "[1.6281403058105044, 1.4527535690201654, 1.4242660714520348, 1.4123969687355888, 1.4064121895366244, 1.403214869234297, 1.4013509809970857, 1.4003351860576205, 1.399590598874622, 1.3992673403686948]\n",
      "\n",
      "[0.25399999916553495, 0.3640000030398369, 0.364000004529953, 0.37000000178813935, 0.3699999988079071, 0.3739999979734421, 0.3680000066757202, 0.3700000047683716, 0.3659999996423721, 0.36200000047683717]\n"
     ]
    }
   ],
   "source": [
    "print(history_off_the_shelf['valid_losses'][::10])\n",
    "print()\n",
    "print(history_off_the_shelf['train_losses'][::10])\n",
    "print()\n",
    "print(history_off_the_shelf['valid_accuracy'][::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classes=5 , layers=[10], 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.594672179222107, 1.4226765275001525, 1.421881663799286, 1.4215632200241088, 1.4215431332588195, 1.421615970134735, 1.4213977694511413, 1.4214471697807312, 1.4216774225234985, 1.4215827703475952]\n",
      "\n",
      "[1.6281403058105044, 1.3990414460500082, 1.3984545992480384, 1.3984812352392408, 1.398521096838845, 1.398539662361145, 1.3984148303667705, 1.3984918137391409, 1.398470558722814, 1.3985287633207109]\n",
      "\n",
      "[0.25399999916553495, 0.3640000015497208, 0.3659999996423721, 0.3679999977350235, 0.3659999966621399, 0.3679999977350235, 0.36999999582767484, 0.3679999977350235, 0.3679999977350235, 0.3679999977350235]\n"
     ]
    }
   ],
   "source": [
    "print(history_off_the_shelf['valid_losses'][::100])\n",
    "print()\n",
    "print(history_off_the_shelf['train_losses'][::100])\n",
    "print()\n",
    "print(history_off_the_shelf['valid_accuracy'][::100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classes=5 , layers=[10, 10], 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6235280156135559, 1.4976302146911622, 1.4778198719024658, 1.4685138702392577, 1.4632822513580321, 1.4599974513053895, 1.4577441096305848, 1.4561721086502075, 1.455070471763611, 1.4541317343711853]\n",
      "\n",
      "[1.6377845022413466, 1.4939362631903754, 1.4735652685165406, 1.4636919293138715, 1.4579832388295069, 1.4542949391735924, 1.4518313255574968, 1.450035469399558, 1.4487053102917142, 1.4477064046594832]\n",
      "\n",
      "[0.15600000098347663, 0.3600000023841858, 0.36200000047683717, 0.36600000262260435, 0.3659999996423721, 0.3740000009536743, 0.37400000393390653, 0.3680000066757202, 0.37200000286102297, 0.37200000286102297]\n"
     ]
    }
   ],
   "source": [
    "print(history_off_the_shelf['valid_losses'][::100])\n",
    "print()\n",
    "print(history_off_the_shelf['train_losses'][::100])\n",
    "print()\n",
    "print(history_off_the_shelf['valid_accuracy'][::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
