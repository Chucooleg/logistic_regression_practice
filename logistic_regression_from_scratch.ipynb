{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "args.data_dir = '/root/data/'\n",
    "args.seed = 123\n",
    "args.lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Off the shelf implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionLazy(nn.Module):\n",
    "\n",
    "    def __init__(self, nx):\n",
    "        super(LogisticRegressionLazy, self).__init__()\n",
    "        self.scorer = nn.Linear(nx, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        # shape (m, 1)\n",
    "        z = self.scorer(X)\n",
    "        # shape (m, 1)\n",
    "        a = torch.sigmoid(z)\n",
    "        return z, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. With custom linear module and sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending pytorch (demo of custom function with custom forward backward, custom LinearFunction)\n",
    "# https://pytorch.org/docs/master/notes/extending.html\n",
    "# https://github.com/pytorch/pytorch/blob/c9bb990707d4bfe524f3f1c4a77ff85fed1cd2a2/torch/csrc/api/include/torch/nn/functional/loss.h\n",
    "\n",
    "# pytorch Autograd function (RELU example)\n",
    "# https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "\n",
    "# discussion custom threshold forward and backward\n",
    "# https://discuss.pytorch.org/t/how-to-call-only-backward-path-of-pytorch-function/22839/2\n",
    "\n",
    "# Define custom autograd.Function and put the function in nn.Module\n",
    "# https://discuss.pytorch.org/t/how-to-call-the-backward-function-of-a-custom-module/7853\n",
    "\n",
    "class LogisticRegressionCustom(nn.Module):\n",
    "    '''Linear and sigmoid with custom backward'''\n",
    "    def __init__(self, nx, init_weight, init_bias):\n",
    "        super(LogisticRegressionCustom, self).__init__()\n",
    "        self.scorer = CustomLinearLayer(nx, init_weight, init_bias)\n",
    "        self.sigmoid = CustomSigmoidFunction.apply\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        # shape(m, ny=1)\n",
    "        z = self.scorer(X)\n",
    "        # shape(m, ny=1)\n",
    "        a = self.sigmoid(z)\n",
    "        return z, a  \n",
    "\n",
    "class CustomSigmoidFunction(torch.autograd.Function):\n",
    "    '''\n",
    "    doesn't get backprop through because loss function takes in logit directly\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        '''\n",
    "        inp: shape(m, ny)\n",
    "        '''\n",
    "        ctx.save_for_backward(inp)\n",
    "        return 1 / (1 + torch.exp(-inp))\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, dA):\n",
    "        '''\n",
    "        Demonstration purpose. Not used in overall backprop since our loss function computes with logits.\n",
    "        dA: shape(m, ny)\n",
    "        '''\n",
    "        # retrieve cache\n",
    "        inp, = ctx.saved_tensors\n",
    "        grad_inp = None\n",
    "        \n",
    "        A = 1.0 / (1.0 + torch.exp(-inp))\n",
    "        # shape(m, ny)\n",
    "        grad_inp = A * (1 - A) * dA\n",
    "        \n",
    "        return grad_inp\n",
    "\n",
    "class CustomLinearFunction(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, inp, wt, b):\n",
    "        '''\n",
    "        inp: shape(nx, m)\n",
    "        wt: shape(ny=1, nx)\n",
    "        b: shape(ny=1, 1)\n",
    "        '''\n",
    "        ctx.save_for_backward(inp, wt, b)\n",
    "        # (ny, m) = (ny, nx)(nx, m) + (ny, 1)t\n",
    "        z = wt.mm(inp) + b\n",
    "        assert z.shape == (1, inp.shape[1])\n",
    "        # (ny, m)\n",
    "        return z\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, dZ):\n",
    "        '''\n",
    "        dZ: shape(ny, m)\n",
    "        '''\n",
    "        \n",
    "        # retrieve cache\n",
    "        inp, wt, b = ctx.saved_tensors\n",
    "        m = inp.shape[1]\n",
    "        grad_inp, grad_wt, grad_b = None, None, None\n",
    "        \n",
    "        # Z = W dot X.T + b \n",
    "        # shape(nx, m)\n",
    "        grad_inp = wt.t().mm(dZ)\n",
    "        # shape(ny=1, nx)\n",
    "        grad_wt = dZ.mm(inp.t())\n",
    "        # shape(ny=1, 1)\n",
    "        grad_b = torch.sum(dZ, dim=1, keepdim=True)\n",
    "        \n",
    "        return grad_inp, grad_wt, grad_b\n",
    "\n",
    "    \n",
    "class CustomLinearLayer(nn.Module):\n",
    "    '''Linear with custom backward'''\n",
    "    def __init__(self, nx, init_weight, init_bias):\n",
    "        super(CustomLinearLayer, self).__init__()\n",
    "        # init weight and bias\n",
    "        self.weight = nn.Parameter(torch.tensor(init_weight))\n",
    "        self.bias = nn.Parameter(torch.tensor(init_bias))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        # (m, ny=1)\n",
    "        z = CustomLinearFunction.apply(X.t(), self.weight, self.bias).t()\n",
    "        return z      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient check Sigmoid\n",
    "inp_test = torch.rand(10, 1, requires_grad=True).double()\n",
    "assert torch.autograd.gradcheck(CustomSigmoidFunction.apply, (inp_test,), raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient check CustomLinear\n",
    "inp_test = torch.rand(5, 1000, requires_grad=True).double()\n",
    "wt_test = torch.rand(1, 5,requires_grad=True).double()\n",
    "b_test = torch.rand(1, 1,requires_grad=True).double()\n",
    "assert torch.autograd.gradcheck(CustomLinearFunction.apply, (inp_test, wt_test, b_test), raise_exception=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. With custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable loss implementation\n",
    "# tensorflow demo\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "# Pytorch source code\n",
    "# https://github.com/pytorch/pytorch/blob/7d6d5f4be0da26079bc81ca49265cde713a75051/aten/src/ATen/native/Loss.cpp#L201\n",
    "\n",
    "# how to write a pytorch loss autograd.function with backward vs nn.module with only forward\n",
    "# https://discuss.pytorch.org/t/custom-loss-autograd-module-what-is-the-difference/69251\n",
    "\n",
    "# DeepLearning Specialization Homework\n",
    "# https://github.com/Chucooleg/DeepLearning_Specialization_Assignments/blob/master/course%201%20Assignments/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb\n",
    "\n",
    "class CustomBCEWithLogitLoss(torch.autograd.Function):\n",
    "    '''\n",
    "    Custom Binary Cross Entropy Loss with Logits.\n",
    "    Implementation Goal -- Numerically stable implementation.\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, Z, Y):\n",
    "        '''\n",
    "        Z: Pre-Activations(i.e. Logits), shape(m, ny=1)\n",
    "        Y: Predictions, shape(m, ny=1)\n",
    "        '''\n",
    "        ctx.save_for_backward(Z, Y)\n",
    "        \n",
    "        # this intuitive version is not numerically stable if Z is a large -ve number\n",
    "#         A = 1 / (1 + torch.exp(-Z))\n",
    "#         loss = - torch.mean(Y * torch.log(A) + (1 - Y) * torch.log(1 - A))\n",
    "        \n",
    "        # follow this tensorflow implmentation\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "        loss = torch.max(Z, torch.zeros(Z.shape, dtype=Z.dtype)) - Z * Y + torch.log(1 + torch.exp(-torch.abs(Z)))\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    " \n",
    "        # retrieve cache\n",
    "        Z, Y = ctx.saved_tensors\n",
    "        grad_Z, grad_Y = None, None\n",
    "        m = Z.shape[0]\n",
    "        \n",
    "        # https://github.com/pytorch/pytorch/blob/7d6d5f4be0da26079bc81ca49265cde713a75051/aten/src/ATen/native/Loss.cpp#L226\n",
    "        grad_Z = (torch.sigmoid(Z) - Y) * grad_output / m\n",
    "        grad_Y = - Z * grad_output / m\n",
    "        \n",
    "        return grad_Z, grad_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradcheck a custom loss function\n",
    "# https://discuss.pytorch.org/t/how-to-check-the-gradients-of-custom-implemented-loss-function/8546\n",
    "\n",
    "# gradient check CustomBCEWithLogitLoss\n",
    "Z_test = torch.rand(10, 1,requires_grad=True).double()\n",
    "Y_test = torch.rand(10, 1,requires_grad=True).double()\n",
    "assert torch.autograd.gradcheck(CustomBCEWithLogitLoss.apply, (Z_test, Y_test), raise_exception=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. With custom optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Optimizer Tutorial\n",
    "# http://mcneela.github.io/machine_learning/2019/09/03/Writing-Your-Own-Optimizers-In-Pytorch.html\n",
    "# https://huggingface.co/transformers/_modules/transformers/optimization.html#AdamW\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class CustomSGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        defaults = dict(lr=lr)\n",
    "        super(CustomSGD, self).__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        '''performs single optimization step'''\n",
    "        loss = None\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                \n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "                \n",
    "                p.data.add_(grad, alpha=-group['lr'])\n",
    "        \n",
    "        return loss     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Train & Pred Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, loss_criterion, optimizer, args, epochs=20):\n",
    "    '''\n",
    "    Train model and report losses on train and dev sets per epoch\n",
    "    '''\n",
    "    \n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'valid_losses': [],        \n",
    "        'valid_accuracy': [],\n",
    "        'weights': [],\n",
    "        'bias': [],\n",
    "    }\n",
    "\n",
    "    # save parameters\n",
    "    write_param_history(model, history)\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for batch_i, batch_data in enumerate(train_loader):\n",
    "            logits, activations = model(batch_data['X'])\n",
    "            loss = loss_criterion(logits, batch_data['y'])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        history['train_losses'].append(sum(batch_losses) / len(batch_losses))\n",
    "\n",
    "        # validate\n",
    "        batch_val_losses, batch_val_accuracies = pred(model, valid_loader, loss_criterion)\n",
    "        history['valid_losses'].append(sum(batch_val_losses) / len(batch_val_losses))\n",
    "        history['valid_accuracy'].append(sum(batch_val_accuracies) / len(batch_val_accuracies))\n",
    "\n",
    "        # save parameters\n",
    "        write_param_history(model, history)\n",
    "        \n",
    "    return history\n",
    "\n",
    "def write_param_history(model, history):\n",
    "    weights = model.scorer.weight.clone().detach().numpy()\n",
    "    bias = model.scorer.bias.data.clone().detach().numpy()\n",
    "    history['weights'].append(weights)\n",
    "    history['bias'].append(bias)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pred(model, test_loader, loss_criterion):\n",
    "    '''Propogate forward on dev or test set, report loss and accuracy.'''\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "    for batch_i, batch_data in enumerate(test_loader):\n",
    "        logits, activations = model(batch_data['X'])\n",
    "        loss = loss_criterion(logits, batch_data['y'])\n",
    "        batch_losses.append(loss.item())\n",
    "        accuracy = torch.mean((activations > 0.5).type(torch.FloatTensor).eq(batch_data['y']).type(torch.FloatTensor))\n",
    "        batch_accuracies.append(accuracy.item())\n",
    "    \n",
    "    return batch_losses, batch_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dataloader\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "# Pytorch Data Collate (Further reading, not implemented here)\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Toy dataset for Logistic Regression.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Path to the directory with data files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # shape (m, nx)\n",
    "        self.X = np.load(os.path.join(data_dir, 'features.npy'))\n",
    "        # shape (m, ny=1)\n",
    "        self.y = np.load(os.path.join(data_dir, 'labels.npy'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        X = torch.from_numpy(self.X[idx, :]).type(torch.FloatTensor)\n",
    "        y = torch.from_numpy(self.y[idx, :]).type(torch.FloatTensor)\n",
    "        sample = {'X': X, 'y': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training indices [1037  655  547  487  307  689  856  309  260  229]\n",
      "X shape (1090, 10)\n",
      "y shape (1090, 1)\n"
     ]
    }
   ],
   "source": [
    "# construct and save toydataset\n",
    "\n",
    "m_train, m_valid, m_test = 90, 500, 500\n",
    "m_total = m_train + m_valid + m_test\n",
    "\n",
    "X, y = make_classification(n_samples=m_total, n_features=10, n_informative=10, n_redundant=0, n_repeated=0, n_classes=1, n_clusters_per_class=4, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=args.seed)\n",
    "y = np.expand_dims(y, -1)\n",
    "\n",
    "np.random.seed(123)\n",
    "permutation = np.random.permutation(m_total)\n",
    "print('First 10 training indices', permutation[:10])\n",
    "print('X shape', X.shape)\n",
    "print('y shape', y.shape)\n",
    "\n",
    "train_indices = permutation[0:m_train]\n",
    "valid_indices = permutation[m_train:m_train+m_valid]\n",
    "test_indices = permutation[m_train+m_valid:m_test]\n",
    "\n",
    "np.save(os.path.join(args.data_dir, 'toy_lr_1', 'train', 'features.npy'), X[train_indices])\n",
    "np.save(os.path.join(args.data_dir, 'toy_lr_1', 'train', 'labels.npy'), y[train_indices])\n",
    "\n",
    "np.save(os.path.join(args.data_dir, 'toy_lr_1', 'valid', 'features.npy'), X[valid_indices])\n",
    "np.save(os.path.join(args.data_dir, 'toy_lr_1', 'valid', 'labels.npy'), y[valid_indices])\n",
    "\n",
    "np.save(os.path.join(args.data_dir, 'toy_lr_1', 'test', 'features.npy'), X[test_indices])\n",
    "np.save(os.path.join(args.data_dir, 'toy_lr_1', 'test', 'labels.npy'), y[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and compare results on toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "training_set = ToyDataset(data_dir=os.path.join(args.data_dir, 'toy_lr_1', 'train'))\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_set = ToyDataset(data_dir=os.path.join(args.data_dir, 'toy_lr_1', 'valid'))\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, batch_size=batch_size)\n",
    "\n",
    "test_set = ToyDataset(data_dir=os.path.join(args.data_dir, 'toy_lr_1', 'test'))\n",
    "test_generator = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "m = training_set.X.shape[0]\n",
    "nx = training_set.X.shape[1]\n",
    "ny = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# set off-the-shelf model, loss function and optimizer\n",
    "model = LogisticRegressionLazy(nx)\n",
    "loss_criterion_lazy = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer_lazy = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "history_off_the_shelf = train(model, training_generator, validation_generator, loss_criterion, optimizer, args, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12895013,  0.01047492, -0.15705723,  0.11925378, -0.26944348,\n",
       "         0.23180881, -0.22984707, -0.25141433, -0.19982024,  0.1432175 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_off_the_shelf['weights'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11684369], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_off_the_shelf['bias'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "wt_arr = [[-0.12895013,  0.01047492, -0.15705723,  0.11925378, -0.26944348,\n",
    "         0.23180881, -0.22984707, -0.25141433, -0.19982024,  0.1432175 ]]\n",
    "bias_arr = [[-0.11684369]]\n",
    "\n",
    "# set custom model, loss function and optimizer\n",
    "model = LogisticRegressionCustom(nx, init_weight=wt_arr, init_bias=bias_arr)\n",
    "loss_criterion = CustomBCEWithLogitLoss.apply\n",
    "optimizer = CustomSGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "history_custom = train(model, training_generator, validation_generator, loss_criterion, optimizer, args, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Cross-comparison on train & valid loss, accuracy and parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6196715947654512, 0.6201359927654266),\n",
       " (0.46291036903858185, 0.4648684403962559),\n",
       " (0.3668956657250722, 0.36838793257872265),\n",
       " (0.3055238318112161, 0.3070671570797761),\n",
       " (0.2641151857872804, 0.2640540964073605),\n",
       " (0.2331896329091655, 0.23380550162659752),\n",
       " (0.2104352960983912, 0.21100338507029745),\n",
       " (0.19241697672340605, 0.1930240887320704),\n",
       " (0.17750125378370285, 0.17796090824736488),\n",
       " (0.16582258914907774, 0.16588551302750906)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['train_losses'], history_off_the_shelf['train_losses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5399632832407951, 0.5413854518532752),\n",
       " (0.435880730599165, 0.4370486372709274),\n",
       " (0.37145381346344947, 0.37267079040408135),\n",
       " (0.32947849318385125, 0.3301878882944584),\n",
       " (0.3007757315039635, 0.30113270990550517),\n",
       " (0.27999262131750585, 0.28022984832525255),\n",
       " (0.264319948926568, 0.26447768017649653),\n",
       " (0.2520730609446764, 0.25213010914623735),\n",
       " (0.24232644483447074, 0.24237974375486374),\n",
       " (0.23437572184950115, 0.2343392314016819)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['valid_losses'], history_off_the_shelf['valid_losses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.718000012487173, 0.718000012487173),\n",
       " (0.7900000104308128, 0.7900000104308128),\n",
       " (0.8500000083446503, 0.8480000084638596),\n",
       " (0.8820000067353249, 0.8820000067353249),\n",
       " (0.8980000054836274, 0.8980000054836274),\n",
       " (0.9120000049471855, 0.9140000048279763),\n",
       " (0.9240000042319297, 0.9240000042319297),\n",
       " (0.9320000037550926, 0.9320000037550926),\n",
       " (0.9340000036358833, 0.9340000036358833),\n",
       " (0.9320000037550926, 0.9320000037550926)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['valid_accuracy'], history_off_the_shelf['valid_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[-0.12895013,  0.01047492, -0.15705723,  0.11925378, -0.26944348,\n",
       "           0.23180881, -0.22984707, -0.25141433, -0.19982024,  0.1432175 ]],\n",
       "        dtype=float32),\n",
       "  array([[-0.12895013,  0.01047492, -0.15705723,  0.11925378, -0.26944348,\n",
       "           0.23180881, -0.22984707, -0.25141433, -0.19982024,  0.1432175 ]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.08323156, -0.04968001, -0.2359461 ,  0.07982081, -0.26038086,\n",
       "           0.29659656, -0.178131  , -0.18310717, -0.15923896,  0.08490142]],\n",
       "        dtype=float32),\n",
       "  array([[-0.08386043, -0.0485882 , -0.23378837,  0.07998081, -0.2593329 ,\n",
       "           0.29605865, -0.17820223, -0.18384506, -0.15970805,  0.08506422]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.04662747, -0.09852936, -0.29087773,  0.04752795, -0.24887085,\n",
       "           0.34518456, -0.13998047, -0.13147025, -0.12872545,  0.03860396]],\n",
       "        dtype=float32),\n",
       "  array([[-0.04731767, -0.09730133, -0.28897136,  0.04735611, -0.24824813,\n",
       "           0.3449241 , -0.13994777, -0.13252696, -0.12934774,  0.03959347]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.01717448, -0.13719505, -0.3315649 ,  0.01907659, -0.23833108,\n",
       "           0.38378352, -0.11090755, -0.09200607, -0.10543218,  0.00094223]],\n",
       "        dtype=float32),\n",
       "  array([[-0.01840698, -0.13611001, -0.32987136,  0.01923787, -0.23734885,\n",
       "           0.38343155, -0.11075728, -0.09471441, -0.10608354,  0.00240059]],\n",
       "        dtype=float32)),\n",
       " (array([[ 0.00604764, -0.1693115 , -0.36374256, -0.00542636, -0.22906072,\n",
       "           0.4151713 , -0.08712494, -0.06306264, -0.08762647, -0.02981915]],\n",
       "        dtype=float32),\n",
       "  array([[ 0.00552112, -0.16837075, -0.36215428, -0.00526457, -0.22757508,\n",
       "           0.41553038, -0.0869793 , -0.06502388, -0.08783148, -0.02787928]],\n",
       "        dtype=float32)),\n",
       " (array([[ 0.02542566, -0.19578993, -0.3896048 , -0.02735199, -0.22063525,\n",
       "           0.4422435 , -0.06765158, -0.04125354, -0.07304512, -0.05498828]],\n",
       "        dtype=float32),\n",
       "  array([[ 0.02457077, -0.19552045, -0.38797456, -0.02719872, -0.21875557,\n",
       "           0.44268203, -0.06777561, -0.04265887, -0.07373405, -0.0530163 ]],\n",
       "        dtype=float32)),\n",
       " (array([[ 0.04136696, -0.21899156, -0.4110118 , -0.04719261, -0.21304534,\n",
       "           0.46557036, -0.05157355, -0.02431266, -0.06134304, -0.07675888]],\n",
       "        dtype=float32),\n",
       "  array([[ 0.04053245, -0.21910244, -0.409297  , -0.04676788, -0.21141359,\n",
       "           0.46585613, -0.05166157, -0.02557138, -0.06230389, -0.07454157]],\n",
       "        dtype=float32)),\n",
       " (array([[ 0.05477418, -0.23940493, -0.42924455, -0.06497625, -0.20580009,\n",
       "           0.48649538, -0.037898  , -0.01059423, -0.05149178, -0.09559289]],\n",
       "        dtype=float32),\n",
       "  array([[ 0.05451928, -0.23967054, -0.42740792, -0.06478773, -0.20463534,\n",
       "           0.48664987, -0.03778034, -0.01168336, -0.05231651, -0.09333447]],\n",
       "        dtype=float32)),\n",
       " (array([[ 6.62384331e-02, -2.57927179e-01, -4.45132852e-01,\n",
       "          -8.14636126e-02, -1.99497089e-01,  5.05174875e-01,\n",
       "          -2.55596917e-02,  3.87817650e-04, -4.34384272e-02,\n",
       "          -1.12329535e-01]], dtype=float32),\n",
       "  array([[ 0.06607909, -0.2583362 , -0.44339818, -0.08137885, -0.19869705,\n",
       "           0.50520325, -0.02543954, -0.00054457, -0.04397643, -0.11009779]],\n",
       "        dtype=float32)),\n",
       " (array([[ 0.07622751, -0.27458265, -0.4590408 , -0.09672493, -0.19379057,\n",
       "           0.52226573, -0.01469538,  0.00935384, -0.03661128, -0.1271315 ]],\n",
       "        dtype=float32),\n",
       "  array([[ 0.07633178, -0.27476743, -0.4574906 , -0.0966289 , -0.19316238,\n",
       "           0.52205354, -0.01458454,  0.00838832, -0.03671534, -0.12516627]],\n",
       "        dtype=float32)),\n",
       " (array([[ 0.08529837, -0.2898622 , -0.47131237, -0.11100692, -0.18878323,\n",
       "           0.53777766, -0.00522303,  0.01671384, -0.03019092, -0.14053875]],\n",
       "        dtype=float32),\n",
       "  array([[ 0.08498213, -0.2900005 , -0.4700849 , -0.11084288, -0.18801333,\n",
       "           0.53779906, -0.00467883,  0.0156306 , -0.03072714, -0.1387227 ]],\n",
       "        dtype=float32))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['weights'], history_off_the_shelf['weights']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[-0.11684369]], dtype=float32), array([-0.11684369], dtype=float32)),\n",
       " (array([[-0.19164065]], dtype=float32), array([-0.1915197], dtype=float32)),\n",
       " (array([[-0.25272644]], dtype=float32), array([-0.25295103], dtype=float32)),\n",
       " (array([[-0.30405846]], dtype=float32), array([-0.30448583], dtype=float32)),\n",
       " (array([[-0.3483866]], dtype=float32), array([-0.34896126], dtype=float32)),\n",
       " (array([[-0.38755324]], dtype=float32), array([-0.38814685], dtype=float32)),\n",
       " (array([[-0.42270768]], dtype=float32), array([-0.42335588], dtype=float32)),\n",
       " (array([[-0.45478928]], dtype=float32), array([-0.4554978], dtype=float32)),\n",
       " (array([[-0.48439842]], dtype=float32), array([-0.4851668], dtype=float32)),\n",
       " (array([[-0.5119091]], dtype=float32), array([-0.51271677], dtype=float32)),\n",
       " (array([[-0.5377259]], dtype=float32), array([-0.5385727], dtype=float32))]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['bias'], history_off_the_shelf['bias']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Further reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Data Collate (Further reading, not implemented here)\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "# Karpathy resource on a tiny implementation of  autodiff from scratch if anyone is interested. Engine.py is where the meat is\n",
    "# https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
