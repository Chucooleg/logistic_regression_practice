{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "args.data_dir = '/datadrive'\n",
    "args.seed = 123\n",
    "args.lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Off the shelf implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionLazy(nn.Module):\n",
    "\n",
    "    def __init__(self, nx):\n",
    "        super(LogisticRegressionLazy, self).__init__()\n",
    "        self.scorer = nn.Linear(nx, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        # shape (m, 1)\n",
    "        z = self.scorer(X)\n",
    "        # shape (m, 1)\n",
    "        a = torch.sigmoid(z)\n",
    "        return z, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. With custom linear module and sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending pytorch (demo of custom function with custom forward backward, custom LinearFunction)\n",
    "# https://pytorch.org/docs/master/notes/extending.html\n",
    "# https://github.com/pytorch/pytorch/blob/c9bb990707d4bfe524f3f1c4a77ff85fed1cd2a2/torch/csrc/api/include/torch/nn/functional/loss.h\n",
    "\n",
    "# pytorch Autograd function (RELU example)\n",
    "# https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "\n",
    "# discussion custom threshold forward and backward\n",
    "# https://discuss.pytorch.org/t/how-to-call-only-backward-path-of-pytorch-function/22839/2\n",
    "\n",
    "# Define custom autograd.Function and put the function in nn.Module\n",
    "# https://discuss.pytorch.org/t/how-to-call-the-backward-function-of-a-custom-module/7853\n",
    "\n",
    "class LogisticRegressionCustom(nn.Module):\n",
    "    '''Linear and sigmoid with custom backward'''\n",
    "    def __init__(self, nx, init_weight, init_bias):\n",
    "        super(LogisticRegressionCustom, self).__init__()\n",
    "        self.scorer = CustomLinearLayer(nx, init_weight, init_bias)\n",
    "        self.sigmoid = CustomSigmoidFunction.apply\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        # shape(m, ny=1)\n",
    "        z = self.scorer(X)\n",
    "        # shape(m, ny=1)\n",
    "        a = self.sigmoid(z)\n",
    "        return z, a  \n",
    "\n",
    "class CustomSigmoidFunction(torch.autograd.Function):\n",
    "    '''\n",
    "    doesn't get backprop through because loss function takes in logit directly\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        '''\n",
    "        inp: shape(m, ny)\n",
    "        '''\n",
    "        ctx.save_for_backward(inp)\n",
    "        return 1 / (1 + torch.exp(-inp))\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, dA):\n",
    "        '''\n",
    "        Demonstration purpose. Not used in overall backprop since our loss function computes with logits.\n",
    "        dA: shape(m, ny)\n",
    "        '''\n",
    "        # retrieve cache\n",
    "        inp, = ctx.saved_tensors\n",
    "        grad_inp = None\n",
    "        \n",
    "        A = 1.0 / (1.0 + torch.exp(-inp))\n",
    "        # shape(m, ny)\n",
    "        grad_inp = A * (1 - A) * dA\n",
    "        \n",
    "        return grad_inp\n",
    "\n",
    "class CustomLinearFunction(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, inp, wt, b):\n",
    "        '''\n",
    "        inp: shape(nx, m)\n",
    "        wt: shape(ny=1, nx)\n",
    "        b: shape(ny=1, 1)\n",
    "        '''\n",
    "        ctx.save_for_backward(inp, wt, b)\n",
    "        # (ny, m) = (ny, nx)(nx, m) + (ny, 1)t\n",
    "        z = wt.mm(inp) + b\n",
    "        assert z.shape == (1, inp.shape[1])\n",
    "        # (ny, m)\n",
    "        return z\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, dZ):\n",
    "        '''\n",
    "        dZ: shape(ny, m)\n",
    "        '''\n",
    "        \n",
    "        # retrieve cache\n",
    "        inp, wt, b = ctx.saved_tensors\n",
    "        m = inp.shape[1]\n",
    "        grad_inp, grad_wt, grad_b = None, None, None\n",
    "        \n",
    "        # Z = W dot X.T + b \n",
    "        # shape(nx, m)\n",
    "        grad_inp = wt.t().mm(dZ)\n",
    "        # shape(ny=1, nx)\n",
    "        grad_wt = dZ.mm(inp.t())\n",
    "        # shape(ny=1, 1)\n",
    "        grad_b = torch.sum(dZ, dim=1, keepdim=True)\n",
    "        \n",
    "        return grad_inp, grad_wt, grad_b\n",
    "\n",
    "    \n",
    "class CustomLinearLayer(nn.Module):\n",
    "    '''Linear with custom backward'''\n",
    "    def __init__(self, nx, init_weight, init_bias):\n",
    "        super(CustomLinearLayer, self).__init__()\n",
    "        # init weight and bias\n",
    "        self.weight = nn.Parameter(torch.tensor(init_weight))\n",
    "        self.bias = nn.Parameter(torch.tensor(init_bias))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        # (m, ny=1)\n",
    "        z = CustomLinearFunction.apply(X.t(), self.weight, self.bias).t()\n",
    "        return z      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient check Sigmoid\n",
    "inp_test = torch.rand(10, 1, requires_grad=True).double()\n",
    "assert torch.autograd.gradcheck(CustomSigmoidFunction.apply, (inp_test,), raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient check CustomLinear\n",
    "inp_test = torch.rand(5, 1000, requires_grad=True).double()\n",
    "wt_test = torch.rand(1, 5,requires_grad=True).double()\n",
    "b_test = torch.rand(1, 1,requires_grad=True).double()\n",
    "assert torch.autograd.gradcheck(CustomLinearFunction.apply, (inp_test, wt_test, b_test), raise_exception=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. With custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable loss implementation\n",
    "# tensorflow demo\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "# Pytorch source code\n",
    "# https://github.com/pytorch/pytorch/blob/7d6d5f4be0da26079bc81ca49265cde713a75051/aten/src/ATen/native/Loss.cpp#L201\n",
    "\n",
    "# how to write a pytorch loss autograd.function with backward vs nn.module with only forward\n",
    "# https://discuss.pytorch.org/t/custom-loss-autograd-module-what-is-the-difference/69251\n",
    "\n",
    "# DeepLearning Specialization Homework\n",
    "# https://github.com/Chucooleg/DeepLearning_Specialization_Assignments/blob/master/course%201%20Assignments/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb\n",
    "\n",
    "class CustomBCEWithLogitLoss(torch.autograd.Function):\n",
    "    '''\n",
    "    Custom Binary Cross Entropy Loss with Logits.\n",
    "    Implementation Goal -- Numerically stable implementation.\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, Z, Y):\n",
    "        '''\n",
    "        Z: Pre-Activations(i.e. Logits), shape(m, ny=1)\n",
    "        Y: Predictions, shape(m, ny=1)\n",
    "        '''\n",
    "        ctx.save_for_backward(Z, Y)\n",
    "        \n",
    "        # this intuitive version is not numerically stable if Z is a large -ve number\n",
    "#         A = 1 / (1 + torch.exp(-Z))\n",
    "#         loss = - torch.mean(Y * torch.log(A) + (1 - Y) * torch.log(1 - A))\n",
    "        \n",
    "        # follow this tensorflow implmentation\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "        loss = torch.max(Z, torch.zeros(Z.shape, dtype=Z.dtype)) - Z * Y + torch.log(1 + torch.exp(-torch.abs(Z)))\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    " \n",
    "        # retrieve cache\n",
    "        Z, Y = ctx.saved_tensors\n",
    "        grad_Z, grad_Y = None, None\n",
    "        m = Z.shape[0]\n",
    "        \n",
    "        # https://github.com/pytorch/pytorch/blob/7d6d5f4be0da26079bc81ca49265cde713a75051/aten/src/ATen/native/Loss.cpp#L226\n",
    "        grad_Z = (torch.sigmoid(Z) - Y) * grad_output / m\n",
    "        grad_Y = - Z * grad_output / m\n",
    "        \n",
    "        return grad_Z, grad_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradcheck a custom loss function\n",
    "# https://discuss.pytorch.org/t/how-to-check-the-gradients-of-custom-implemented-loss-function/8546\n",
    "\n",
    "# gradient check CustomBCEWithLogitLoss\n",
    "Z_test = torch.rand(10, 1,requires_grad=True).double()\n",
    "Y_test = torch.rand(10, 1,requires_grad=True).double()\n",
    "assert torch.autograd.gradcheck(CustomBCEWithLogitLoss.apply, (Z_test, Y_test), raise_exception=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. With custom optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Optimizer Tutorial\n",
    "# http://mcneela.github.io/machine_learning/2019/09/03/Writing-Your-Own-Optimizers-In-Pytorch.html\n",
    "# https://huggingface.co/transformers/_modules/transformers/optimization.html#AdamW\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class CustomSGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        defaults = dict(lr=lr)\n",
    "        super(CustomSGD, self).__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        '''performs single optimization step'''\n",
    "        loss = None\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                \n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "                \n",
    "                p.data.add_(grad, alpha=-group['lr'])\n",
    "        \n",
    "        return loss     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Train & Pred Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, loss_criterion, optimizer, args, epochs=20):\n",
    "    '''\n",
    "    Train model and report losses on train and dev sets per epoch\n",
    "    '''\n",
    "    \n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'valid_losses': [],        \n",
    "        'valid_accuracy': [],\n",
    "        'weights': [],\n",
    "        'bias': [],\n",
    "    }\n",
    "\n",
    "    # save parameters\n",
    "    write_param_history(model, history)\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for batch_i, batch_data in enumerate(train_loader):\n",
    "            logits, activations = model(batch_data['X'])\n",
    "            loss = loss_criterion(logits, batch_data['y'])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        history['train_losses'].append(sum(batch_losses) / len(batch_losses))\n",
    "\n",
    "        # validate\n",
    "        batch_val_losses, batch_val_accuracies = pred(model, valid_loader, loss_criterion)\n",
    "        history['valid_losses'].append(sum(batch_val_losses) / len(batch_val_losses))\n",
    "        history['valid_accuracy'].append(sum(batch_val_accuracies) / len(batch_val_accuracies))\n",
    "\n",
    "        # save parameters\n",
    "        write_param_history(model, history)\n",
    "        \n",
    "    return history\n",
    "\n",
    "def write_param_history(model, history):\n",
    "    weights = model.scorer.weight.clone().detach().numpy()\n",
    "    bias = model.scorer.bias.data.clone().detach().numpy()\n",
    "    history['weights'].append(weights)\n",
    "    history['bias'].append(bias)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pred(model, test_loader, loss_criterion):\n",
    "    '''Propogate forward on dev or test set, report loss and accuracy.'''\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "    for batch_i, batch_data in enumerate(test_loader):\n",
    "        logits, activations = model(batch_data['X'])\n",
    "        loss = loss_criterion(logits, batch_data['y'])\n",
    "        batch_losses.append(loss.item())\n",
    "        accuracy = torch.mean((activations > 0.5).type(torch.FloatTensor).eq(batch_data['y']).type(torch.FloatTensor))\n",
    "        batch_accuracies.append(accuracy.item())\n",
    "    \n",
    "    return batch_losses, batch_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dataloader\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "# Pytorch Data Collate (Further reading, not implemented here)\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Toy dataset for Logistic Regression.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Path to the directory with data files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # shape (m, nx)\n",
    "        self.X = np.load(os.path.join(data_dir, 'features.npy'))\n",
    "        # shape (m, ny=1)\n",
    "        self.y = np.load(os.path.join(data_dir, 'labels.npy'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        X = torch.from_numpy(self.X[idx, :]).type(torch.FloatTensor)\n",
    "        y = torch.from_numpy(self.y[idx, :]).type(torch.FloatTensor)\n",
    "        sample = {'X': X, 'y': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give permission to access /datadrive\n",
    "!sudo chmod -R 777 /datadrive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training indices [1037  655  547  487  307  689  856  309  260  229]\n",
      "X shape (1090, 10)\n",
      "y shape (1090, 1)\n"
     ]
    }
   ],
   "source": [
    "# construct and save toydataset\n",
    "\n",
    "m_train, m_valid, m_test = 90, 500, 500\n",
    "m_total = m_train + m_valid + m_test\n",
    "\n",
    "X, y = make_classification(n_samples=m_total, n_features=10, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=4, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=args.seed)\n",
    "y = np.expand_dims(y, -1)\n",
    "\n",
    "np.random.seed(123)\n",
    "permutation = np.random.permutation(m_total)\n",
    "print('First 10 training indices', permutation[:10])\n",
    "print('X shape', X.shape)\n",
    "print('y shape', y.shape)\n",
    "\n",
    "train_indices = permutation[0:m_train]\n",
    "valid_indices = permutation[m_train:m_train+m_valid]\n",
    "test_indices = permutation[m_train+m_valid:]\n",
    "\n",
    "# np.save(os.path.join(args.data_dir, 'toy_lr_1', 'train', 'features.npy'), X[train_indices])\n",
    "# np.save(os.path.join(args.data_dir, 'toy_lr_1', 'train', 'labels.npy'), y[train_indices])\n",
    "\n",
    "# np.save(os.path.join(args.data_dir, 'toy_lr_1', 'valid', 'features.npy'), X[valid_indices])\n",
    "# np.save(os.path.join(args.data_dir, 'toy_lr_1', 'valid', 'labels.npy'), y[valid_indices])\n",
    "\n",
    "# np.save(os.path.join(args.data_dir, 'toy_lr_1', 'test', 'features.npy'), X[test_indices])\n",
    "# np.save(os.path.join(args.data_dir, 'toy_lr_1', 'test', 'labels.npy'), y[test_indices])\n",
    "\n",
    "\n",
    "dataset_dir = 'toy_lr_1'\n",
    "os.makedirs(os.path.join(args.data_dir, dataset_dir, 'train'), mode = 0o777, exist_ok = True) \n",
    "os.makedirs(os.path.join(args.data_dir, dataset_dir, 'valid'), mode = 0o777, exist_ok = True) \n",
    "os.makedirs(os.path.join(args.data_dir, dataset_dir, 'test'), mode = 0o777, exist_ok = True) \n",
    "\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'train', 'features.npy'), X[train_indices])\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'train', 'labels.npy'), y[train_indices])\n",
    "\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'valid', 'features.npy'), X[valid_indices])\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'valid', 'labels.npy'), y[valid_indices])\n",
    "\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'test', 'features.npy'), X[test_indices])\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'test', 'labels.npy'), y[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and compare results on toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "training_set = ToyDataset(data_dir=os.path.join(args.data_dir, 'toy_lr_1', 'train'))\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_set = ToyDataset(data_dir=os.path.join(args.data_dir, 'toy_lr_1', 'valid'))\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, batch_size=batch_size)\n",
    "\n",
    "test_set = ToyDataset(data_dir=os.path.join(args.data_dir, 'toy_lr_1', 'test'))\n",
    "test_generator = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "m = training_set.X.shape[0]\n",
    "nx = training_set.X.shape[1]\n",
    "ny = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# set off-the-shelf model, loss function and optimizer\n",
    "model = LogisticRegressionLazy(nx)\n",
    "loss_criterion_lazy = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer_lazy = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "history_off_the_shelf = train(model, training_generator, validation_generator, loss_criterion_lazy, optimizer_lazy, args, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12895013,  0.01047492, -0.15705723,  0.11925378, -0.26944348,\n",
       "         0.23180881, -0.22984707, -0.25141433, -0.19982024,  0.1432175 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_off_the_shelf['weights'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11684369], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_off_the_shelf['bias'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "wt_arr = [[-0.12895013,  0.01047492, -0.15705723,  0.11925378, -0.26944348,\n",
    "         0.23180881, -0.22984707, -0.25141433, -0.19982024,  0.1432175 ]]\n",
    "bias_arr = [[-0.11684369]]\n",
    "\n",
    "# set custom model, loss function and optimizer\n",
    "model = LogisticRegressionCustom(nx, init_weight=wt_arr, init_bias=bias_arr)\n",
    "loss_criterion = CustomBCEWithLogitLoss.apply\n",
    "optimizer = CustomSGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "history_custom = train(model, training_generator, validation_generator, loss_criterion, optimizer, args, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Cross-comparison on train & valid loss, accuracy and parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5809669726424747, 0.577768819199668),\n",
       " (0.49381006757418316, 0.49348143819305634),\n",
       " (0.4390552697910203, 0.43901902271641624),\n",
       " (0.4032263747519917, 0.40312086708015865),\n",
       " (0.3805066785878605, 0.38032788783311844),\n",
       " (0.36376417097118163, 0.36256616645389134),\n",
       " (0.3502613811029328, 0.35040000246630776),\n",
       " (0.33932048827409744, 0.3394729180468453),\n",
       " (0.33107655743757886, 0.33110859327846104),\n",
       " (0.3242419502801365, 0.32440519788199)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['train_losses'], history_off_the_shelf['train_losses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5576036085188388, 0.5574356658756733),\n",
       " (0.5193752017617226, 0.5199329514801502),\n",
       " (0.49643942549824716, 0.4968894647061825),\n",
       " (0.48121415615081786, 0.48186307355761526),\n",
       " (0.4706562738120556, 0.47149196460843085),\n",
       " (0.4629448476433754, 0.4638245105743408),\n",
       " (0.4572421546280384, 0.45781891606748104),\n",
       " (0.4529050077497959, 0.453350076302886),\n",
       " (0.4493377766013145, 0.44998479798436164),\n",
       " (0.4465303386002779, 0.44703921392560003)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['valid_losses'], history_off_the_shelf['valid_losses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7000000117719174, 0.7000000117719174),\n",
       " (0.7420000120997429, 0.7400000122189522),\n",
       " (0.7500000119209289, 0.7480000120401382),\n",
       " (0.7720000118017196, 0.7660000118613243),\n",
       " (0.7760000112652778, 0.7760000112652778),\n",
       " (0.7780000108480454, 0.7760000106692314),\n",
       " (0.788000010251999, 0.788000010251999),\n",
       " (0.7900000101327896, 0.7900000101327896),\n",
       " (0.7920000100135803, 0.7920000100135803),\n",
       " (0.796000010073185, 0.7980000099539757)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['valid_accuracy'], history_off_the_shelf['valid_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[-0.12895013,  0.01047492, -0.15705723,  0.11925378, -0.26944348,\n",
       "           0.23180881, -0.22984707, -0.25141433, -0.19982024,  0.1432175 ]],\n",
       "        dtype=float32),\n",
       "  array([[-0.12895013,  0.01047492, -0.15705723,  0.11925378, -0.26944348,\n",
       "           0.23180881, -0.22984707, -0.25141433, -0.19982024,  0.1432175 ]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.1495411 , -0.02349061, -0.22837886,  0.07674928, -0.25467896,\n",
       "           0.21042143, -0.23045243, -0.2415796 , -0.22928452,  0.05251009]],\n",
       "        dtype=float32),\n",
       "  array([[-0.15052389, -0.02369768, -0.22956073,  0.07711416, -0.2532521 ,\n",
       "           0.20928231, -0.2290748 , -0.242651  , -0.22922324,  0.05096457]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.16749202, -0.05449502, -0.2866693 ,  0.04461355, -0.24168415,\n",
       "           0.19611567, -0.23023418, -0.23195024, -0.252494  , -0.02308795]],\n",
       "        dtype=float32),\n",
       "  array([[-0.16806675, -0.05138322, -0.28755304,  0.04552607, -0.23789325,\n",
       "           0.19817658, -0.22975099, -0.2333527 , -0.25107193, -0.02259674]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.18041074, -0.0744482 , -0.33418474,  0.02303663, -0.23048353,\n",
       "           0.19084792, -0.23271444, -0.22420773, -0.2693751 , -0.08344068]],\n",
       "        dtype=float32),\n",
       "  array([[-0.18284376, -0.07247232, -0.33478302,  0.02379456, -0.22629525,\n",
       "           0.19296306, -0.23278433, -0.22523731, -0.2684025 , -0.08302245]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.19143836, -0.08960422, -0.3728183 ,  0.00786625, -0.22074081,\n",
       "           0.1916349 , -0.23654008, -0.21699297, -0.28296652, -0.13370226]],\n",
       "        dtype=float32),\n",
       "  array([[-0.19439793, -0.08861963, -0.37376487,  0.008744  , -0.2168045 ,\n",
       "           0.1935887 , -0.23777501, -0.21849114, -0.28138882, -0.13250354]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.20298335, -0.1026126 , -0.40548334, -0.00350434, -0.21214215,\n",
       "           0.19752908, -0.24221627, -0.21102272, -0.29302981, -0.17540133]],\n",
       "        dtype=float32),\n",
       "  array([[-0.20389912, -0.10051595, -0.4073302 , -0.00194352, -0.21125156,\n",
       "           0.19648224, -0.24158944, -0.21317312, -0.29289666, -0.17559105]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.21278255, -0.11284857, -0.43472943, -0.01157195, -0.20535861,\n",
       "           0.20482086, -0.24789482, -0.2059523 , -0.301606  , -0.21188475]],\n",
       "        dtype=float32),\n",
       "  array([[-0.21301965, -0.1107953 , -0.4366064 , -0.00977796, -0.20447208,\n",
       "           0.20369281, -0.2470275 , -0.20835708, -0.30097434, -0.21169288]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.22041135, -0.12155049, -0.4607853 , -0.01662655, -0.20178272,\n",
       "           0.2118499 , -0.25262836, -0.20211196, -0.3094044 , -0.24483143]],\n",
       "        dtype=float32),\n",
       "  array([[-0.22053166, -0.11969408, -0.46295664, -0.01532399, -0.19920415,\n",
       "           0.2115568 , -0.252529  , -0.20388368, -0.3086235 , -0.24430521]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.22684573, -0.12718406, -0.48462605, -0.01917115, -0.19902103,\n",
       "           0.21970282, -0.2574571 , -0.19930092, -0.31638676, -0.27382576]],\n",
       "        dtype=float32),\n",
       "  array([[-0.22798026, -0.12677534, -0.48637202, -0.01819636, -0.19606656,\n",
       "           0.22066022, -0.25826672, -0.20049068, -0.314849  , -0.2724679 ]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.2335968 , -0.13342607, -0.5059591 , -0.02084868, -0.19605131,\n",
       "           0.22991882, -0.26280594, -0.19674249, -0.32110375, -0.29873636]],\n",
       "        dtype=float32),\n",
       "  array([[-0.23389088, -0.13253373, -0.5072588 , -0.01902574, -0.19526935,\n",
       "           0.22933531, -0.26208398, -0.19852765, -0.32064396, -0.29829803]],\n",
       "        dtype=float32)),\n",
       " (array([[-0.23995203, -0.13788834, -0.5255913 , -0.02080987, -0.19481787,\n",
       "           0.23986167, -0.2672507 , -0.19460021, -0.32621825, -0.32174906]],\n",
       "        dtype=float32),\n",
       "  array([[-0.23950112, -0.1375027 , -0.5269931 , -0.01969428, -0.1938458 ,\n",
       "           0.23857413, -0.2662083 , -0.19618785, -0.32606152, -0.3223069 ]],\n",
       "        dtype=float32))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['weights'], history_off_the_shelf['weights']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[-0.11684369]], dtype=float32), array([-0.11684369], dtype=float32)),\n",
       " (array([[-0.11910512]], dtype=float32), array([-0.11937625], dtype=float32)),\n",
       " (array([[-0.12003764]], dtype=float32), array([-0.11988522], dtype=float32)),\n",
       " (array([[-0.11861362]], dtype=float32), array([-0.11849485], dtype=float32)),\n",
       " (array([[-0.11625292]], dtype=float32), array([-0.11595139], dtype=float32)),\n",
       " (array([[-0.11301926]], dtype=float32), array([-0.11235729], dtype=float32)),\n",
       " (array([[-0.10927813]], dtype=float32), array([-0.10855978], dtype=float32)),\n",
       " (array([[-0.10487188]], dtype=float32), array([-0.10431983], dtype=float32)),\n",
       " (array([[-0.09997234]], dtype=float32), array([-0.09948463], dtype=float32)),\n",
       " (array([[-0.09504167]], dtype=float32), array([-0.09434004], dtype=float32)),\n",
       " (array([[-0.08987144]], dtype=float32), array([-0.08915619], dtype=float32))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(history_custom['bias'], history_off_the_shelf['bias']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Further reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Data Collate (Further reading, not implemented here)\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "# Karpathy resource on a tiny implementation of  autodiff from scratch if anyone is interested. Engine.py is where the meat is\n",
    "# https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
