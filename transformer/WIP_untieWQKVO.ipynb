{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import math\n",
    "from typing import Callable, Iterable, Tuple\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Depth 1\n",
    "\n",
    "def construct_full_model(hparams):\n",
    "    '''\n",
    "    return: nn.Module. Take a sequence of input symbols and \n",
    "    return a sequence of output symbols\n",
    "    '''\n",
    "\n",
    "    # cloned\n",
    "    clone = copy.deepcopy\n",
    "    pff = Positiontwise_FF(d_model=hparams['d_model'], d_ff=hparams['d_ff'])\n",
    "    attn = MultiHeadAttention(\n",
    "        d_model=hparams['d_model'], \n",
    "        h=hparams['num_heads'], \n",
    "        attn_wt_dropout=hparams['attn_wt_dropout'],\n",
    "    )\n",
    "    layer_norm = LayerNorm(d_model=hparams['d_model'])\n",
    "\n",
    "    # shared\n",
    "    scaled_embed = ScaledEmbedding(hparams['vocab_size'], hparams['d_model'])\n",
    "    position_encoder = PositionEncoder(\n",
    "        d_model=hparams['d_model'], max_len=hparams['max_len']\n",
    "    )\n",
    "    embed_dropout = nn.Dropout(hparams['embed_dropout'])\n",
    "\n",
    "    # full model\n",
    "    model = EncoderDecoder(\n",
    "        encoder=Encoder(\n",
    "            encoder_layer=EncoderLayer(\n",
    "                poswise_ff=clone(pff),\n",
    "                self_attn=clone(attn), \n",
    "                layer_norm=clone(layer_norm), \n",
    "                heads_dropout=hparams['heads_dropout'],\n",
    "                pff_dropout=hparams['pff_dropout']\n",
    "            ), \n",
    "            N_layers=hparams['N_enc'],\n",
    "            d_model=hparams['d_model'],\n",
    "        ),\n",
    "        decoder=Decoder(\n",
    "            decoder_layer=DecoderLayer(\n",
    "                poswise_ff=clone(pff),\n",
    "                self_attn=clone(attn), \n",
    "                cross_attn=clone(attn), \n",
    "                layer_norm=clone(layer_norm), \n",
    "                heads_dropout=hparams['heads_dropout'],\n",
    "                pff_dropout=hparams['pff_dropout']\n",
    "            ),\n",
    "            N_layers=hparams['N_dec'],\n",
    "            d_model=hparams['d_model'],\n",
    "        ),\n",
    "        classifier=Classifier(\n",
    "            shared_embed=scaled_embed.embedding\n",
    "        ),\n",
    "        inp_layer=nn.Sequential(\n",
    "            OrderedDict([('scaled_embed', scaled_embed), \n",
    "                        ('position_encoder', position_encoder),\n",
    "                        ('embed_dropout', embed_dropout)]\n",
    "            )\n",
    "        ),\n",
    "        out_layer=nn.Sequential(\n",
    "            OrderedDict([('scaled_embed', scaled_embed), \n",
    "                        ('position_encoder', position_encoder),\n",
    "                        ('embed_dropout', embed_dropout)]\n",
    "            )\n",
    "        ),\n",
    "        pad_idx=hparams['padding_idx']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Depth 2\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, classifier, inp_layer, out_layer, pad_idx):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.inp_layer = inp_layer\n",
    "        self.out_layer = out_layer\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    # def forward(self, X, Y, inp_pads, out_pads):\n",
    "    def forward(self, X, Y):\n",
    "        '''\n",
    "        Args\n",
    "        X : a sequence of input symbols. shape(batch_size=b, inp_len)\n",
    "        Y : a sequence of output symbols. shape(batch_size=b, out_len)\n",
    "            offset to the right by one position.\n",
    "        Returns\n",
    "        y_pred : predicted probabilities. shape(batch_size=b, out_len, V)\n",
    "                should be offset by one position to be fed into decoder.\n",
    "        '''\n",
    "        # shape(batch_size=b, inp_len)\n",
    "        inp_pads = (X == self.pad_idx).int()\n",
    "        # shape(batch_size=b, out_len)\n",
    "        out_pads = (Y == self.pad_idx).int()\n",
    "\n",
    "        # shape(b, inp_len, d_model)\n",
    "        encoder_out = self.encode(X, inp_pads)\n",
    "        # shape(b, out_len, d_model)\n",
    "        decoder_out = self.decode(encoder_out, Y, inp_pads, out_pads)\n",
    "        # shape (b, out_len, V)\n",
    "        out_logits = self.classifier(decoder_out)\n",
    "\n",
    "        return out_logits\n",
    "\n",
    "    def encode(self, X, inp_pads):\n",
    "        # shape(b, inp_len, d_model)\n",
    "        inp_embed = self.inp_layer(X)\n",
    "        # shape(b, inp_len, d_model)\n",
    "        encoder_out = self.encoder(inp_embed, inp_pads)\n",
    "        return encoder_out\n",
    "\n",
    "    def decode(self, encoder_out, Y, inp_pads, out_pads):\n",
    "        # shape(b, out_len, d_model)\n",
    "        out_embed = self.out_layer(Y)\n",
    "        # shape(b, out_len, d_model)\n",
    "        decoder_out = self.decoder(encoder_out, out_embed, inp_pads, out_pads)\n",
    "        return decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Depth 3\n",
    "# http://karlstratos.com/notes/transformer17.pdf\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, N_layers, d_model):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [copy.deepcopy(encoder_layer) for _ in range(N_layers)])\n",
    "        self.N_layers = N_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, inp_embedding, inp_pads):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        inp_embedding: Input embeddings. Already position encoded.\n",
    "                        shape (batch_size=b, inp_len, d_model)\n",
    "        inp_pads: Input pads. shape (batch_size=b, inp_len). 1s are padded.\n",
    "        Returns\n",
    "        encoder_out: output from encoder stack. \n",
    "                    shape (batch_size=b, inp_len, d_model)\n",
    "        \"\"\"\n",
    "        m, inp_len, d_model = inp_embedding.shape\n",
    "\n",
    "        # Make self-attn mask\n",
    "        self_attn_mask = make_attn_mask(inp_pads, inp_pads, mask_forward=False)\n",
    "\n",
    "        # Initiate encoder output tensor\n",
    "        encoder_out = torch.empty(\n",
    "            m, self.N_layers, inp_len, self.d_model).type_as(inp_embedding)\n",
    "\n",
    "        # Loop through layers in stack\n",
    "        last_z = inp_embedding\n",
    "        for l, encoder_layer in enumerate(self.encoder_layers):\n",
    "            # shape (b, inp_len, d_model)\n",
    "            last_z, _ = encoder_layer(last_z, self_attn_mask)\n",
    "            encoder_out[:, l, :, :] = last_z\n",
    "        return encoder_out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, N_layers, d_model):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [copy.deepcopy(decoder_layer) for _ in range(N_layers)])\n",
    "        self.N_layers = N_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, encoder_out, out_embedding, \n",
    "                inp_pads, out_pads, max_decode=100):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        encoder_out: output from encoder stack.\n",
    "                    shape (batch_size=b, N_enc, inp_len, d_model)\n",
    "        out_embedding: Output embedding. Shape (batch_size=b, out_len, d_model)\n",
    "                    Already position encoded. Offset by one position.\n",
    "        inp_pads: Input pads. shape (batch_size=b, inp_len). 1s are padded.\n",
    "        out_pads: Input pads. shape (batch_size=b, out_len). 1s are padded.\n",
    "        Returns\n",
    "            decoder_out: output from decoder stack\n",
    "        \"\"\"\n",
    "        # b, N_enc, inp_len, d_model = encoder_out.shape\n",
    "        # b, out_len, d_model = out_embedding.shape\n",
    "\n",
    "        self_attn_mask = make_attn_mask(out_pads, out_pads, mask_forward=True)\n",
    "        cross_attn_mask = make_attn_mask(out_pads, inp_pads, mask_forward=False)\n",
    "\n",
    "        # Loop through layers in stack\n",
    "        last_o = out_embedding\n",
    "        for l, decoder_layer in enumerate(self.decoder_layers):\n",
    "            # shape (b, out_len, d_model)\n",
    "            last_o, _, _ = decoder_layer(\n",
    "                last_o, \n",
    "                # encoder_out[:, -1, :, :],\n",
    "                encoder_out[:, l, :, :],\n",
    "                self_attn_mask, cross_attn_mask\n",
    "            )\n",
    "        decoder_output = last_o\n",
    "        return decoder_output\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, shared_embed):\n",
    "        '''\n",
    "        shared_embed: same embedding matrix as the input and output layers.\n",
    "                    shape(V, d_model)\n",
    "        '''\n",
    "        super(Classifier, self).__init__()\n",
    "        self.shared_embed = shared_embed\n",
    "\n",
    "    def forward(self, decoder_out):\n",
    "        '''\n",
    "        decoder_out: last layer output of decoder stack. \n",
    "                 shape(batch_size=b, out_len, d_model)\n",
    "        '''\n",
    "        # shape (b, out_len, d_model) mm (d_model, V) = (b, out_len, V)\n",
    "        logits = decoder_out.matmul(self.shared_embed.weight.t())\n",
    "        # # shape (b, out_len, V) too expensive to compute everytime\n",
    "        # probs = torch.softmax(logits, dim=-1)\n",
    "        return logits #, probs\n",
    "\n",
    "\n",
    "class ScaledEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, V, d_model):\n",
    "        super(ScaledEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(V, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        '''\n",
    "        tokens: shape (batch_size=b, len)\n",
    "        '''\n",
    "        # shape (b, len, d_model)\n",
    "        embedded = self.embedding(tokens) * math.sqrt(self.d_model)\n",
    "        # if torch.max(embedded) > 2000.:\n",
    "        #     import pdb; pdb.set_trace()\n",
    "        return embedded\n",
    "\n",
    "\n",
    "class PositionEncoder(nn.Module):\n",
    "    # Alternative implementation\n",
    "    # https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb\n",
    "\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionEncoder, self).__init__()\n",
    "        # even\n",
    "        # i = 0, 2, 4, 6, ..., 510\n",
    "        # j = 0, 1, 2, 3, ..., 255\n",
    "        # PE(pos, i=2j) = sin(pos/10000**(2*j/d_model))\n",
    "        # odd\n",
    "        # i = 1, 3, 5, 7, ..., 511\n",
    "        # j = 0, 1, 2, 3, ..., 255\n",
    "        # PE(pos, i=2j+1) = cos(pos/10000**(2*j/d_model))\n",
    "        # shape(256,)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.register_buffer(\n",
    "            name=\"positional_encoding\", \n",
    "            tensor=self.make_encodings(self.d_model, self.max_len)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def make_encodings(d_model, max_len):\n",
    "        # shape (d_model/2)\n",
    "        div_terms = torch.pow(\n",
    "            10000, \n",
    "            2*torch.arange(start=0, # j = 0\n",
    "                       end=d_model/2 # j = 255\n",
    "            )/d_model\n",
    "        )\n",
    "\n",
    "        # shape (max_len,) -> shape (max_len, d_model)\n",
    "        positions = torch.arange(\n",
    "            max_len\n",
    "            ).unsqueeze(-1).expand(-1, d_model)\n",
    "\n",
    "        # shape (max_len, d_model)\n",
    "        positional_encoding = torch.empty(\n",
    "            max_len, d_model, dtype=torch.float, \n",
    "        )\n",
    "    \n",
    "        # shape (max_len, d_model/2)\n",
    "        pos_at_even_dims = positions[:, ::2]\n",
    "        # shape (max_len, d_model/2)\n",
    "        pos_at_odd_dims  = positions[:, 1::2]\n",
    "\n",
    "        # shape (max_len, d_model/2)\n",
    "        div_terms_expand = div_terms.unsqueeze(0).expand(max_len, -1)\n",
    "\n",
    "        # shape (max_len, d_model/2)\n",
    "        positional_encoding[:, ::2] = torch.sin(pos_at_even_dims / div_terms_expand)\n",
    "        # shape (max_len, d_model/2)\n",
    "        positional_encoding[:, 1::2] = torch.cos(pos_at_odd_dims / div_terms_expand)\n",
    "\n",
    "        # shape (max_len, d_model)\n",
    "        return positional_encoding\n",
    "\n",
    "    def forward(self, embedded):\n",
    "        '''\n",
    "        embedded: shape (batch_size=b, len, d_model)\n",
    "        '''\n",
    "        b, len, d_model = embedded.shape\n",
    "        assert len <= self.max_len\n",
    "        # shape (batch_size=b, len, d_model)\n",
    "        positional_encoding = (\n",
    "            self.positional_encoding[:len, :].unsqueeze(0).expand(b, -1, -1)\n",
    "        )\n",
    "        # shape (batch_size=b, len, d_model)\n",
    "        return positional_encoding + embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing positional encoder\n",
    "\n",
    "def test_positional_encoder(PositionEncoder, max_len=10, d_model=64):\n",
    "    # test positional encoding\n",
    "    b, len, d_model = 20, max_len, d_model\n",
    "    embedding = torch.randn(b, len, d_model)\n",
    "    pos_enc = PositionEncoder(d_model, 50)\n",
    "    positional_encoding = pos_enc.positional_encoding\n",
    "    plot_positional_encoding(positional_encoding)\n",
    "\n",
    "def plot_positional_encoding(positional_encoding):\n",
    "    '''\n",
    "    positional_encoding: tensor shape (len, d_model)\n",
    "    '''\n",
    "\n",
    "    tokens, dimensions = positional_encoding.shape\n",
    "  \n",
    "    pos_encoding = positional_encoding.unsqueeze(0).cpu().numpy()\n",
    "\n",
    "    print (pos_encoding.shape)\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.pcolormesh(pos_encoding[0], cmap='viridis')\n",
    "    plt.xlabel('Embedding Dimensions')\n",
    "    plt.xlim((0, dimensions))\n",
    "    plt.ylim((tokens,0))\n",
    "    plt.ylabel('Token Position')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "test_positional_encoder(PositionEncoder, d_model=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Depth 4\n",
    "# http://karlstratos.com/notes/transformer17.pdf\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    '''\n",
    "    single layer encoder\n",
    "    '''\n",
    "    def __init__(self, poswise_ff, self_attn, layer_norm,\n",
    "                heads_dropout, pff_dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.poswise_ff = poswise_ff\n",
    "        self.self_attn = self_attn\n",
    "        self.layer_norms = nn.ModuleList([copy.deepcopy(layer_norm) for _ in range(2)])\n",
    "        self.heads_dropout = nn.Dropout(heads_dropout)\n",
    "        self.pff_dropout = nn.Dropout(pff_dropout)\n",
    "\n",
    "    def forward(self, z_lm1, self_attn_mask):\n",
    "        '''\n",
    "        z_lm1 : last encoder layer activations. shape (batch_size=b, inp_len, d_model)\n",
    "        '''\n",
    "\n",
    "        # (b, inp_len, d_model)\n",
    "        z_lm1_h, self_attn_wts = self.self_attn(z_lm1, z_lm1, self_attn_mask)\n",
    "        # (b, inp_len, d_model)\n",
    "        z_lm1_h_norm = self.layer_norms[0](z_lm1 + self.heads_dropout(z_lm1_h))\n",
    "        # (b, inp_len, d_model)\n",
    "        z_lm1_ff = self.poswise_ff(z_lm1_h_norm)\n",
    "        # (b, inp_len, d_model)\n",
    "        z_l = self.layer_norms[1](z_lm1_h_norm + self.pff_dropout(z_lm1_ff))   \n",
    "        return z_l, self_attn_wts\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    '''\n",
    "    single layer decoder\n",
    "    '''\n",
    "    def __init__(self, poswise_ff, self_attn, cross_attn, \n",
    "                layer_norm, heads_dropout, pff_dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.poswise_ff = poswise_ff\n",
    "        self.self_attn = self_attn\n",
    "        self.cross_attn = cross_attn\n",
    "        self.layer_norms = nn.ModuleList([copy.deepcopy(layer_norm) for _ in range(3)])\n",
    "        self.heads_dropout = nn.Dropout(heads_dropout)\n",
    "        self.pff_dropout = nn.Dropout(pff_dropout)\n",
    "\n",
    "    def forward(self, o_lm1, encoder_l_output, self_attn_mask, cross_attn_mask):\n",
    "        '''\n",
    "        o_lm1 : last decoder layer activations. \n",
    "            shape (batch_size=b, out_len, d_model).\n",
    "        encoder_l_output : encoder output from at the same layer index. \n",
    "                       shape (batch_size=b, inp_len, d_model)\n",
    "        '''\n",
    "\n",
    "        # (b, out_len, d_model)\n",
    "        o_lm1_self_h, self_attn_wts = self.self_attn(o_lm1, o_lm1, self_attn_mask)\n",
    "        # (b, out_len, d_model)\n",
    "        o_lm1_self_h_norm = self.layer_norms[0](o_lm1 + self.heads_dropout(o_lm1_self_h))\n",
    "        # (b, out_len, d_model)\n",
    "        o_lm1_cross_h, cross_attn_wts = self.cross_attn(o_lm1_self_h_norm, encoder_l_output, cross_attn_mask)\n",
    "        # (b, out_len, d_model)\n",
    "        o_lm1_cross_h_norm = self.layer_norms[1](o_lm1_self_h_norm + self.heads_dropout(o_lm1_cross_h))\n",
    "        # (b, out_len, d_model)\n",
    "        o_lm1_ff = self.poswise_ff(o_lm1_cross_h_norm)\n",
    "        # (b, out_len, d_model)\n",
    "        o_l = self.layer_norms[2](o_lm1_cross_h_norm + self.pff_dropout(o_lm1_ff))\n",
    "        return o_l, self_attn_wts, cross_attn_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Depth 5\n",
    "# https://arxiv.org/pdf/1607.06450.pdf\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    '''Layer Normalization'''\n",
    "\n",
    "    def __init__(self, d_model, epsilon=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gain = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x : shape (m, len, d_model)\n",
    "        Returns:\n",
    "            whitened_x : shape (m, len, d_model)\n",
    "        '''\n",
    "        # (m, len, 1)\n",
    "        mu = torch.mean(x, dim=-1, keepdim=True)\n",
    "        std = torch.std(x, dim=-1, keepdim=True)\n",
    "        # (m, len, d_model)\n",
    "        whitened_x = self.gain * (x - mu / (std + self.epsilon)) + self.bias\n",
    "        return whitened_x\n",
    "\n",
    "\n",
    "class Positiontwise_FF(nn.Module):\n",
    "    '''Pointwise FeedForward / Fat-RELU'''\n",
    "\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(Positiontwise_FF, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x : shape (m, len, d_model)\n",
    "        Returns:\n",
    "            shape (m, len, d_model)\n",
    "        '''\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multihead Attention'''\n",
    "\n",
    "    def __init__(self, d_model, h, attn_wt_dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        # d_k, same as d_q, d_v\n",
    "        self.d_k = int(d_model / h)\n",
    "        \n",
    "\n",
    "        ############################################################\n",
    "        # # simply use clone\n",
    "    \n",
    "        # # shape (d_model, d_k * h = d_model)\n",
    "        # projection = nn.Linear(d_model, d_model, bias=True)\n",
    "        # # clone projection to become WQ, WK, WV, WO\n",
    "        # self.projections_QKVO = nn.ModuleList([copy.deepcopy(projection) for _ in range(4)])\n",
    "        ############################################################\n",
    "        # Untie only WQ\n",
    "\n",
    "        # template for WQ, WK, WV, WO projection matrices\n",
    "        # shape (d_model, d_k * h = d_model)\n",
    "        make_projection = lambda: nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        # shape (d_model, d_k * h = d_model)\n",
    "        shared_projection = make_projection()\n",
    "\n",
    "        # WQ independent, WKVO shared\n",
    "        projections = (\n",
    "            [make_projection()] + [copy.deepcopy(shared_projection) for _ in range(3)]\n",
    "        )\n",
    "        self.projections_QKVO = nn.ModuleList(projections)\n",
    "\n",
    "        ############################################################\n",
    "#         # Untie WQ, WK, WV, WO\n",
    "        \n",
    "#         # template for WQ, WK, WV, WO projection matrices\n",
    "#         # shape (d_model, d_k * h = d_model)\n",
    "#         make_projection = lambda: nn.Linear(d_model, d_model, bias=True)\n",
    "#         # make WQ, WK, WV, WO\n",
    "#         self.projections_QKVO = nn.ModuleList([make_projection() for _ in range(4)])\n",
    "        ############################################################\n",
    "#        # initialize WO as zeros to start training w/ identity function\n",
    "#         self.projections_QKVO[3].weight.data = self.projections_QKVO[3].weight.data * 0.0\n",
    "#         self.projections_QKVO[3].bias.data = self.projections_QKVO[3].bias.data * 0.0\n",
    "        ############################################################       \n",
    "        \n",
    "        self.attn_wt_dropout = nn.Dropout(p=attn_wt_dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, X, Y, mask):\n",
    "        '''\n",
    "        Args:\n",
    "            X : Attender. shape (batch_size=b, attender len=n, d_model)\n",
    "            Y : Attendee. shape (batch_size=b, attendee len=m, d_model)\n",
    "        Return:\n",
    "            attn_V : shape (b, n, h*d_k=d_model)\n",
    "        '''\n",
    "        b, n, d_model = X.shape\n",
    "\n",
    "        # Project X and Y to Q, K, V matrices\n",
    "        # Step 1 W(vals)\n",
    "        # XQ shape(b, n, d_k *h = d_model)\n",
    "        # YK shape(b, m, d_k *h = d_model)\n",
    "        # YV shape(b, m, d_k *h = d_model)\n",
    "        # Step 2 reshape()\n",
    "        # XQ shape(b, n, h, d_k)\n",
    "        # YK shape(b, m, h, d_k)\n",
    "        # YV shape(b, m, h, d_k)\n",
    "        # Step 3 swap axis with transpose()\n",
    "        # XQ shape(b, h, n, d_k)\n",
    "        # YK shape(b, h, m, d_k)\n",
    "        # YV shape(b, h, m, d_k)\n",
    "        XQ, YK, YV = [\n",
    "                      W(vals).reshape(b, -1, self.h, self.d_k)\n",
    "                      .transpose(1, 2) \n",
    "                      for (W, vals) in zip(self.projections_QKVO[:3], (X, Y, Y))]\n",
    "\n",
    "        # attention weighted values, attention weights\n",
    "        # shape (b, n, h, d_k), (b, h, n, m)\n",
    "        concat_V, attn = dotproduct_attention(\n",
    "            XQ, YK, YV, mask, self.attn_wt_dropout)\n",
    "        # shape (b, n, h*d_k=d_model)\n",
    "        concat_V = concat_V.reshape(b, n, -1)\n",
    "\n",
    "        # project by WO, shape (b, n, h*d_k=d_model)\n",
    "        attn_V = self.projections_QKVO[3](concat_V)\n",
    "   \n",
    "        return attn_V, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Depth 6\n",
    "\n",
    "def dotproduct_attention(Q, K, V, mask, beta_dropout, debug=False):\n",
    "    '''\n",
    "    Q: shape(batch_size=b, num heads=h, attender len=n, d_k)\n",
    "    K: shape(batch_size=b, num heads=h, attendee len=m, d_k)\n",
    "    V: shape(batch_size=b, num heads=h, attendee len=m, d_k)\n",
    "    mask: shape(batch_size=b, n, m)\n",
    "    beta_dropout: nn.Dropout().apply module\n",
    "    '''\n",
    "    b, h, n, d_k = Q.shape\n",
    "    b, h, m, d_k = K.shape\n",
    "\n",
    "    # XQ shape(b, h, n, d_k) matmul YK.T shape(b, h, d_k, m)\n",
    "    # = alpha shape (b, h, n, m)\n",
    "    alpha = torch.matmul(Q, K.transpose(-1, -2))/ math.sqrt(d_k)\n",
    "\n",
    "    # Apply mask \n",
    "    # (b, h, n, m)\n",
    "    mask_stack = mask.unsqueeze(1).expand(-1, h, -1, -1)\n",
    "    alpha_masked = torch.masked_fill(alpha, mask_stack==1, -1e4)\n",
    "\n",
    "    # normalize across attendee len m\n",
    "    # (b, h, n, m)\n",
    "    beta = beta_dropout(torch.softmax(alpha_masked, dim=-1))\n",
    "\n",
    "    # beta shape(b, h, n, m) bmm YK.T shape(b, h, m, d_k) = shape (b, h, n, d_k)\n",
    "    # transpose to (b, n, h, d_k)\n",
    "    wt_V = torch.matmul(beta, V).transpose(1, 2)\n",
    "\n",
    "    if debug:\n",
    "        print('alpha\\n', alpha)\n",
    "        print('mask_stack\\n', mask_stack)\n",
    "        print('alpha_masked\\n', alpha_masked)\n",
    "        print('beta\\n', beta)\n",
    "        print('wt_V\\n', wt_V)\n",
    "    if torch.isnan(wt_V).any():\n",
    "        import pdb; pdb.set_trace()  \n",
    "    return wt_V, beta\n",
    "\n",
    "\n",
    "def ref_attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 1, -1e9)\n",
    "    p_attn = torch.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "def test_attention(attn_fn):\n",
    "    torch.manual_seed(123)\n",
    "    b, h, n, m, d_k = 1, 2, 3, 3, 5\n",
    "    Q = torch.randn(b, h, n, d_k)\n",
    "    K = torch.randn(b, h, m, d_k)\n",
    "    V = torch.randn(b, h, m, d_k)\n",
    "\n",
    "    # print('Q\\n', Q)\n",
    "    # print('K\\n', K)\n",
    "    # print('V\\n', V)\n",
    "\n",
    "    mask = torch.zeros((b, n, m))\n",
    "    mask[:,:,1] = 1\n",
    "    mask[:,2,:1] = 1\n",
    "    print('input', mask)\n",
    "\n",
    "    my_wt_V = dotproduct_attention(Q, K, V, mask, nn.Dropout(0.0), debug=True)\n",
    "    ref_wt_V, ref_p_attn = ref_attention(Q, K, V, mask, nn.Dropout(0.0))\n",
    "  \n",
    "    print('ref wt_V\\n', ref_wt_V.transpose(1, 2))\n",
    "    print('ref wt_V\\n', ref_wt_V.shape)\n",
    "\n",
    "test_attention(dotproduct_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_attn_mask(attender_pads, attendee_pads, mask_forward=False, debug=False):\n",
    "    '''\n",
    "    Mask away attendee positions from attender.\n",
    "    Args:\n",
    "        attender_pads: shape(batch_size=b, attender len=n). 1s are pads.\n",
    "        attendee_pads: shape(batch_size=b, attender len=m). 1s are pads.\n",
    "    Return:\n",
    "        attn_mask: shape(b, n, m)\n",
    "    '''\n",
    "\n",
    "    b, n = attender_pads.shape\n",
    "    b, m = attendee_pads.shape\n",
    "\n",
    "    if mask_forward: \n",
    "        assert n == m\n",
    "        # shape (n, m)\n",
    "        try:\n",
    "            future_mask = torch.from_numpy(\n",
    "                np.triu(np.ones((n, m)), k=1)).type_as(attender_pads)\n",
    "        except:\n",
    "            import pdb; pdb.set_trace()\n",
    "        # shape (b, n, m)\n",
    "        future_mask_expanded = future_mask.unsqueeze(0).expand(b, -1, -1)\n",
    "\n",
    "    # shape(b, n, m)\n",
    "    attender_mask_expanded = attender_pads.unsqueeze(-1).expand(-1, -1, m)\n",
    "    # shape(b, n, m)\n",
    "    attendee_mask_expanded = attendee_pads.unsqueeze(1).expand(-1, n, -1)\n",
    "\n",
    "    # shape(b, n, m)\n",
    "    if mask_forward: \n",
    "        sum_mask = attender_mask_expanded + attendee_mask_expanded + future_mask\n",
    "    else:\n",
    "        sum_mask = attender_mask_expanded + attendee_mask_expanded\n",
    "    sum_mask = (sum_mask > 0).type(torch.int)\n",
    "\n",
    "    if debug:\n",
    "        if mask_forward:\n",
    "            print('future_mask\\n',future_mask)\n",
    "        print('attender_mask_expanded\\n',attender_mask_expanded)\n",
    "        print('attendee_mask_expanded\\n',attendee_mask_expanded)\n",
    "        print('sum mask\\n', sum_mask)\n",
    "\n",
    "    return sum_mask\n",
    "\n",
    "\n",
    "def test_make_attn_mask():\n",
    "    #   attender_pads = torch.tensor([[0,0,0,1,1], [0,0,1,1,1], [0,0,0,0,0]])\n",
    "    attender_pads = torch.tensor([[0,0,0,1], [0,0,0,0], [0,0,0,1]])\n",
    "    attendee_pads = torch.tensor([[0,0,0,1], [0,0,0,0], [0,0,0,1]])\n",
    "    print('Cross Attention:')\n",
    "    make_attn_mask(attender_pads, attendee_pads, mask_forward=False, debug=True)\n",
    "    print('--------------------------------------')\n",
    "    print('Self Attention:')\n",
    "    make_attn_mask(attender_pads, attender_pads, mask_forward=True, debug=True)\n",
    "\n",
    "\n",
    "test_make_attn_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothedLoss(nn.Module):\n",
    "    '''\n",
    "    KL divergence Loss with Label Smoothing and Temperature scaling\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, padding_idx, smoothing_const=0.0, temperature_const=1.0):\n",
    "        super(LabelSmoothedLoss, self).__init__()\n",
    "        self.smoothing_const = smoothing_const\n",
    "        self.temperature_const = temperature_const\n",
    "        self.K = K\n",
    "        self.padding_idx = padding_idx\n",
    "        self.KLdiv_criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.logprob = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, logits, labels, debug=False):\n",
    "        '''\n",
    "        logits: shape (batch_size=b, output_len=m, vocab_size=K)\n",
    "        labels: shape (batch_size=b, output_len=m)\n",
    "        '''\n",
    "        b, m, K = logits.shape\n",
    "\n",
    "        # Temperature Scaling\n",
    "        # shape (b*m, K)\n",
    "        scaled_logits = (logits / self.temperature_const).reshape(-1, K)\n",
    "        pred_logprobs = self.logprob(scaled_logits)\n",
    "\n",
    "        # Expand Labels to one-hot, Smooth the values\n",
    "        gt_probs_smoothed = torch.full(\n",
    "            size=(b*m, K), \n",
    "            # fill_value=self.smoothing_const / (K - 1), # more mathematicaly correct\n",
    "            fill_value=self.smoothing_const / (K - 2) #minus true and padding\n",
    "        ).type_as(logits)\n",
    "\n",
    "        gt_probs_smoothed = gt_probs_smoothed.scatter(\n",
    "            dim=-1, \n",
    "            index=labels.reshape(-1, 1), \n",
    "            value=(1. - self.smoothing_const),\n",
    "            # value=(1. - self.smoothing_const) + (self.smoothing_const / (K - 1)) # more mathematicaly correct\n",
    "        )\n",
    "        # Zero out padding idx\n",
    "        # shape (b*m, K)\n",
    "        gt_probs_smoothed[:, self.padding_idx] = 0.\n",
    "    \n",
    "        # Apply mask (e.g. if end of context is padded)\n",
    "        # shape (b*m, 1)\n",
    "        mask_ctx_pos = torch.nonzero(torch.flatten(labels) == self.padding_idx)\n",
    "        if mask_ctx_pos.dim() > 0:\n",
    "            # zero out rows for padded context positions\n",
    "            # e.g. word at position 10 is a pad, we zero out all probs for row 10\n",
    "            gt_probs_smoothed.index_fill_(\n",
    "                dim=0, index=mask_ctx_pos.squeeze(), value=0.0)\n",
    "\n",
    "        if debug:\n",
    "            print(scaled_logits)\n",
    "            print(labels_onehot)\n",
    "            print(mask_ctx_pos)\n",
    "\n",
    "        # 32-bit\n",
    "        # assert torch.all(torch.eq(torch.sum(gt_probs_smoothed, dim=-1), 1.))\n",
    "        # 16-bit\n",
    "        # assert torch.all(torch.greater(torch.sum(gt_probs_smoothed, dim=-1), 0.9998))\n",
    "        assert torch.all(torch.sum(gt_probs_smoothed, dim=-1) >  0.9998)\n",
    "\n",
    "        return self.KLdiv_criterion(input=pred_logprobs, target=gt_probs_smoothed)\n",
    "\n",
    "\n",
    "# Only Use for verification\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "\n",
    "def test_loss_function(myLoss, refLoss, smoothing_const, temperature_const):\n",
    "    K = 3\n",
    "    padding_idx = 0\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    myKL = myLoss(K, padding_idx, smoothing_const, temperature_const)\n",
    "    refKL = refLoss(K, padding_idx, smoothing_const)\n",
    "\n",
    "    labels = torch.tensor([[1, 1, 1, 2], [2, 2, 1, 1], [1, 1, 2, 1], [2, 1, 1, 1], [2, 1, 1, 2]])\n",
    "    inputs = torch.randn(5, 4, 3) * 4\n",
    "\n",
    "    ref_loss = refKL(F.log_softmax(inputs, dim=-1).reshape(-1, 3), torch.flatten(labels))\n",
    "    my_loss = myKL(inputs, labels )\n",
    "    #   if temperature_const == 1.0:\n",
    "    #     assert ref_loss == my_loss\n",
    "    print(ref_loss, )\n",
    "    print(my_loss)\n",
    "\n",
    "test_loss_function(myLoss=LabelSmoothedLoss, refLoss=LabelSmoothing, smoothing_const=0.1, temperature_const=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduledAdam(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements Adam algorithm with learning rate schedule.\n",
    "    Modified based on hugging face implementation:\n",
    "    https://huggingface.co/transformers/_modules/transformers/optimization.html#AdamW\n",
    "\n",
    "    Parameters:\n",
    "        params (:obj:`Iterable[torch.nn.parameter.Parameter]`):\n",
    "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "        d_model (:obj:`int`):\n",
    "            Dimension of embedding vector.\n",
    "        warmup_steps (:obj:`int`):\n",
    "            Steps for warmup before learning rate peaks.\n",
    "        lr (:obj:`float`, `optional`, defaults to 0.):\n",
    "            The learning rate to use.\n",
    "        betas (:obj:`Tuple[float,float]`, `optional`, defaults to (0.9, 0.999)):\n",
    "            Adam's betas parameters (b1, b2).\n",
    "        eps (:obj:`float`, `optional`, defaults to 1e-6):\n",
    "            Adam's epsilon for numerical stability.\n",
    "        weight_decay (:obj:`float`, `optional`, defaults to 0):\n",
    "            Decoupled weight decay to apply.\n",
    "        correct_bias (:obj:`bool`, `optional`, defaults to `True`):\n",
    "            Whether ot not to correct bias in Adam (for instance, in Bert TF repository they use :obj:`False`).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[torch.nn.parameter.Parameter],\n",
    "        d_model: int,\n",
    "        warmup_steps: int,\n",
    "        lr: float = 0.,\n",
    "        betas: Tuple[float, float] = (0.9, 0.98),\n",
    "        eps: float = 1e-9,\n",
    "        correct_bias: bool = True,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(\n",
    "            lr=lr, betas=betas, eps=eps, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def calc_step_size(self, step_num):\n",
    "        '''udpate lr'''\n",
    "        return (\n",
    "            self.d_model**(-0.5) * min(step_num**(-0.5), step_num * self.warmup_steps**(-1.5))\n",
    "        )\n",
    "\n",
    "    def step(self, closure: Callable = None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (:obj:`Callable`, `optional`): A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1.0 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                group[\"lr\"] = self.calc_step_size(state[\"step\"])\n",
    "                step_size = group[\"lr\"]\n",
    "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "        return loss\n",
    "\n",
    "def plot_lr(myOpt):\n",
    "    model = nn.Linear(2,3)\n",
    "    opts = [myOpt(model.parameters(), 512, 4000), \n",
    "            myOpt(model.parameters(), 512, 8000),\n",
    "            myOpt(model.parameters(), 256, 4000)]\n",
    "    plt.plot(np.arange(1, 20000), [[opt.calc_step_size(i) for opt in opts] for i in range(1, 20000)])\n",
    "    plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "\n",
    "plot_lr(LRScheduledAdam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_sequence(model, X, start_idx, pad_idx, max_len):\n",
    "    '''\n",
    "    Greedy Decoding\n",
    "    X : a sequence of input symbols. shape(batch_size=b, inp_len)\n",
    "    start_idx: int.\n",
    "    pad_idx: int.\n",
    "    max_len: int. Maximum context length for generation. Include <SOS> and <EOS>\n",
    "    '''\n",
    "\n",
    "    b, inp_len = X.shape\n",
    "    inp_pads = (X == pad_idx).int()\n",
    "\n",
    "    # encode inputs\n",
    "    # shape (b, inp_len, d_model)\n",
    "    encoded_memory = model.encode(X, inp_pads)\n",
    " \n",
    "    # shape (b, max_len) e.g. (b, 10)\n",
    "    Y = torch.ones(b, max_len).type_as(X) * pad_idx\n",
    "    Y[:,0] = start_idx\n",
    "    # shape (b, max_len)\n",
    "    out_pads = (Y == pad_idx).int()\n",
    "  \n",
    "    # generate one token at a time\n",
    "    for t in range(1, max_len): # (1 to 9)\n",
    "\n",
    "        # shape (b, t, d_model)\n",
    "        decoder_output = model.decode(encoded_memory, Y[:, :t], inp_pads, out_pads[:, :t])\n",
    "\n",
    "        # shape (b, t, V)\n",
    "        decoded_logits = model.classifier(decoder_output)\n",
    "        # decoded_probs = torch.softmax(decoded_logits, dim=-1)\n",
    "    \n",
    "        # shape (b,), (b,)\n",
    "        max_prob, max_idx = torch.max(decoded_logits[:, -1, :], dim=-1)\n",
    "    \n",
    "        # update Y, out_pads for next timestep\n",
    "        Y[:, t] = max_idx\n",
    "        out_pads[:, t] = (max_idx == pad_idx).int()\n",
    "\n",
    "    # shape (b, max_len)\n",
    "    # (1 to 9)\n",
    "    return Y[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_accuracy(start_idx, pad_idx, pred, labels):\n",
    "    '''\n",
    "    pred: shape (batch_size, max_len)\n",
    "    labels: shape (batch_size, max_len) \n",
    "    '''\n",
    "    assert not start_idx in labels, '<GO> should not be evaluated'\n",
    "\n",
    "    mask = 1 - (labels == pad_idx).int() # 0 means padded\n",
    "    tot_not_masked = torch.sum(mask)\n",
    "\n",
    "    same = (pred == labels).int() * mask\n",
    "    tot_correct = torch.sum(same)\n",
    "\n",
    "    return tot_correct * 1.0 / tot_not_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lightning Module\n",
    "# https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=UIXLW8CO-W8w\n",
    "\n",
    "class Transformer(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        self.encdec_model = construct_full_model(hparams)\n",
    "        \n",
    "        self.loss_criterion = LabelSmoothedLoss(\n",
    "            K=hparams['vocab_size'],\n",
    "            padding_idx=hparams['padding_idx'],\n",
    "            smoothing_const=hparams['smoothing_const'],\n",
    "            temperature_const=hparams['temperature_const']\n",
    "        )\n",
    "\n",
    "        self.accuracy = partial(\n",
    "            multiclass_accuracy, hparams['start_idx'], hparams['padding_idx'])\n",
    "    \n",
    "    def log_metrics(self, metrics_dict):\n",
    "        for k, v in metrics_dict.items():\n",
    "            self.log(k, v)\n",
    "\n",
    "    def generate_sequence(self, X, start_idx, pad_idx, max_len):\n",
    "        # shape (b, max_len)\n",
    "        Y = greedy_decode_sequence(self.encdec_model, X, start_idx, pad_idx, max_len)\n",
    "        return Y\n",
    "\n",
    "    def get_max_memory_alloc(self):\n",
    "        devices_max_memory_alloc = {}\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            device = torch.device(f'cuda:{i}')\n",
    "            devices_max_memory_alloc[device] = torch.cuda.max_memory_allocated(device) / 1e6\n",
    "            torch.cuda.reset_max_memory_allocated(device)\n",
    "        return devices_max_memory_alloc\n",
    "    \n",
    "    def forward(self, X, Y, student_force_acc=False):\n",
    "        '''\n",
    "        X : (b, inp_len)\n",
    "        Y : (b, out_len)\n",
    "        '''\n",
    "\n",
    "        # fwd\n",
    "        # inp X[:,:]: tensor([1, 9, 6, 7, 9, 2])\n",
    "        # out Y[:, :-1]: tensor([1, 9, 6, 7, 9])\n",
    "        logits = self.encdec_model(X[:,:], Y[:, :-1])\n",
    "        \n",
    "        # loss\n",
    "        # target Y[:, 1:]: tensor([9, 6, 7, 9, 2])\n",
    "        loss = self.loss_criterion(logits, Y[:, 1:])\n",
    "\n",
    "        # acc\n",
    "        if student_force_acc:\n",
    "            y_hat = self.generate_sequence(\n",
    "                X, \n",
    "                hparams['start_idx'], \n",
    "                hparams['padding_idx'], \n",
    "                hparams['max_len'])         \n",
    "        else:\n",
    "            _, y_hat = torch.max(logits, dim=-1)\n",
    "\n",
    "        try:\n",
    "            acc = self.accuracy(y_hat, Y[:, 1:])\n",
    "        except:\n",
    "            import pdb; pdb.set_trace()\n",
    "\n",
    "        return loss, acc, y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # (b, inp_len), (b, 1 GO + out_len) \n",
    "        X, Y = batch     \n",
    "        loss, acc, y_hat = self(X, Y)\n",
    "        # logs\n",
    "        step_metrics = {'train_loss': loss, 'train_acc':acc}\n",
    "        self.log_metrics(step_metrics)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "    \n",
    "        X, Y = batch\n",
    "        loss, acc, y_hat = self(X, Y, student_force_acc=True)\n",
    "        \n",
    "        # print sample\n",
    "        print('X:',X[0], '\\nY:',Y[0] ,'\\ninp X', X[0, :], '\\nout Y[:, :-1]', Y[0, :-1], \n",
    "              '\\ntgt Y', Y[0, 1:],'\\ny_hat', y_hat[0])\n",
    "        \n",
    "        # log\n",
    "        step_metrics = {'val_loss': loss, 'val_acc': acc}\n",
    "        devices_max_memory_alloc = self.get_max_memory_alloc()\n",
    "        for device, val in devices_max_memory_alloc.items():\n",
    "            step_metrics[f'step_max_memory_alloc_cuda:{device}'] = val\n",
    "        self.log_metrics(step_metrics)\n",
    "        return step_metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        X, Y = batch\n",
    "        loss, acc, y_hat = self(X, Y, student_force_acc=True)\n",
    "        # log\n",
    "        step_metrics = {'test_loss': loss, 'test_acc': acc}\n",
    "        # print('X:',X[0], '\\nY:',Y[0], '\\ny_hat', y_hat[0])\n",
    "        self.log_metrics(step_metrics)\n",
    "        return step_metrics\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        # log metrics\n",
    "        epoch_metrics = {\n",
    "            'avg_val_loss': avg_loss, \n",
    "            'avg_val_acc': avg_acc,\n",
    "            'encoder_pff_wt': self.encdec_model.encoder.encoder_layers[0].poswise_ff.linear1.weight[0,0],\n",
    "            'decoder_pff_wt': self.encdec_model.decoder.decoder_layers[0].poswise_ff.linear1.weight[0,0]}\n",
    "        self.log_metrics(epoch_metrics)\n",
    "        return epoch_metrics\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        # log\n",
    "        epoch_metrics = {'avg_test_loss': avg_loss, 'avg_test_acc': avg_acc}\n",
    "        self.log_metrics(epoch_metrics)\n",
    "        return epoch_metrics\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        opt = LRScheduledAdam(\n",
    "            params=self.encdec_model.parameters(),\n",
    "            d_model=self.hparams['d_model'], \n",
    "            warmup_steps=self.hparams['warmup_steps'],\n",
    "            lr=0.,\n",
    "            betas=(\n",
    "                self.hparams['adam_beta1'], self.hparams['adam_beta2']),\n",
    "            eps=self.hparams['adam_epsilon'],\n",
    "            correct_bias=True\n",
    "        )\n",
    "        # opt = torch.optim.SGD(self.encdec_model.parameters(), lr=0.001, momentum=0.9)\n",
    "        # opt = AdamW(\n",
    "        #     params=self.encdec_model.parameters(),\n",
    "        #     lr=0.,\n",
    "        #     betas=(\n",
    "        #         self.hparams['adam_beta1'], self.hparams['adam_beta2']),\n",
    "        #     eps=self.hparams['adam_epsilon'],\n",
    "        # )\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataloaders\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    '''Toy Dataset. Input copying task.'''\n",
    "\n",
    "    def __init__(self, V=20, seed=0, context_len=6, tot_size=1000):\n",
    "        super(ToyDataset, self).__init__()\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        \n",
    "#         #############################################################\n",
    "#         # completely random series\n",
    "#         self.X = np.random.randint(3, V, size=(tot_size, context_len)) # [3, 49]\n",
    "#         self.X[:, 0] = 1 # <SOS>\n",
    "#         self.X[:, -1] = 2 # <EOS>\n",
    "#         self.Y = self.X[:,:].copy()\n",
    "        #############################################################\n",
    "        # reverse order\n",
    "        self.X = np.random.randint(3, V, size=(tot_size, context_len)) # [3, 49]\n",
    "        self.X[:, 0] = 1 # <SOS>\n",
    "        self.X[:, -1] = 2 # <EOS>\n",
    "        self.Y = self.X[:,::-1].copy()        \n",
    "        self.Y[:, 0] = 1 # <SOS>\n",
    "        self.Y[:, -1] = 2 # <EOS>  \n",
    "        #############################################################\n",
    "#         # + 1 series\n",
    "#         self.X = np.empty((tot_size, context_len))\n",
    "#         self.X[:, 1] = np.random.randint(3, V - (context_len - 3), size=(tot_size, ))\n",
    "#         for i in range(2, context_len):\n",
    "#             self.X[:, i] = self.X[:, i-1] + 1\n",
    "#         self.X[:, 0] = 1 # <SOS>\n",
    "#         self.X[:, -1] = 2 # <EOS>\n",
    "#         self.Y = self.X[:,:].copy()   \n",
    "        #############################################################\n",
    "        # # sum of current X + last X\n",
    "        # self.X = np.empty((tot_size, context_len))\n",
    "        # self.X[:, 1:context_len] = np.random.randint(3, V//2+1, size=(tot_size,context_len-1))        \n",
    "        # self.X[:, 0] = 1 # <SOS>\n",
    "        # self.X[:, -1] = 2 # <EOS>\n",
    "        # self.Y = np.empty((tot_size, context_len))\n",
    "        # for i in range(1, context_len):\n",
    "        #     self.Y[:, i] = self.X[:, i-1] + self.X[:, i]\n",
    "        # self.Y[:, 0] = 1 # <SOS>\n",
    "        # self.Y[:, -1] = 2 # <EOS>          \n",
    "        #############################################################\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx, :]).long()\n",
    "        y = torch.from_numpy(self.Y[idx, :]).long()\n",
    "        return x, y\n",
    "\n",
    "toy_dataset = ToyDataset()\n",
    "print('X')\n",
    "print(toy_dataset[:10][0])\n",
    "print('Y')\n",
    "print(toy_dataset[:10][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples of datasets\n",
    "\n",
    "## replicate random\n",
    "```\n",
    "X\n",
    "(tensor([[ 1, 18,  3,  6,  6,  2],\n",
    "         [ 1,  7,  9, 15,  4,  2],\n",
    "         [ 1, 17,  8, 16, 11,  2],\n",
    "         [ 1,  8, 18, 18,  3,  2],\n",
    "         [ 1, 10,  3,  4, 12,  2],\n",
    "         [ 1,  6, 14,  5,  3,  2],\n",
    "         [ 1,  8,  9, 11, 18,  2],\n",
    "         [ 1, 13,  4,  4, 10,  2],\n",
    "         [ 1,  9, 14, 17,  3,  2],\n",
    "         [ 1, 15, 13, 14,  7,  2]]),\n",
    "Y\n",
    " tensor([[ 1, 18,  3,  6,  6,  2],\n",
    "         [ 1,  7,  9, 15,  4,  2],\n",
    "         [ 1, 17,  8, 16, 11,  2],\n",
    "         [ 1,  8, 18, 18,  3,  2],\n",
    "         [ 1, 10,  3,  4, 12,  2],\n",
    "         [ 1,  6, 14,  5,  3,  2],\n",
    "         [ 1,  8,  9, 11, 18,  2],\n",
    "         [ 1, 13,  4,  4, 10,  2],\n",
    "         [ 1,  9, 14, 17,  3,  2],\n",
    "         [ 1, 15, 13, 14,  7,  2]]))\n",
    "```\n",
    "\n",
    "\n",
    "## reversed random\n",
    "```\n",
    "(tensor([[ 1, 18,  3,  6,  6,  2],\n",
    "         [ 1,  7,  9, 15,  4,  2],\n",
    "         [ 1, 17,  8, 16, 11,  2],\n",
    "         [ 1,  8, 18, 18,  3,  2],\n",
    "         [ 1, 10,  3,  4, 12,  2],\n",
    "         [ 1,  6, 14,  5,  3,  2],\n",
    "         [ 1,  8,  9, 11, 18,  2],\n",
    "         [ 1, 13,  4,  4, 10,  2],\n",
    "         [ 1,  9, 14, 17,  3,  2],\n",
    "         [ 1, 15, 13, 14,  7,  2]]),\n",
    " tensor([[ 1,  6,  6,  3, 18,  2],\n",
    "         [ 1,  4, 15,  9,  7,  2],\n",
    "         [ 1, 11, 16,  8, 17,  2],\n",
    "         [ 1,  3, 18, 18,  8,  2],\n",
    "         [ 1, 12,  4,  3, 10,  2],\n",
    "         [ 1,  3,  5, 14,  6,  2],\n",
    "         [ 1, 18, 11,  9,  8,  2],\n",
    "         [ 1, 10,  4,  4, 13,  2],\n",
    "         [ 1,  3, 17, 14,  9,  2],\n",
    "         [ 1,  7, 14, 13, 15,  2]]))\n",
    "```\n",
    "\n",
    "\n",
    "## autoregressive +1 series\n",
    "```\n",
    "X\n",
    "(tensor([[ 1, 15, 16, 17, 18,  2],\n",
    "         [ 1,  8,  9, 10, 11,  2],\n",
    "         [ 1,  3,  4,  5,  6,  2],\n",
    "         [ 1,  6,  7,  8,  9,  2],\n",
    "         [ 1, 14, 15, 16, 17,  2],\n",
    "         [ 1,  6,  7,  8,  9,  2],\n",
    "         [ 1, 10, 11, 12, 13,  2],\n",
    "         [ 1, 12, 13, 14, 15,  2],\n",
    "         [ 1,  6,  7,  8,  9,  2],\n",
    "         [ 1,  8,  9, 10, 11,  2]]),\n",
    "Y\n",
    " tensor([[ 1, 15, 16, 17, 18,  2],\n",
    "         [ 1,  8,  9, 10, 11,  2],\n",
    "         [ 1,  3,  4,  5,  6,  2],\n",
    "         [ 1,  6,  7,  8,  9,  2],\n",
    "         [ 1, 14, 15, 16, 17,  2],\n",
    "         [ 1,  6,  7,  8,  9,  2],\n",
    "         [ 1, 10, 11, 12, 13,  2],\n",
    "         [ 1, 12, 13, 14, 15,  2],\n",
    "         [ 1,  6,  7,  8,  9,  2],\n",
    "         [ 1,  8,  9, 10, 11,  2]]))\n",
    "```\n",
    "\n",
    "\n",
    "## current of X + last of X series\n",
    "```\n",
    "X\n",
    "(tensor([[ 1,  7, 10,  8,  3,  2],\n",
    "         [ 1,  6,  6, 10,  4,  2],\n",
    "         [ 1,  8,  5,  7, 10,  2],\n",
    "         [ 1,  3,  3,  7,  5,  2],\n",
    "         [ 1,  9, 10, 10,  9,  2],\n",
    "         [ 1,  4,  8,  4,  8,  2],\n",
    "         [ 1,  4,  7,  6,  3,  2],\n",
    "         [ 1,  8,  9, 10, 10,  2],\n",
    "         [ 1,  5,  6,  3,  4,  2],\n",
    "         [ 1,  8,  6,  6,  9,  2]]),\n",
    "Y\n",
    " tensor([[ 1,  8, 17, 18, 11,  2],\n",
    "         [ 1,  7, 12, 16, 14,  2],\n",
    "         [ 1,  9, 13, 12, 17,  2],\n",
    "         [ 1,  4,  6, 10, 12,  2],\n",
    "         [ 1, 10, 19, 20, 19,  2],\n",
    "         [ 1,  5, 12, 12, 12,  2],\n",
    "         [ 1,  5, 11, 13,  9,  2],\n",
    "         [ 1,  9, 17, 19, 20,  2],\n",
    "         [ 1,  6, 11,  9,  7,  2],\n",
    "         [ 1,  9, 14, 12, 15,  2]]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataModule(pl.LightningDataModule):\n",
    "    # https://wandb.ai/cayush/pytorchlightning/reports/Use-Pytorch-Lightning-with-Weights-Biases--Vmlldzo2NjQ1Mw\n",
    "\n",
    "    def __init__(self, batch_size, V, seed, context_len, tot_size):\n",
    "        super().__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.V = V\n",
    "        self.seed = seed\n",
    "        self.context_len = context_len\n",
    "        self.tot_size = tot_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage in None:\n",
    "            self.toy_train = ToyDataset(\n",
    "                V=self.V, seed=self.seed, \n",
    "                context_len=self.context_len, tot_size=self.tot_size)\n",
    "            self.toy_val = self.toy_train            \n",
    "            # self.toy_val = ToyDataset(\n",
    "            #     V=self.V, seed=42, \n",
    "            #     context_len=self.context_len, tot_size=self.batch_size*2) \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.toy_test = self.toy_train\n",
    "            # self.toy_test = ToyDataset(\n",
    "            #     V=self.V, seed=41, \n",
    "            #     context_len=self.context_len, tot_size=self.batch_size*2)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(self.toy_train, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(self.toy_val, batch_size=self.batch_size, shuffle=False)\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(self.toy_test, batch_size=self.batch_size, shuffle=False)\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'd_model': 512,\n",
    "    'd_ff': 2048,\n",
    "    'max_len': 6,\n",
    "    'num_heads': 2,\n",
    "    'embed_dropout': 0.0,\n",
    "    'attn_wt_dropout': 0.0,\n",
    "    'heads_dropout': 0.0,\n",
    "    'pff_dropout': 0.0,\n",
    "    'N_enc': 2,\n",
    "    'N_dec': 2,\n",
    "    'start_idx': 1,\n",
    "    'padding_idx': 0,\n",
    "    'smoothing_const': 0.1,\n",
    "    'temperature_const': 1.0,\n",
    "    'adam_beta1': 0.9,\n",
    "    'adam_beta2': 0.98,\n",
    "    'adam_epsilon': 1e-9,\n",
    "    'warmup_steps': 12000,\n",
    "    'vocab_size': 21\n",
    "}\n",
    "# model\n",
    "my_transformer = Transformer(hparams)\n",
    "print(pl.core.memory.ModelSummary(my_transformer, mode='full'),'\\n')\n",
    "# data\n",
    "toy_data = ToyDataModule(batch_size=100, V=hparams['vocab_size'], seed=40, context_len=hparams['max_len'], tot_size=500)\n",
    "# logger\n",
    "wd_logger = WandbLogger(name=\"replicate-reverse-32bit-warmup12000-sinEmbed-untieWQcloneWKVO-tieAllLayers\",project='pytorchlightning_test')\n",
    "# wd_logger = WandbLogger(name=\"test-dummy\",project='pytorchlightning_test')\n",
    "# trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, min_epochs=2, max_epochs=20000,\n",
    "    logger=wd_logger,log_gpu_memory='all',\n",
    "    precision=32, amp_backend='apex', amp_level='O3')\n",
    "# fit!\n",
    "with torch.autograd.detect_anomaly():\n",
    "    trainer.fit(my_transformer, toy_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
