{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchucooleg\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torch.autograd.profiler as profiler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import apex.fp16_utils as fp16\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from utils.moduleCodeProfiler import rankByCriteria\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 22 23:52:00 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 000099D1:00:00.0 Off |                    0 |\r\n",
      "| N/A   42C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "args.data_dir = '~/datadrive'\n",
    "args.dataset_dir = 'toy_mlp_1'\n",
    "args.seed = 123\n",
    "args.batch_size = 1000\n",
    "# https://stackoverflow.com/questions/15753701/how-can-i-pass-a-list-as-a-command-line-argument-with-argparse\n",
    "args.hidden_layer_dims = [10, 10, 10, 10]\n",
    "args.lr = 0.01\n",
    "args.epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training indices [1603 8472 2213  498 1038 8399 3324 7535 1519 1959]\n",
      "X shape (9000, 10)\n",
      "y shape (9000,)\n"
     ]
    }
   ],
   "source": [
    "# construct and save toydataset\n",
    "\n",
    "m_train = 9000\n",
    "m_total = m_train\n",
    "\n",
    "X, y = make_classification(n_samples=m_total, n_features=10, n_informative=10, n_redundant=0, n_repeated=0, n_classes=5, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=args.seed)\n",
    "# y = np.expand_dims(y, -1)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "permutation = np.random.permutation(m_total)\n",
    "print('First 10 training indices', permutation[:10])\n",
    "print('X shape', X.shape)\n",
    "print('y shape', y.shape)\n",
    "\n",
    "train_indices = permutation[0:m_train]\n",
    "\n",
    "dataset_dir = 'toy_mlp_1'\n",
    "os.makedirs(os.path.join(args.data_dir, dataset_dir, 'train'), mode = 0o777, exist_ok = True) \n",
    "\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'train', 'features.npy'), X[train_indices])\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'train', 'labels.npy'), y[train_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Toy dataset construction.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Path to the directory with data files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # shape (m, nx)\n",
    "        self.X = np.load(os.path.join(data_dir, 'features.npy'))\n",
    "        # shape (m, ny=1)\n",
    "        self.y = np.load(os.path.join(data_dir, 'labels.npy'))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        else:\n",
    "            X = torch.from_numpy(self.X[idx, :]).type(torch.FloatTensor)\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "#             y = torch.from_numpy(self.y[idx, :]).type(torch.FloatTensor)\n",
    "            sample = {'X': X, 'y': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "class ToyDataModule(pl.LightningDataModule):\n",
    "    # https://wandb.ai/cayush/pytorchlightning/reports/Use-Pytorch-Lightning-with-Weights-Biases--Vmlldzo2NjQ1Mw\n",
    "\n",
    "    def __init__(self, batch_size, V, seed, context_len, tot_size):\n",
    "        super().__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.V = V\n",
    "        self.seed = seed\n",
    "        self.context_len = context_len\n",
    "        self.tot_size = tot_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage in None:\n",
    "            self.toy_train = ToyDataset(\n",
    "                V=self.V, seed=self.seed, \n",
    "                context_len=self.context_len, tot_size=self.tot_size)\n",
    "            self.toy_val = self.toy_train            \n",
    "            # self.toy_val = ToyDataset(\n",
    "            #     V=self.V, seed=42, \n",
    "            #     context_len=self.context_len, tot_size=self.batch_size*2) \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.toy_test = self.toy_train\n",
    "            # self.toy_test = ToyDataset(\n",
    "            #     V=self.V, seed=41, \n",
    "            #     context_len=self.context_len, tot_size=self.batch_size*2)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(self.toy_train, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(self.toy_val, batch_size=self.batch_size, shuffle=False)\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(self.toy_test, batch_size=self.batch_size, shuffle=False)\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLazy(nn.Module):\n",
    "\n",
    "    def __init__(self, nx, hidden_layer_dims, ny):\n",
    "        super(MLPLazy, self).__init__()\n",
    "        self.hidden_layer_dims = hidden_layer_dims\n",
    "        \n",
    "        linear_layers = []\n",
    "        last_dim = nx\n",
    "        for next_dim in hidden_layer_dims:\n",
    "            linear_layer = nn.Linear(last_dim, next_dim)\n",
    "            linear_layers.append(linear_layer)\n",
    "            last_dim = next_dim\n",
    "        # should push to ModuleList so that params stay on cuda\n",
    "        self.linear_layers = nn.ModuleList(linear_layers)\n",
    "        self.scorer = nn.Linear(last_dim, ny)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        last_X = X\n",
    "        for i, linear_layer in enumerate(self.linear_layers):\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = linear_layer(last_X)\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = torch.relu(last_X)\n",
    "        # shape (m, ny)\n",
    "        z = self.scorer(last_X)\n",
    "#         # shape (m, ny)\n",
    "#         a = torch.softmax(z, dim=1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weights_precision(model):\n",
    "    '''check weight precisions for each layer of MLP'''\n",
    "    for i, layer in enumerate(model.linear_layers):\n",
    "        print(f'layer {i}, weight dtype {layer.weight.dtype}')\n",
    "        print(f'layer {i}, bias dtype {layer.bias.dtype}')\n",
    "    print(f'scorer weight dtype {model.scorer.weight.dtype}')\n",
    "    print(f'scorer bias dtype {model.scorer.bias.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_memory_alloc():\n",
    "    '''read and reset max memory allocation'''\n",
    "    devices_max_memory_alloc = {}\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        devices_max_memory_alloc[device] = torch.cuda.max_memory_allocated(device) / 1e6\n",
    "        torch.cuda.reset_max_memory_allocated(device)\n",
    "    return devices_max_memory_alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMixedPrecision(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        # 1. Create model\n",
    "        self.model = MLPLazy(hparams['nx'], hparams['hidden_layer_dims'], hparams['ny'])\n",
    "        self.model_pgs, self.master_pgs = None, None\n",
    "        self.loss_criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.loss_scale = 50000\n",
    "        self.overflow = False\n",
    "\n",
    "    def log_metrics(self, metrics_dict):\n",
    "        for k, v in metrics_dict.items():\n",
    "            self.log(k, v)\n",
    "        \n",
    "    def get_max_memory_alloc(self):\n",
    "        devices_max_memory_alloc = {}\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            device = torch.device(f'cuda:{i}')\n",
    "            devices_max_memory_alloc[device] = torch.cuda.max_memory_allocated(device) / 1e6\n",
    "            torch.cuda.reset_max_memory_allocated(device)\n",
    "        return devices_max_memory_alloc\n",
    "    \n",
    "    def forward(self, X, Y):\n",
    "        assert X.dtype == torch.float16\n",
    "        assert Y.dtype != torch.float16 or Y.dtype != torch.long\n",
    "        \n",
    "        # 6. model forward with float16 data\n",
    "        logits = self.model(X)\n",
    "        # 7. compute loss in float16\n",
    "        loss = self.loss_criterion(logits, Y)\n",
    "        # 8. scale up loss here\n",
    "        loss *= self.loss_scale\n",
    "        # 8.1 check if loss overflow\n",
    "        self.overflow = torch.isinf(loss)\n",
    "        return loss, logits\n",
    "        \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        self.overflow = False\n",
    "        X, Y = batch\n",
    "        loss, logits = self(X, Y)\n",
    "        step_metrics = {'train_loss': loss, 'loss_scale': self.loss_scale}\n",
    "        \n",
    "        devices_max_memory_alloc = self.get_max_memory_alloc()\n",
    "        for device, val in devices_max_memory_alloc.items():\n",
    "            step_metrics[f'step_max_memory_alloc_cuda:{device}'] = val\n",
    "            \n",
    "        self.log_metrics(step_metrics)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        X, Y = batch\n",
    "        loss, logits = self(X, Y)\n",
    "        step_metrics = {'val_loss': loss}\n",
    "        self.log_metrics(step_metrics)\n",
    "        return step_metrics\n",
    "        \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        X, Y = batch\n",
    "        loss, logits = self(X, Y)\n",
    "        step_metrics = {'test_loss': loss}\n",
    "        self.log_metrics(step_metrics)        \n",
    "        return step_metrics\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        epoch_metrics = {\n",
    "            'avg_val_loss': avg_loss\n",
    "        }\n",
    "        self.log_metrics(epoch_metrics)        \n",
    "        return epoch_metrics\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        epoch_metrics = {\n",
    "            'avg_test_loss': avg_loss\n",
    "        }\n",
    "        self.log_metrics(epoch_metrics)        \n",
    "        return epoch_metrics\n",
    "\n",
    "    def check_grad(self):\n",
    "        print('optimizer grad:\\n', self.optimizers().param_groups[0]['params'][0].grad)\n",
    "        print('master pg grad:\\n', self.master_pgs[0][0].grad)\n",
    "        print('model pg grad:\\n', self.model_pgs[0][0].grad)\n",
    "\n",
    "    def check_weights(self):\n",
    "        print('optimizer weights:\\n', self.optimizers().param_groups[0]['params'][0])\n",
    "        print('master pg weights:\\n', self.master_pgs[0][0])\n",
    "        print('model pg weights:\\n', self.model_pgs[0][0])\n",
    "    \n",
    "    def get_master(self, opt):\n",
    "        '''create a float32 master copy of float16 model weights in optimizer'''\n",
    "        model_pgs = [[param for param in pg['params'] if param.requires_grad] for pg in opt.param_groups]\n",
    "        master_pgs = [[param.clone().float().detach() for param in pg] for pg in model_pgs]\n",
    "        for pg in master_pgs:\n",
    "            for param in pg: param.requires_grad_(True)\n",
    "        return model_pgs, master_pgs\n",
    "\n",
    "    def push_master_to_optimizer(self, opt, master_pgs):\n",
    "        '''\n",
    "            link master copy pgs to optimizer, \n",
    "            keeping other hparams such as lr, momentum dampening, weight_decay... etc\n",
    "        '''\n",
    "        for opt_pg, master_pg in zip(opt.param_groups, master_pgs):\n",
    "            opt_pg['params'] = master_pg\n",
    "\n",
    "    def to_master_grads(self, model_pgs, master_pgs):\n",
    "        '''copy float16 gradients from model to float32 gradients in master copy of weights'''\n",
    "        for (model_params, master_params) in zip(model_pgs,master_pgs):\n",
    "            fp16.model_grads_to_master_grads(model_params, master_params, flat_master=False)\n",
    "       \n",
    "    def scale_down_master_grad(self, master_pgs, loss_scale):\n",
    "        '''\n",
    "        scale down all gradients for all master param groups\n",
    "        '''\n",
    "        for master_params in master_pgs:\n",
    "            for param in master_params:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.div_(loss_scale)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # 2. initialize optimizer\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.hparams['lr'])\n",
    "        # 3. Cast model to float16\n",
    "        fp16.convert_network(self.model, torch.float16)\n",
    "        # 4. Create a copy of this float16 model's weight in float32 as the master copy\n",
    "        self.model_pgs, self.master_pgs = self.get_master(opt)\n",
    "        # 5. replace optimizer float16 weights with float32 master copy\n",
    "        self.push_master_to_optimizer(opt, self.master_pgs)\n",
    "        return opt\n",
    "    \n",
    "    def backward(self, loss: Tensor, optimizer: Optimizer, optimizer_idx: int, *args, **kwargs) -> None:\n",
    "        if self.trainer.train_loop.automatic_optimization or self._running_manual_backward:\n",
    "            if not self.overflow:\n",
    "                # 9. backprop to compute gradients\n",
    "                loss.backward(*args, **kwargs)\n",
    "                # 10. check if gradient overflow\n",
    "                self.overflow = any(\n",
    "                    [torch.any(torch.isinf(p.grad)).item() for pg in self.model_pgs for p in pg]\n",
    "                )\n",
    "            if self.overflow:\n",
    "                # half the loss scale\n",
    "                self.loss_scale /= 2.\n",
    "                self.model.zero_grad()\n",
    "            else:\n",
    "                # up the loss scale\n",
    "                self.loss_Scale *= 1.05\n",
    "                # 11. copy float16 gradients from model to float32 gradients in master copy of weights\n",
    "                self.to_master_grads(self.model_pgs, self.master_pgs)\n",
    "                # 12. scale down master copy gradients\n",
    "                self.scale_down_master_grad(self.master_pgs, self.loss_scale)\n",
    "                \n",
    "    def optimizer_step(\n",
    "        self,\n",
    "        *args,\n",
    "        epoch: int = None,\n",
    "        batch_idx: int = None,\n",
    "        optimizer: Optimizer = None,\n",
    "        optimizer_idx: int = None,\n",
    "        optimizer_closure: Optional[Callable] = None,\n",
    "        on_tpu: bool = None,\n",
    "        using_native_amp: bool = None,\n",
    "        using_lbfgs: bool = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        if not self.overflow:\n",
    "            # 13.optimizer step on master weights\n",
    "            optimizer.step(closure=optimizer_closure, *args, **kwargs)\n",
    "            # 14.zero out gradients in model\n",
    "            self.model.zero_grad()\n",
    "            # 15.copy float32 master weights to float16 model weights\n",
    "            self.to_model_params(self.model_pgs, self.master_pgs)\n",
    "            \n",
    "        \n",
    "#     def optimizer_zero_grad(\n",
    "#         self, epoch: int, batch_idx: int, optimizer: Optimizer, optimizer_idx: int\n",
    "#     ):\n",
    "#         optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 22 22:43:38 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 000099D1:00:00.0 Off |                    0 |\r\n",
      "| N/A   54C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model weights at init\n",
      "layer 0, weight dtype torch.float32\n",
      "layer 0, bias dtype torch.float32\n",
      "layer 1, weight dtype torch.float32\n",
      "layer 1, bias dtype torch.float32\n",
      "layer 2, weight dtype torch.float32\n",
      "layer 2, bias dtype torch.float32\n",
      "layer 3, weight dtype torch.float32\n",
      "layer 3, bias dtype torch.float32\n",
      "scorer weight dtype torch.float32\n",
      "scorer bias dtype torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "history, model = main_train(args, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model weights at init\n",
      "layer 0, weight dtype torch.float32\n",
      "layer 0, bias dtype torch.float32\n",
      "layer 1, weight dtype torch.float32\n",
      "layer 1, bias dtype torch.float32\n",
      "layer 2, weight dtype torch.float32\n",
      "layer 2, bias dtype torch.float32\n",
      "layer 3, weight dtype torch.float32\n",
      "layer 3, bias dtype torch.float32\n",
      "scorer weight dtype torch.float32\n",
      "scorer bias dtype torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "with profiler.profile(profile_memory=True, record_shapes=True, use_cuda=False, with_stack=True) as prof:\n",
    "    with profiler.record_function(\"forward\"):\n",
    "        history, model = main_train(args, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_losses': [1.6131540536880493,\n",
       "  1.6128689050674438,\n",
       "  1.6125844717025757,\n",
       "  1.6123093366622925,\n",
       "  1.6120353937149048,\n",
       "  1.6117639541625977,\n",
       "  1.6114981174468994,\n",
       "  1.6112393140792847,\n",
       "  1.610988736152649,\n",
       "  1.610745906829834,\n",
       "  1.6105149984359741,\n",
       "  1.6102882623672485,\n",
       "  1.610068678855896,\n",
       "  1.6098577976226807,\n",
       "  1.6096543073654175,\n",
       "  1.60945463180542,\n",
       "  1.6092621088027954,\n",
       "  1.6090692281723022,\n",
       "  1.6088793277740479,\n",
       "  1.6086912155151367],\n",
       " 'max_memory_allocation': [{device(type='cuda', index=0): 0.005632},\n",
       "  {device(type='cuda', index=0): 0.325632},\n",
       "  {device(type='cuda', index=0): 0.330752},\n",
       "  {device(type='cuda', index=0): 0.330752},\n",
       "  {device(type='cuda', index=0): 0.330752},\n",
       "  {device(type='cuda', index=0): 0.330752},\n",
       "  {device(type='cuda', index=0): 0.330752},\n",
       "  {device(type='cuda', index=0): 0.330752},\n",
       "  {device(type='cuda', index=0): 0.330752},\n",
       "  {device(type='cuda', index=0): 0.330752},\n",
       "  {device(type='cuda', index=0): 0.331264},\n",
       "  {device(type='cuda', index=0): 0.331264},\n",
       "  {device(type='cuda', index=0): 0.331264},\n",
       "  {device(type='cuda', index=0): 0.331264},\n",
       "  {device(type='cuda', index=0): 0.331264},\n",
       "  {device(type='cuda', index=0): 0.331264},\n",
       "  {device(type='cuda', index=0): 0.331264},\n",
       "  {device(type='cuda', index=0): 0.331264},\n",
       "  {device(type='cuda', index=0): 0.331264},\n",
       "  {device(type='cuda', index=0): 0.331776},\n",
       "  {device(type='cuda', index=0): 0.331776},\n",
       "  {device(type='cuda', index=0): 0.331776},\n",
       "  {device(type='cuda', index=0): 0.331776},\n",
       "  {device(type='cuda', index=0): 0.331776},\n",
       "  {device(type='cuda', index=0): 0.331776},\n",
       "  {device(type='cuda', index=0): 0.331776},\n",
       "  {device(type='cuda', index=0): 0.331776},\n",
       "  {device(type='cuda', index=0): 0.331776},\n",
       "  {device(type='cuda', index=0): 0.332288},\n",
       "  {device(type='cuda', index=0): 0.332288},\n",
       "  {device(type='cuda', index=0): 0.332288},\n",
       "  {device(type='cuda', index=0): 0.332288},\n",
       "  {device(type='cuda', index=0): 0.332288},\n",
       "  {device(type='cuda', index=0): 0.332288},\n",
       "  {device(type='cuda', index=0): 0.332288},\n",
       "  {device(type='cuda', index=0): 0.332288},\n",
       "  {device(type='cuda', index=0): 0.332288},\n",
       "  {device(type='cuda', index=0): 0.3328},\n",
       "  {device(type='cuda', index=0): 0.3328},\n",
       "  {device(type='cuda', index=0): 0.3328},\n",
       "  {device(type='cuda', index=0): 0.3328},\n",
       "  {device(type='cuda', index=0): 0.3328},\n",
       "  {device(type='cuda', index=0): 0.3328},\n",
       "  {device(type='cuda', index=0): 0.3328},\n",
       "  {device(type='cuda', index=0): 0.3328},\n",
       "  {device(type='cuda', index=0): 0.3328},\n",
       "  {device(type='cuda', index=0): 0.333312},\n",
       "  {device(type='cuda', index=0): 0.333312},\n",
       "  {device(type='cuda', index=0): 0.333312},\n",
       "  {device(type='cuda', index=0): 0.333312},\n",
       "  {device(type='cuda', index=0): 0.333312},\n",
       "  {device(type='cuda', index=0): 0.333312},\n",
       "  {device(type='cuda', index=0): 0.333312},\n",
       "  {device(type='cuda', index=0): 0.333312},\n",
       "  {device(type='cuda', index=0): 0.333312},\n",
       "  {device(type='cuda', index=0): 0.333824},\n",
       "  {device(type='cuda', index=0): 0.333824},\n",
       "  {device(type='cuda', index=0): 0.333824},\n",
       "  {device(type='cuda', index=0): 0.333824},\n",
       "  {device(type='cuda', index=0): 0.333824},\n",
       "  {device(type='cuda', index=0): 0.333824},\n",
       "  {device(type='cuda', index=0): 0.333824},\n",
       "  {device(type='cuda', index=0): 0.333824},\n",
       "  {device(type='cuda', index=0): 0.333824},\n",
       "  {device(type='cuda', index=0): 0.334336},\n",
       "  {device(type='cuda', index=0): 0.334336},\n",
       "  {device(type='cuda', index=0): 0.334336},\n",
       "  {device(type='cuda', index=0): 0.334336},\n",
       "  {device(type='cuda', index=0): 0.334336},\n",
       "  {device(type='cuda', index=0): 0.334336},\n",
       "  {device(type='cuda', index=0): 0.334336},\n",
       "  {device(type='cuda', index=0): 0.334336},\n",
       "  {device(type='cuda', index=0): 0.334336},\n",
       "  {device(type='cuda', index=0): 0.334848},\n",
       "  {device(type='cuda', index=0): 0.334848},\n",
       "  {device(type='cuda', index=0): 0.334848},\n",
       "  {device(type='cuda', index=0): 0.334848},\n",
       "  {device(type='cuda', index=0): 0.334848},\n",
       "  {device(type='cuda', index=0): 0.334848},\n",
       "  {device(type='cuda', index=0): 0.334848},\n",
       "  {device(type='cuda', index=0): 0.334848},\n",
       "  {device(type='cuda', index=0): 0.334848},\n",
       "  {device(type='cuda', index=0): 0.33536},\n",
       "  {device(type='cuda', index=0): 0.33536},\n",
       "  {device(type='cuda', index=0): 0.33536},\n",
       "  {device(type='cuda', index=0): 0.33536},\n",
       "  {device(type='cuda', index=0): 0.33536},\n",
       "  {device(type='cuda', index=0): 0.33536},\n",
       "  {device(type='cuda', index=0): 0.33536},\n",
       "  {device(type='cuda', index=0): 0.33536},\n",
       "  {device(type='cuda', index=0): 0.33536},\n",
       "  {device(type='cuda', index=0): 0.335872},\n",
       "  {device(type='cuda', index=0): 0.335872},\n",
       "  {device(type='cuda', index=0): 0.335872},\n",
       "  {device(type='cuda', index=0): 0.335872},\n",
       "  {device(type='cuda', index=0): 0.335872},\n",
       "  {device(type='cuda', index=0): 0.335872},\n",
       "  {device(type='cuda', index=0): 0.335872},\n",
       "  {device(type='cuda', index=0): 0.335872},\n",
       "  {device(type='cuda', index=0): 0.335872},\n",
       "  {device(type='cuda', index=0): 0.336384},\n",
       "  {device(type='cuda', index=0): 0.336384},\n",
       "  {device(type='cuda', index=0): 0.336384},\n",
       "  {device(type='cuda', index=0): 0.336384},\n",
       "  {device(type='cuda', index=0): 0.336384},\n",
       "  {device(type='cuda', index=0): 0.336384},\n",
       "  {device(type='cuda', index=0): 0.336384},\n",
       "  {device(type='cuda', index=0): 0.336384},\n",
       "  {device(type='cuda', index=0): 0.336384},\n",
       "  {device(type='cuda', index=0): 0.336896},\n",
       "  {device(type='cuda', index=0): 0.336896},\n",
       "  {device(type='cuda', index=0): 0.336896},\n",
       "  {device(type='cuda', index=0): 0.336896},\n",
       "  {device(type='cuda', index=0): 0.336896},\n",
       "  {device(type='cuda', index=0): 0.336896},\n",
       "  {device(type='cuda', index=0): 0.336896},\n",
       "  {device(type='cuda', index=0): 0.336896},\n",
       "  {device(type='cuda', index=0): 0.336896},\n",
       "  {device(type='cuda', index=0): 0.337408},\n",
       "  {device(type='cuda', index=0): 0.337408},\n",
       "  {device(type='cuda', index=0): 0.337408},\n",
       "  {device(type='cuda', index=0): 0.337408},\n",
       "  {device(type='cuda', index=0): 0.337408},\n",
       "  {device(type='cuda', index=0): 0.337408},\n",
       "  {device(type='cuda', index=0): 0.337408},\n",
       "  {device(type='cuda', index=0): 0.337408},\n",
       "  {device(type='cuda', index=0): 0.337408},\n",
       "  {device(type='cuda', index=0): 0.33792},\n",
       "  {device(type='cuda', index=0): 0.33792},\n",
       "  {device(type='cuda', index=0): 0.33792},\n",
       "  {device(type='cuda', index=0): 0.33792},\n",
       "  {device(type='cuda', index=0): 0.33792},\n",
       "  {device(type='cuda', index=0): 0.33792},\n",
       "  {device(type='cuda', index=0): 0.33792},\n",
       "  {device(type='cuda', index=0): 0.33792},\n",
       "  {device(type='cuda', index=0): 0.33792},\n",
       "  {device(type='cuda', index=0): 0.338432},\n",
       "  {device(type='cuda', index=0): 0.338432},\n",
       "  {device(type='cuda', index=0): 0.338432},\n",
       "  {device(type='cuda', index=0): 0.338432},\n",
       "  {device(type='cuda', index=0): 0.338432},\n",
       "  {device(type='cuda', index=0): 0.338432},\n",
       "  {device(type='cuda', index=0): 0.338432},\n",
       "  {device(type='cuda', index=0): 0.338432},\n",
       "  {device(type='cuda', index=0): 0.338432},\n",
       "  {device(type='cuda', index=0): 0.338944},\n",
       "  {device(type='cuda', index=0): 0.338944},\n",
       "  {device(type='cuda', index=0): 0.338944},\n",
       "  {device(type='cuda', index=0): 0.338944},\n",
       "  {device(type='cuda', index=0): 0.338944},\n",
       "  {device(type='cuda', index=0): 0.338944},\n",
       "  {device(type='cuda', index=0): 0.338944},\n",
       "  {device(type='cuda', index=0): 0.338944},\n",
       "  {device(type='cuda', index=0): 0.338944},\n",
       "  {device(type='cuda', index=0): 0.339456},\n",
       "  {device(type='cuda', index=0): 0.339456},\n",
       "  {device(type='cuda', index=0): 0.339456},\n",
       "  {device(type='cuda', index=0): 0.339456},\n",
       "  {device(type='cuda', index=0): 0.339456},\n",
       "  {device(type='cuda', index=0): 0.339456},\n",
       "  {device(type='cuda', index=0): 0.339456},\n",
       "  {device(type='cuda', index=0): 0.339456},\n",
       "  {device(type='cuda', index=0): 0.339456},\n",
       "  {device(type='cuda', index=0): 0.339968},\n",
       "  {device(type='cuda', index=0): 0.339968},\n",
       "  {device(type='cuda', index=0): 0.339968},\n",
       "  {device(type='cuda', index=0): 0.339968},\n",
       "  {device(type='cuda', index=0): 0.339968},\n",
       "  {device(type='cuda', index=0): 0.339968},\n",
       "  {device(type='cuda', index=0): 0.339968},\n",
       "  {device(type='cuda', index=0): 0.339968},\n",
       "  {device(type='cuda', index=0): 0.339968},\n",
       "  {device(type='cuda', index=0): 0.34048},\n",
       "  {device(type='cuda', index=0): 0.34048},\n",
       "  {device(type='cuda', index=0): 0.34048},\n",
       "  {device(type='cuda', index=0): 0.34048},\n",
       "  {device(type='cuda', index=0): 0.34048},\n",
       "  {device(type='cuda', index=0): 0.34048},\n",
       "  {device(type='cuda', index=0): 0.34048},\n",
       "  {device(type='cuda', index=0): 0.34048}],\n",
       " 'loss_scales': [65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0,\n",
       "  65536.0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ran without profiler\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0,\n",
       " 65536.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['loss_scales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fac1b71fd50>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(history['loss_scales'])), np.array(history['loss_scales']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked by cuda_memory_usage\n",
      "\n",
      "29.57 Mb\n",
      "##############################################\n",
      "model, aten::addmm, forward, (24) last_X = linear_layer(last_X)\n",
      "3.23 Mb\n",
      "##############################################\n",
      "model.scorer, aten::addmm, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::addmm, forward, (24) last_X = linear_layer(last_X)\n",
      "3.23 Mb\n",
      "##############################################\n",
      "model, aten::empty, forward, (26) last_X = torch.relu(last_X)\n",
      "2.81 Mb\n",
      "##############################################\n",
      "model, aten::resize_, forward, (24) last_X = linear_layer(last_X)\n",
      "1.41 Mb\n",
      "##############################################\n",
      "model.scorer, aten::resize_, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::resize_, forward, (24) last_X = linear_layer(last_X)\n",
      "1.41 Mb\n",
      "##############################################\n",
      "model, aten::relu, forward, (26) last_X = torch.relu(last_X)\n",
      "1.41 Mb\n",
      "##############################################\n",
      "model, aten::threshold, forward, (26) last_X = torch.relu(last_X)\n",
      "1.41 Mb\n",
      "##############################################\n",
      "model, ReluBackward0, forward, (26) last_X = torch.relu(last_X)\n",
      "1.41 Mb\n",
      "##############################################\n",
      "model, aten::threshold_backward, forward, (26) last_X = torch.relu(last_X)\n",
      "1.41 Mb\n",
      "##############################################\n",
      "model, aten::empty, forward, (24) last_X = linear_layer(last_X)\n",
      "1.09 Mb\n",
      "##############################################\n",
      "model.scorer, aten::empty, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::empty, forward, (24) last_X = linear_layer(last_X)\n",
      "1.09 Mb\n",
      "##############################################\n",
      "model, AddmmBackward, forward, (24) last_X = linear_layer(last_X)\n",
      "1.09 Mb\n",
      "##############################################\n",
      "model.scorer, AddmmBackward, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, AddmmBackward, forward, (24) last_X = linear_layer(last_X)\n",
      "1.09 Mb\n",
      "##############################################\n",
      "model, aten::mm, forward, (24) last_X = linear_layer(last_X)\n",
      "1.09 Mb\n",
      "##############################################\n",
      "model.scorer, aten::mm, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::mm, forward, (24) last_X = linear_layer(last_X)\n",
      "1.09 Mb\n",
      "##############################################\n",
      "model, aten::softmax, forward, (30) a = torch.softmax(z, dim=1)\n",
      "720.00 Kb\n",
      "##############################################\n",
      "model, aten::to, forward, (24) last_X = linear_layer(last_X)\n",
      "504.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::to, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::to, forward, (24) last_X = linear_layer(last_X)\n",
      "504.00 Kb\n",
      "##############################################\n",
      "model, aten::empty_strided, forward, (24) last_X = linear_layer(last_X)\n",
      "504.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::empty_strided, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::empty_strided, forward, (24) last_X = linear_layer(last_X)\n",
      "504.00 Kb\n",
      "##############################################\n",
      "model, aten::addmm, forward, (28) z = self.scorer(last_X)\n",
      "378.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::addmm, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::addmm, forward, (28) z = self.scorer(last_X)\n",
      "378.00 Kb\n",
      "##############################################\n",
      "model, aten::empty, forward, (28) z = self.scorer(last_X)\n",
      "369.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::empty, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::empty, forward, (28) z = self.scorer(last_X)\n",
      "369.00 Kb\n",
      "##############################################\n",
      "model, AddmmBackward, forward, (28) z = self.scorer(last_X)\n",
      "369.00 Kb\n",
      "##############################################\n",
      "model.scorer, AddmmBackward, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, AddmmBackward, forward, (28) z = self.scorer(last_X)\n",
      "369.00 Kb\n",
      "##############################################\n",
      "model, aten::mm, forward, (28) z = self.scorer(last_X)\n",
      "369.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::mm, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::mm, forward, (28) z = self.scorer(last_X)\n",
      "369.00 Kb\n",
      "##############################################\n",
      "model, aten::_softmax, forward, (30) a = torch.softmax(z, dim=1)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model, aten::empty_like, forward, (30) a = torch.softmax(z, dim=1)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model, aten::empty, forward, (30) a = torch.softmax(z, dim=1)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model, aten::resize_, forward, (28) z = self.scorer(last_X)\n",
      "180.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::resize_, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::resize_, forward, (28) z = self.scorer(last_X)\n",
      "180.00 Kb\n",
      "##############################################\n",
      "model, torch::autograd::CopyBackwards, forward, (24) last_X = linear_layer(last_X)\n",
      "72.00 Kb\n",
      "##############################################\n",
      "model.scorer, torch::autograd::CopyBackwards, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, torch::autograd::CopyBackwards, forward, (24) last_X = linear_layer(last_X)\n",
      "72.00 Kb\n",
      "##############################################\n",
      "model, aten::to, forward, (28) z = self.scorer(last_X)\n",
      "36.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::to, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::to, forward, (28) z = self.scorer(last_X)\n",
      "36.00 Kb\n",
      "##############################################\n",
      "model, aten::empty_strided, forward, (28) z = self.scorer(last_X)\n",
      "36.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::empty_strided, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::empty_strided, forward, (28) z = self.scorer(last_X)\n",
      "36.00 Kb\n",
      "##############################################\n",
      "model, torch::autograd::CopyBackwards, forward, (28) z = self.scorer(last_X)\n",
      "18.00 Kb\n",
      "##############################################\n",
      "model.scorer, torch::autograd::CopyBackwards, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, torch::autograd::CopyBackwards, forward, (28) z = self.scorer(last_X)\n",
      "18.00 Kb\n",
      "##############################################\n",
      "model, aten::t, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::t, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::t, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::transpose, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::transpose, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::transpose, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::as_strided, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::as_strided, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::as_strided, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::is_floating_point, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::is_floating_point, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::is_floating_point, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::copy_, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::copy_, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::copy_, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::expand, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::expand, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::expand, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::stride, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::stride, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::stride, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::t, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::t, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::t, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::transpose, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::transpose, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::transpose, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::as_strided, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::as_strided, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::as_strided, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::is_floating_point, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::is_floating_point, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::is_floating_point, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::copy_, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::copy_, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::copy_, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::expand, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::expand, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::expand, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::stride, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::stride, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::stride, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::is_floating_point, forward, (30) a = torch.softmax(z, dim=1)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::contiguous, forward, (30) a = torch.softmax(z, dim=1)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, TBackward, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, TBackward, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, TBackward, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, TBackward, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, TBackward, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, TBackward, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n"
     ]
    }
   ],
   "source": [
    "rankByCriteria(prof, model, criteria='cuda_memory_usage', per_thread=False, per_inp_shapes=False, include_external=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nvidia Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 22 22:43:11 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 000099D1:00:00.0 Off |                    0 |\r\n",
      "| N/A   56C    P0    58W / 149W |    338MiB / 11441MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     16624      C   ...s/py37_pytorch/bin/python      335MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Ran without profiler\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   10240 B  |  340480 B  |   88046 KB |   88036 KB |\n",
      "|       from large pool |       0 B  |       0 B  |       0 KB |       0 KB |\n",
      "|       from small pool |   10240 B  |  340480 B  |   88046 KB |   88036 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   10240 B  |  340480 B  |   88046 KB |   88036 KB |\n",
      "|       from large pool |       0 B  |       0 B  |       0 KB |       0 KB |\n",
      "|       from small pool |   10240 B  |  340480 B  |   88046 KB |   88036 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2038 KB |    2038 KB |   90083 KB |   88045 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |    2038 KB |    2038 KB |   90083 KB |   88045 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      20    |      69    |   12112    |   12092    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      20    |      69    |   12112    |   12092    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      20    |      69    |   12112    |   12092    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      20    |      69    |   12112    |   12092    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       3    |      10    |    6424    |    6421    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       3    |      10    |    6424    |    6421    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ran without profiler\n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
