{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    "\n",
    "from utils.moduleCodeProfiler import rankByCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 22 17:34:45 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 000047DD:00:00.0 Off |                    0 |\r\n",
      "| N/A   64C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "args.data_dir = '~/datadrive'\n",
    "args.dataset_dir = 'toy_mlp_1'\n",
    "args.seed = 123\n",
    "args.batch_size = 1000\n",
    "# https://stackoverflow.com/questions/15753701/how-can-i-pass-a-list-as-a-command-line-argument-with-argparse\n",
    "args.hidden_layer_dims = [10, 10, 10, 10]\n",
    "args.lr = 0.01\n",
    "args.epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training indices [1603 8472 2213  498 1038 8399 3324 7535 1519 1959]\n",
      "X shape (9000, 10)\n",
      "y shape (9000,)\n"
     ]
    }
   ],
   "source": [
    "# construct and save toydataset\n",
    "\n",
    "m_train = 9000\n",
    "m_total = m_train\n",
    "\n",
    "X, y = make_classification(n_samples=m_total, n_features=10, n_informative=10, n_redundant=0, n_repeated=0, n_classes=5, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=args.seed)\n",
    "# y = np.expand_dims(y, -1)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "permutation = np.random.permutation(m_total)\n",
    "print('First 10 training indices', permutation[:10])\n",
    "print('X shape', X.shape)\n",
    "print('y shape', y.shape)\n",
    "\n",
    "train_indices = permutation[0:m_train]\n",
    "\n",
    "dataset_dir = 'toy_mlp_1'\n",
    "os.makedirs(os.path.join(args.data_dir, dataset_dir, 'train'), mode = 0o777, exist_ok = True) \n",
    "\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'train', 'features.npy'), X[train_indices])\n",
    "np.save(os.path.join(args.data_dir, dataset_dir, 'train', 'labels.npy'), y[train_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Toy dataset construction.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Path to the directory with data files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # shape (m, nx)\n",
    "        self.X = np.load(os.path.join(data_dir, 'features.npy'))\n",
    "        # shape (m, ny=1)\n",
    "        self.y = np.load(os.path.join(data_dir, 'labels.npy'))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        else:\n",
    "            X = torch.from_numpy(self.X[idx, :]).type(torch.FloatTensor)\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "#             y = torch.from_numpy(self.y[idx, :]).type(torch.FloatTensor)\n",
    "            sample = {'X': X, 'y': y}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLazy(nn.Module):\n",
    "\n",
    "    def __init__(self, nx, hidden_layer_dims, ny):\n",
    "        super(MLPLazy, self).__init__()\n",
    "        self.hidden_layer_dims = hidden_layer_dims\n",
    "        \n",
    "        linear_layers = []\n",
    "        last_dim = nx\n",
    "        for next_dim in hidden_layer_dims:\n",
    "            linear_layer = nn.Linear(last_dim, next_dim)\n",
    "            linear_layers.append(linear_layer)\n",
    "            last_dim = next_dim\n",
    "        # should push to ModuleList so that params stay on cuda\n",
    "        self.linear_layers = nn.ModuleList(linear_layers)\n",
    "        self.scorer = nn.Linear(last_dim, ny)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        last_X = X\n",
    "        for i, linear_layer in enumerate(self.linear_layers):\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = linear_layer(last_X)\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = torch.relu(last_X)\n",
    "        # shape (m, ny)\n",
    "        z = self.scorer(last_X)\n",
    "        # shape (m, ny)\n",
    "        a = torch.softmax(z, dim=1)\n",
    "        return z, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weights_precision(model):\n",
    "    '''specific to checking MLP'''\n",
    "    for i, layer in enumerate(model.linear_layers):\n",
    "        print(f'layer {i}, weight dtype {layer.weight.dtype}')\n",
    "        print(f'layer {i}, bias dtype {layer.bias.dtype}')\n",
    "    print(f'scorer weight dtype {model.scorer.weight.dtype}')\n",
    "    print(f'scorer bias dtype {model.scorer.bias.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_memory_alloc():\n",
    "    devices_max_memory_alloc = {}\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        devices_max_memory_alloc[device] = torch.cuda.max_memory_allocated(device) / 1e6\n",
    "        torch.cuda.reset_max_memory_allocated(device)\n",
    "    return devices_max_memory_alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(args, gpu=0, debug=False):\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    \n",
    "    ################################################################\n",
    "    # load datasets\n",
    "    training_set = ToyDataset(data_dir=os.path.join(args.data_dir, args.dataset_dir, 'train'))\n",
    "    training_generator = torch.utils.data.DataLoader(dataset=training_set, \n",
    "                                                        batch_size=args.batch_size, \n",
    "                                                        shuffle=True, \n",
    "                                                        num_workers=0, \n",
    "                                                        pin_memory=True)\n",
    "\n",
    "    nx = training_set.X.shape[1]\n",
    "    ny = max(training_set.y) + 1\n",
    "    ################################################################\n",
    "\n",
    "    model = MLPLazy(nx, args.hidden_layer_dims, ny)  # single\n",
    "    loss_criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model.to(device=gpu)    \n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=args.lr)  # half\n",
    "    if debug:\n",
    "        print('\\nmodel weights at init')\n",
    "        check_weights_precision(model)\n",
    "\n",
    "    history = {'train_losses': [], 'max_memory_allocation':[]}\n",
    "\n",
    "    for e in range(2):\n",
    "        model.train()\n",
    "        sum_batch_losses = torch.tensor([0.], dtype=torch.float, device=gpu)\n",
    "        batch_max_memory_alloc = []\n",
    "        for batch_i, batch_data in enumerate(training_generator):\n",
    "            batch_max_memory_alloc.append(get_max_memory_alloc())\n",
    "\n",
    "            batch_X = batch_data['X'].cuda(gpu, non_blocking=True) # single\n",
    "            batch_y = batch_data['y'].cuda(gpu, non_blocking=True) # long\n",
    "            logits, activations = model(batch_X) # single\n",
    "\n",
    "            loss = loss_criterion(logits, batch_y)  # single\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()  # single\n",
    "            opt.step()\n",
    "                            \n",
    "            sum_batch_losses += loss\n",
    "\n",
    "        num_batches = batch_i + 1.\n",
    "        history['train_losses'].append(sum_batch_losses/num_batches)\n",
    "        history['max_memory_allocation'] += batch_max_memory_alloc\n",
    "    \n",
    "    itemize = lambda x: [tensor_val.item() for tensor_val in x]\n",
    "    history['train_losses'] = itemize(history['train_losses'])    \n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 22 17:34:48 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 000047DD:00:00.0 Off |                    0 |\r\n",
      "| N/A   63C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model weights at init\n",
      "layer 0, weight dtype torch.float32\n",
      "layer 0, bias dtype torch.float32\n",
      "layer 1, weight dtype torch.float32\n",
      "layer 1, bias dtype torch.float32\n",
      "layer 2, weight dtype torch.float32\n",
      "layer 2, bias dtype torch.float32\n",
      "layer 3, weight dtype torch.float32\n",
      "layer 3, bias dtype torch.float32\n",
      "scorer weight dtype torch.float32\n",
      "scorer bias dtype torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "with profiler.profile(profile_memory=True, record_shapes=True, use_cuda=False, with_stack=True) as prof:\n",
    "    with profiler.record_function(\"forward\"):\n",
    "        history, model = main_train(args, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked by cuda_memory_usage\n",
      "\n",
      "43.98 Mb\n",
      "##############################################\n",
      "model, aten::empty, forward, (26) last_X = torch.relu(last_X)\n",
      "5.55 Mb\n",
      "##############################################\n",
      "model, aten::addmm, forward, (24) last_X = linear_layer(last_X)\n",
      "2.78 Mb\n",
      "##############################################\n",
      "model.scorer, aten::addmm, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::addmm, forward, (24) last_X = linear_layer(last_X)\n",
      "2.78 Mb\n",
      "##############################################\n",
      "model, aten::resize_, forward, (24) last_X = linear_layer(last_X)\n",
      "2.78 Mb\n",
      "##############################################\n",
      "model.scorer, aten::resize_, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::resize_, forward, (24) last_X = linear_layer(last_X)\n",
      "2.78 Mb\n",
      "##############################################\n",
      "model, aten::relu, forward, (26) last_X = torch.relu(last_X)\n",
      "2.78 Mb\n",
      "##############################################\n",
      "model, aten::threshold, forward, (26) last_X = torch.relu(last_X)\n",
      "2.78 Mb\n",
      "##############################################\n",
      "model, ReluBackward0, forward, (26) last_X = torch.relu(last_X)\n",
      "2.78 Mb\n",
      "##############################################\n",
      "model, aten::threshold_backward, forward, (26) last_X = torch.relu(last_X)\n",
      "2.78 Mb\n",
      "##############################################\n",
      "model, aten::empty, forward, (24) last_X = linear_layer(last_X)\n",
      "2.12 Mb\n",
      "##############################################\n",
      "model.scorer, aten::empty, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::empty, forward, (24) last_X = linear_layer(last_X)\n",
      "2.12 Mb\n",
      "##############################################\n",
      "model, AddmmBackward, forward, (24) last_X = linear_layer(last_X)\n",
      "2.12 Mb\n",
      "##############################################\n",
      "model.scorer, AddmmBackward, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, AddmmBackward, forward, (24) last_X = linear_layer(last_X)\n",
      "2.12 Mb\n",
      "##############################################\n",
      "model, aten::mm, forward, (24) last_X = linear_layer(last_X)\n",
      "2.12 Mb\n",
      "##############################################\n",
      "model.scorer, aten::mm, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::mm, forward, (24) last_X = linear_layer(last_X)\n",
      "2.12 Mb\n",
      "##############################################\n",
      "model, aten::empty, forward, (28) z = self.scorer(last_X)\n",
      "720.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::empty, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::empty, forward, (28) z = self.scorer(last_X)\n",
      "720.00 Kb\n",
      "##############################################\n",
      "model, AddmmBackward, forward, (28) z = self.scorer(last_X)\n",
      "720.00 Kb\n",
      "##############################################\n",
      "model.scorer, AddmmBackward, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, AddmmBackward, forward, (28) z = self.scorer(last_X)\n",
      "720.00 Kb\n",
      "##############################################\n",
      "model, aten::mm, forward, (28) z = self.scorer(last_X)\n",
      "720.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::mm, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::mm, forward, (28) z = self.scorer(last_X)\n",
      "720.00 Kb\n",
      "##############################################\n",
      "model, aten::addmm, forward, (28) z = self.scorer(last_X)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::addmm, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::addmm, forward, (28) z = self.scorer(last_X)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model, aten::resize_, forward, (28) z = self.scorer(last_X)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model.scorer, aten::resize_, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::resize_, forward, (28) z = self.scorer(last_X)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model, aten::softmax, forward, (30) a = torch.softmax(z, dim=1)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model, aten::_softmax, forward, (30) a = torch.softmax(z, dim=1)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model, aten::empty_like, forward, (30) a = torch.softmax(z, dim=1)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model, aten::empty, forward, (30) a = torch.softmax(z, dim=1)\n",
      "360.00 Kb\n",
      "##############################################\n",
      "model, aten::t, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::t, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::t, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::transpose, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::transpose, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::transpose, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::as_strided, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::as_strided, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::as_strided, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::expand, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::expand, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::expand, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::stride, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::stride, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::stride, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::t, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::t, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::t, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::transpose, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::transpose, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::transpose, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::as_strided, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::as_strided, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::as_strided, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::expand, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::expand, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::expand, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::stride, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, aten::stride, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, aten::stride, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, aten::contiguous, forward, (30) a = torch.softmax(z, dim=1)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, TBackward, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, TBackward, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, TBackward, forward, (28) z = self.scorer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model, TBackward, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n",
      "model.scorer, TBackward, forward, (93) return F.linear(input, self.weight, self.bias)\n",
      "model, TBackward, forward, (24) last_X = linear_layer(last_X)\n",
      "0.0 b\n",
      "##############################################\n"
     ]
    }
   ],
   "source": [
    "rankByCriteria(prof, model, criteria='cuda_memory_usage', per_thread=False, per_inp_shapes=False, include_external=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_losses': [1.613153338432312,\n",
       "  1.6128677129745483,\n",
       "  1.6125847101211548,\n",
       "  1.6123082637786865,\n",
       "  1.6120353937149048,\n",
       "  1.611763834953308,\n",
       "  1.6114978790283203,\n",
       "  1.6112381219863892,\n",
       "  1.6109875440597534,\n",
       "  1.6107460260391235,\n",
       "  1.6105140447616577,\n",
       "  1.6102869510650635,\n",
       "  1.610068917274475,\n",
       "  1.6098575592041016,\n",
       "  1.6096538305282593,\n",
       "  1.609453797340393,\n",
       "  1.6092605590820312,\n",
       "  1.6090681552886963,\n",
       "  1.6088788509368896,\n",
       "  1.6086915731430054],\n",
       " 'max_memory_allocation': [{device(type='cuda', index=0): 0.005632},\n",
       "  {device(type='cuda', index=0): 0.481792},\n",
       "  {device(type='cuda', index=0): 0.486912},\n",
       "  {device(type='cuda', index=0): 0.486912},\n",
       "  {device(type='cuda', index=0): 0.486912},\n",
       "  {device(type='cuda', index=0): 0.486912},\n",
       "  {device(type='cuda', index=0): 0.486912},\n",
       "  {device(type='cuda', index=0): 0.486912},\n",
       "  {device(type='cuda', index=0): 0.486912},\n",
       "  {device(type='cuda', index=0): 0.486912},\n",
       "  {device(type='cuda', index=0): 0.487424},\n",
       "  {device(type='cuda', index=0): 0.487424},\n",
       "  {device(type='cuda', index=0): 0.487424},\n",
       "  {device(type='cuda', index=0): 0.487424},\n",
       "  {device(type='cuda', index=0): 0.487424},\n",
       "  {device(type='cuda', index=0): 0.487424},\n",
       "  {device(type='cuda', index=0): 0.487424},\n",
       "  {device(type='cuda', index=0): 0.487424},\n",
       "  {device(type='cuda', index=0): 0.487424},\n",
       "  {device(type='cuda', index=0): 0.487936},\n",
       "  {device(type='cuda', index=0): 0.487936},\n",
       "  {device(type='cuda', index=0): 0.487936},\n",
       "  {device(type='cuda', index=0): 0.487936},\n",
       "  {device(type='cuda', index=0): 0.487936},\n",
       "  {device(type='cuda', index=0): 0.487936},\n",
       "  {device(type='cuda', index=0): 0.487936},\n",
       "  {device(type='cuda', index=0): 0.487936},\n",
       "  {device(type='cuda', index=0): 0.487936},\n",
       "  {device(type='cuda', index=0): 0.488448},\n",
       "  {device(type='cuda', index=0): 0.488448},\n",
       "  {device(type='cuda', index=0): 0.488448},\n",
       "  {device(type='cuda', index=0): 0.488448},\n",
       "  {device(type='cuda', index=0): 0.488448},\n",
       "  {device(type='cuda', index=0): 0.488448},\n",
       "  {device(type='cuda', index=0): 0.488448},\n",
       "  {device(type='cuda', index=0): 0.488448},\n",
       "  {device(type='cuda', index=0): 0.488448},\n",
       "  {device(type='cuda', index=0): 0.48896},\n",
       "  {device(type='cuda', index=0): 0.48896},\n",
       "  {device(type='cuda', index=0): 0.48896},\n",
       "  {device(type='cuda', index=0): 0.48896},\n",
       "  {device(type='cuda', index=0): 0.48896},\n",
       "  {device(type='cuda', index=0): 0.48896},\n",
       "  {device(type='cuda', index=0): 0.48896},\n",
       "  {device(type='cuda', index=0): 0.48896},\n",
       "  {device(type='cuda', index=0): 0.48896},\n",
       "  {device(type='cuda', index=0): 0.489472},\n",
       "  {device(type='cuda', index=0): 0.489472},\n",
       "  {device(type='cuda', index=0): 0.489472},\n",
       "  {device(type='cuda', index=0): 0.489472},\n",
       "  {device(type='cuda', index=0): 0.489472},\n",
       "  {device(type='cuda', index=0): 0.489472},\n",
       "  {device(type='cuda', index=0): 0.489472},\n",
       "  {device(type='cuda', index=0): 0.489472},\n",
       "  {device(type='cuda', index=0): 0.489472},\n",
       "  {device(type='cuda', index=0): 0.489984},\n",
       "  {device(type='cuda', index=0): 0.489984},\n",
       "  {device(type='cuda', index=0): 0.489984},\n",
       "  {device(type='cuda', index=0): 0.489984},\n",
       "  {device(type='cuda', index=0): 0.489984},\n",
       "  {device(type='cuda', index=0): 0.489984},\n",
       "  {device(type='cuda', index=0): 0.489984},\n",
       "  {device(type='cuda', index=0): 0.489984},\n",
       "  {device(type='cuda', index=0): 0.489984},\n",
       "  {device(type='cuda', index=0): 0.490496},\n",
       "  {device(type='cuda', index=0): 0.490496},\n",
       "  {device(type='cuda', index=0): 0.490496},\n",
       "  {device(type='cuda', index=0): 0.490496},\n",
       "  {device(type='cuda', index=0): 0.490496},\n",
       "  {device(type='cuda', index=0): 0.490496},\n",
       "  {device(type='cuda', index=0): 0.490496},\n",
       "  {device(type='cuda', index=0): 0.490496},\n",
       "  {device(type='cuda', index=0): 0.490496},\n",
       "  {device(type='cuda', index=0): 0.491008},\n",
       "  {device(type='cuda', index=0): 0.491008},\n",
       "  {device(type='cuda', index=0): 0.491008},\n",
       "  {device(type='cuda', index=0): 0.491008},\n",
       "  {device(type='cuda', index=0): 0.491008},\n",
       "  {device(type='cuda', index=0): 0.491008},\n",
       "  {device(type='cuda', index=0): 0.491008},\n",
       "  {device(type='cuda', index=0): 0.491008},\n",
       "  {device(type='cuda', index=0): 0.491008},\n",
       "  {device(type='cuda', index=0): 0.49152},\n",
       "  {device(type='cuda', index=0): 0.49152},\n",
       "  {device(type='cuda', index=0): 0.49152},\n",
       "  {device(type='cuda', index=0): 0.49152},\n",
       "  {device(type='cuda', index=0): 0.49152},\n",
       "  {device(type='cuda', index=0): 0.49152},\n",
       "  {device(type='cuda', index=0): 0.49152},\n",
       "  {device(type='cuda', index=0): 0.49152},\n",
       "  {device(type='cuda', index=0): 0.49152},\n",
       "  {device(type='cuda', index=0): 0.492032},\n",
       "  {device(type='cuda', index=0): 0.492032},\n",
       "  {device(type='cuda', index=0): 0.492032},\n",
       "  {device(type='cuda', index=0): 0.492032},\n",
       "  {device(type='cuda', index=0): 0.492032},\n",
       "  {device(type='cuda', index=0): 0.492032},\n",
       "  {device(type='cuda', index=0): 0.492032},\n",
       "  {device(type='cuda', index=0): 0.492032},\n",
       "  {device(type='cuda', index=0): 0.492032},\n",
       "  {device(type='cuda', index=0): 0.492544},\n",
       "  {device(type='cuda', index=0): 0.492544},\n",
       "  {device(type='cuda', index=0): 0.492544},\n",
       "  {device(type='cuda', index=0): 0.492544},\n",
       "  {device(type='cuda', index=0): 0.492544},\n",
       "  {device(type='cuda', index=0): 0.492544},\n",
       "  {device(type='cuda', index=0): 0.492544},\n",
       "  {device(type='cuda', index=0): 0.492544},\n",
       "  {device(type='cuda', index=0): 0.492544},\n",
       "  {device(type='cuda', index=0): 0.493056},\n",
       "  {device(type='cuda', index=0): 0.493056},\n",
       "  {device(type='cuda', index=0): 0.493056},\n",
       "  {device(type='cuda', index=0): 0.493056},\n",
       "  {device(type='cuda', index=0): 0.493056},\n",
       "  {device(type='cuda', index=0): 0.493056},\n",
       "  {device(type='cuda', index=0): 0.493056},\n",
       "  {device(type='cuda', index=0): 0.493056},\n",
       "  {device(type='cuda', index=0): 0.493056},\n",
       "  {device(type='cuda', index=0): 0.493568},\n",
       "  {device(type='cuda', index=0): 0.493568},\n",
       "  {device(type='cuda', index=0): 0.493568},\n",
       "  {device(type='cuda', index=0): 0.493568},\n",
       "  {device(type='cuda', index=0): 0.493568},\n",
       "  {device(type='cuda', index=0): 0.493568},\n",
       "  {device(type='cuda', index=0): 0.493568},\n",
       "  {device(type='cuda', index=0): 0.493568},\n",
       "  {device(type='cuda', index=0): 0.493568},\n",
       "  {device(type='cuda', index=0): 0.49408},\n",
       "  {device(type='cuda', index=0): 0.49408},\n",
       "  {device(type='cuda', index=0): 0.49408},\n",
       "  {device(type='cuda', index=0): 0.49408},\n",
       "  {device(type='cuda', index=0): 0.49408},\n",
       "  {device(type='cuda', index=0): 0.49408},\n",
       "  {device(type='cuda', index=0): 0.49408},\n",
       "  {device(type='cuda', index=0): 0.49408},\n",
       "  {device(type='cuda', index=0): 0.49408},\n",
       "  {device(type='cuda', index=0): 0.494592},\n",
       "  {device(type='cuda', index=0): 0.494592},\n",
       "  {device(type='cuda', index=0): 0.494592},\n",
       "  {device(type='cuda', index=0): 0.494592},\n",
       "  {device(type='cuda', index=0): 0.494592},\n",
       "  {device(type='cuda', index=0): 0.494592},\n",
       "  {device(type='cuda', index=0): 0.494592},\n",
       "  {device(type='cuda', index=0): 0.494592},\n",
       "  {device(type='cuda', index=0): 0.494592},\n",
       "  {device(type='cuda', index=0): 0.495104},\n",
       "  {device(type='cuda', index=0): 0.495104},\n",
       "  {device(type='cuda', index=0): 0.495104},\n",
       "  {device(type='cuda', index=0): 0.495104},\n",
       "  {device(type='cuda', index=0): 0.495104},\n",
       "  {device(type='cuda', index=0): 0.495104},\n",
       "  {device(type='cuda', index=0): 0.495104},\n",
       "  {device(type='cuda', index=0): 0.495104},\n",
       "  {device(type='cuda', index=0): 0.495104},\n",
       "  {device(type='cuda', index=0): 0.495616},\n",
       "  {device(type='cuda', index=0): 0.495616},\n",
       "  {device(type='cuda', index=0): 0.495616},\n",
       "  {device(type='cuda', index=0): 0.495616},\n",
       "  {device(type='cuda', index=0): 0.495616},\n",
       "  {device(type='cuda', index=0): 0.495616},\n",
       "  {device(type='cuda', index=0): 0.495616},\n",
       "  {device(type='cuda', index=0): 0.495616},\n",
       "  {device(type='cuda', index=0): 0.495616},\n",
       "  {device(type='cuda', index=0): 0.496128},\n",
       "  {device(type='cuda', index=0): 0.496128},\n",
       "  {device(type='cuda', index=0): 0.496128},\n",
       "  {device(type='cuda', index=0): 0.496128},\n",
       "  {device(type='cuda', index=0): 0.496128},\n",
       "  {device(type='cuda', index=0): 0.496128},\n",
       "  {device(type='cuda', index=0): 0.496128},\n",
       "  {device(type='cuda', index=0): 0.496128},\n",
       "  {device(type='cuda', index=0): 0.496128},\n",
       "  {device(type='cuda', index=0): 0.49664},\n",
       "  {device(type='cuda', index=0): 0.49664},\n",
       "  {device(type='cuda', index=0): 0.49664},\n",
       "  {device(type='cuda', index=0): 0.49664},\n",
       "  {device(type='cuda', index=0): 0.49664},\n",
       "  {device(type='cuda', index=0): 0.49664},\n",
       "  {device(type='cuda', index=0): 0.49664},\n",
       "  {device(type='cuda', index=0): 0.49664}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nvidia Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 22 16:31:42 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 000047DD:00:00.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    56W / 149W |    338MiB / 11441MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     24644      C   ...s/py37_pytorch/bin/python      335MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Ran without profiler\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |  496640 B  |  141505 KB |  141505 KB |\n",
      "|       from large pool |       0 B  |       0 B  |       0 KB |       0 KB |\n",
      "|       from small pool |       0 B  |  496640 B  |  141505 KB |  141505 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |  496640 B  |  141505 KB |  141505 KB |\n",
      "|       from large pool |       0 B  |       0 B  |       0 KB |       0 KB |\n",
      "|       from small pool |       0 B  |  496640 B  |  141505 KB |  141505 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |    2047 KB |  143552 KB |  143552 KB |\n",
      "|       from large pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |       0 B  |    2047 KB |  143552 KB |  143552 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |      58    |    6530    |    6530    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |      58    |    6530    |    6530    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |      58    |    6530    |    6530    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |      58    |    6530    |    6530    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |      12    |    2979    |    2979    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |      12    |    2979    |    2979    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ran without profiler\n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
