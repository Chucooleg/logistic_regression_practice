{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "a is not a leaf, we cannot access its gradients after calling backward\n",
      "None\n",
      "b is a leaf\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float16)\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Only leaves have .grad attributes\n",
    "\n",
    "# https://discuss.pytorch.org/t/valueerror-cant-optimize-a-non-leaf-tensor/21751/2\n",
    "a = torch.rand(10).requires_grad_().half()\n",
    "b = torch.rand(10).half().requires_grad_() \n",
    "\n",
    "sum_a = torch.sum(a)\n",
    "sum_b = torch.sum(b)\n",
    "\n",
    "sum_a.backward()\n",
    "sum_b.backward()\n",
    "\n",
    "print('--------------------------')\n",
    "print('a is not a leaf, we cannot access its gradients after calling backward')\n",
    "print(a.grad)\n",
    "print('b is a leaf')\n",
    "print(b.grad)\n",
    "print('--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a dtype torch.float32 require_grad= True\n",
      "b dtype torch.float16 require_grad= True\n",
      "b_sum dtype torch.float16 require_grad= True\n",
      "a.grad tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "a.grad dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Cast a 32-bit tensor into 16-bit during forward,\n",
    "# after backward, its gradients of a will be 32-bit again\n",
    "\n",
    "a = torch.rand(10).requires_grad_()\n",
    "b = a.half()\n",
    "\n",
    "print('a dtype', a.dtype, 'require_grad=',a.requires_grad)\n",
    "print('b dtype', b.dtype, 'require_grad=',b.requires_grad)\n",
    "\n",
    "b_sum = torch.sum(b)\n",
    "print('b_sum dtype', b_sum.dtype, 'require_grad=',b_sum.requires_grad)\n",
    "\n",
    "b_sum.backward()\n",
    "\n",
    "print('a.grad',a.grad)\n",
    "print('a.grad dtype', a.grad.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: W dtype torch.float32\n",
      "Optimizer opt[0]['params'][0].dtype torch.float32\n",
      "Data X dtype torch.float16\n",
      "Data Y dtype torch.float16\n",
      "Forward: W16 dtype torch.float16\n",
      "Forward: Y_hat dtype torch.float16\n",
      "Forward: loss dtype torch.float16\n",
      "-------------------------------\n",
      "After backward: loss dtype torch.float16\n",
      "After backward: Y_hat dtype torch.float16\n",
      "After backward: X dtype torch.float16\n",
      "After backward: X.grad dtype torch.float16\n",
      "After backward: Y dtype torch.float16\n",
      "After backward: Y.grad dtype torch.float16\n",
      "After backward: Y dtype torch.float16\n",
      "After backward: Y.grad dtype torch.float16\n",
      "After backward: W dtype torch.float32\n",
      "After backward: W.grad dtype torch.float32\n",
      "Optimizer opt[0]['params'][0].dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Backward will also backward through the .half() operation\n",
    "# Therefore, gradient of W and in the optimizer will become 32-bit again\n",
    "\n",
    "W = nn.Parameter(torch.tensor([1,2,3], dtype=torch.float32))\n",
    "opt = torch.optim.SGD([W], lr=0.1)\n",
    "print('Init: W dtype', W.dtype)\n",
    "print('Optimizer opt[0][\\'params\\'][0].dtype', opt.param_groups[0]['params'][0].dtype)\n",
    "\n",
    "X = torch.tensor([2,4,6], dtype=torch.float16, requires_grad=True)\n",
    "Y = torch.tensor([20.], dtype=torch.float16, requires_grad=True)\n",
    "print('Data X dtype', X.dtype)\n",
    "print('Data Y dtype', Y.dtype)\n",
    "\n",
    "W16 = W.half()\n",
    "print('Forward: W16 dtype', W16.dtype)\n",
    "\n",
    "Y_hat = torch.dot(W16, X)\n",
    "print('Forward: Y_hat dtype', Y_hat.dtype)\n",
    "\n",
    "loss = Y - Y_hat\n",
    "print('Forward: loss dtype', loss.dtype)\n",
    "\n",
    "loss.backward()\n",
    "print('-------------------------------')\n",
    "\n",
    "print('After backward: loss dtype', loss.dtype)\n",
    "print('After backward: Y_hat dtype', Y_hat.dtype)\n",
    "print('After backward: X dtype', X.dtype)\n",
    "print('After backward: X.grad dtype', X.grad.dtype)\n",
    "print('After backward: Y dtype', Y.dtype)\n",
    "print('After backward: Y.grad dtype', Y.grad.dtype)\n",
    "print('After backward: Y dtype', Y.dtype)\n",
    "print('After backward: Y.grad dtype', Y.grad.dtype)\n",
    "print('After backward: W dtype', W.dtype)\n",
    "print('After backward: W.grad dtype', W.grad.dtype)\n",
    "print('Optimizer opt[0][\\'params\\'][0].dtype', opt.param_groups[0]['params'][0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: model.W dtype torch.float32\n",
      "Init: opt.param_groups[0]['params'][0] dtype torch.float32\n",
      "Data X dtype torch.float16\n",
      "Data Y dtype torch.float16\n",
      "Forward: Y_hat dtype torch.float16\n",
      "Forward: loss dtype torch.float16\n",
      "-------------------------------\n",
      "After backward: self.W dtype torch.float32\n",
      "After backward: self.W.grad dtype torch.float32\n",
      "Optimizer opt[0]['params'][0].dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Wrapping the above as a nn.Module\n",
    "\n",
    "class SimpleWModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.tensor([1,2,3], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, X):\n",
    "        W_16 = self.W.half()\n",
    "        return torch.dot(W_16, X)\n",
    "\n",
    "model = SimpleWModel()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "print('Init: model.W dtype', model.W.dtype)\n",
    "print('Init: opt.param_groups[0][\\'params\\'][0] dtype', opt.param_groups[0]['params'][0].data.dtype)\n",
    "\n",
    "X = torch.tensor([2,4,6], dtype=torch.float16, requires_grad=True)\n",
    "Y = torch.tensor([20.], dtype=torch.float16, requires_grad=True)\n",
    "print('Data X dtype', X.dtype)\n",
    "print('Data Y dtype', Y.dtype)\n",
    "\n",
    "Y_hat = model(X)\n",
    "print('Forward: Y_hat dtype', Y_hat.dtype)\n",
    "loss = Y - Y_hat\n",
    "print('Forward: loss dtype', loss.dtype)\n",
    "\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "print('-------------------------------')\n",
    "print('After backward: self.W dtype', model.W.dtype)\n",
    "print('After backward: self.W.grad dtype', model.W.grad.data.dtype)\n",
    "print('Optimizer opt[0][\\'params\\'][0].dtype', opt.param_groups[0]['params'][0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: model.W.weight dtype torch.float32\n",
      "Init: model.W.weightopt.param_groups[0]['params'][1] dtype torch.float32\n",
      "After backward: model.W.weightopt.param_groups[0]['params'][1] data tensor([[ 0.4414,  0.4792, -0.1353]])\n",
      "Init: model.W.bias dtype torch.float32\n",
      "Init: model.W.bias opt.param_groups[0]['params'][0] dtype torch.float32\n",
      "Init: model.W2 dtype torch.float32\n",
      "Init: model.W2 opt.param_groups[0]['params'][0] dtype torch.float32\n",
      "-------------------------------\n",
      "Data X dtype torch.float16\n",
      "Data Y dtype torch.float16\n",
      "Forward: Y_hat dtype torch.float16\n",
      "Forward: loss dtype torch.float16\n",
      "-------------------------------\n",
      "After backward: model.W.weight dtype torch.float16\n",
      "After backward: model.W.weightopt.param_groups[0]['params'][1] dtype torch.float16\n",
      "After backward: model.W.weightopt.param_groups[0]['params'][1] grad data tensor([[-2., -4., -6.]], dtype=torch.float16)\n",
      "After backward:: model.W.bias dtype torch.float16\n",
      "After backward:: model.W.bias opt.param_groups[0]['params'][0] dtype torch.float16\n",
      "After backward: model.W2 dtype torch.float32\n",
      "After backward: model.W2 opt.param_groups[0]['params'][0] dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "# !!! !!! INCORRECT! Doesn't work because calling half on a module(nn.Linear) modifies the data in place \n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SimpleLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(3,1)\n",
    "        self.W2 = nn.Parameter(torch.tensor([1,2,3], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, X):\n",
    "        W_16 = self.W.half()\n",
    "        W2_16 = self.W2.half()\n",
    "        return torch.mean(W_16(X) + W2_16*3)\n",
    "\n",
    "model = SimpleLinear()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "print('Init: model.W.weight dtype', model.W.weight.dtype)\n",
    "print('Init: model.W.weightopt.param_groups[0][\\'params\\'][1] dtype', opt.param_groups[0]['params'][1].data.dtype)\n",
    "print('After backward: model.W.weightopt.param_groups[0][\\'params\\'][1] data', opt.param_groups[0]['params'][1].data)\n",
    "print('Init: model.W.bias dtype', model.W.bias.dtype)\n",
    "print('Init: model.W.bias opt.param_groups[0][\\'params\\'][0] dtype', opt.param_groups[0]['params'][2].data.dtype)\n",
    "print('Init: model.W2 dtype', model.W2.dtype)\n",
    "print('Init: model.W2 opt.param_groups[0][\\'params\\'][0] dtype', opt.param_groups[0]['params'][0].data.dtype)\n",
    "print('-------------------------------')\n",
    "X = torch.tensor([2,4,6], dtype=torch.float16, requires_grad=True)\n",
    "Y = torch.tensor([20.], dtype=torch.float16, requires_grad=True)\n",
    "print('Data X dtype', X.dtype)\n",
    "print('Data Y dtype', Y.dtype)\n",
    "\n",
    "Y_hat = model(X)\n",
    "print('Forward: Y_hat dtype', Y_hat.dtype)\n",
    "loss = Y - Y_hat\n",
    "print('Forward: loss dtype', loss.dtype)\n",
    "\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "print('-------------------------------')\n",
    "print('After backward: model.W.weight dtype', model.W.weight.dtype)\n",
    "print('After backward: model.W.weightopt.param_groups[0][\\'params\\'][1] dtype', opt.param_groups[0]['params'][1].data.dtype)\n",
    "print('After backward: model.W.weightopt.param_groups[0][\\'params\\'][1] grad data', opt.param_groups[0]['params'][1].grad.data)\n",
    "print('After backward:: model.W.bias dtype', model.W.bias.dtype)\n",
    "print('After backward:: model.W.bias opt.param_groups[0][\\'params\\'][0] dtype', opt.param_groups[0]['params'][2].data.dtype)\n",
    "print('After backward: model.W2 dtype', model.W2.dtype)\n",
    "print('After backward: model.W2 opt.param_groups[0][\\'params\\'][0] dtype', opt.param_groups[0]['params'][0].data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: model.W.weight dtype torch.float32\n",
      "Init: model.W.weightopt.param_groups[0]['params'][1] dtype torch.float32\n",
      "After backward: model.W.weightopt.param_groups[0]['params'][1] data tensor([[ 0.4414,  0.4792, -0.1353]])\n",
      "Init: model.W.bias dtype torch.float32\n",
      "Init: model.W.bias opt.param_groups[0]['params'][0] dtype torch.float32\n",
      "Init: model.W2 dtype torch.float32\n",
      "Init: model.W2 opt.param_groups[0]['params'][0] dtype torch.float32\n",
      "-------------------------------\n",
      "Data X dtype torch.float16\n",
      "Data Y dtype torch.float16\n",
      "Forward: Y_hat dtype torch.float16\n",
      "Forward: loss dtype torch.float16\n",
      "-------------------------------\n",
      "After backward: model.W.weight dtype torch.float32\n",
      "After backward: model.W.weightopt.param_groups[0]['params'][1] dtype torch.float32\n",
      "After backward:: model.W.bias dtype torch.float32\n",
      "After backward:: model.W.bias opt.param_groups[0]['params'][0] dtype torch.float32\n",
      "After backward: model.W2 dtype torch.float32\n",
      "After backward: model.W2 opt.param_groups[0]['params'][0] dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "# !!! INCORRECT! Using deep copy before calling half() is incorrect because gradient doesn't flow through deepcopy\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SimpleLinear2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(3,1)\n",
    "        self.W2 = nn.Parameter(torch.tensor([1,2,3], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, X):\n",
    "        W_16 = copy.deepcopy(self.W).half()\n",
    "        W2_16 = self.W2.half()\n",
    "        return torch.mean(W_16(X) + W2_16*3)\n",
    "\n",
    "model = SimpleLinear2()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "print('Init: model.W.weight dtype', model.W.weight.dtype)\n",
    "print('Init: model.W.weightopt.param_groups[0][\\'params\\'][1] dtype', opt.param_groups[0]['params'][1].data.dtype)\n",
    "print('After backward: model.W.weightopt.param_groups[0][\\'params\\'][1] data', opt.param_groups[0]['params'][1].data)\n",
    "print('Init: model.W.bias dtype', model.W.bias.dtype)\n",
    "print('Init: model.W.bias opt.param_groups[0][\\'params\\'][0] dtype', opt.param_groups[0]['params'][2].data.dtype)\n",
    "print('Init: model.W2 dtype', model.W2.dtype)\n",
    "print('Init: model.W2 opt.param_groups[0][\\'params\\'][0] dtype', opt.param_groups[0]['params'][0].data.dtype)\n",
    "print('-------------------------------')\n",
    "X = torch.tensor([2,4,6], dtype=torch.float16, requires_grad=True)\n",
    "Y = torch.tensor([20.], dtype=torch.float16, requires_grad=True)\n",
    "print('Data X dtype', X.dtype)\n",
    "print('Data Y dtype', Y.dtype)\n",
    "\n",
    "Y_hat = model(X)\n",
    "print('Forward: Y_hat dtype', Y_hat.dtype)\n",
    "loss = Y - Y_hat\n",
    "print('Forward: loss dtype', loss.dtype)\n",
    "\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "print('-------------------------------')\n",
    "print('After backward: model.W.weight dtype', model.W.weight.dtype)\n",
    "print('After backward: model.W.weightopt.param_groups[0][\\'params\\'][1] dtype', opt.param_groups[0]['params'][1].data.dtype)\n",
    "# will raise an error because deepcopy doesn't allow gradient to flow through\n",
    "# print('After backward: model.W.weightopt.param_groups[0][\\'params\\'][1] grad data', opt.param_groups[0]['params'][1].grad.data) \n",
    "print('After backward:: model.W.bias dtype', model.W.bias.dtype)\n",
    "print('After backward:: model.W.bias opt.param_groups[0][\\'params\\'][0] dtype', opt.param_groups[0]['params'][2].data.dtype)\n",
    "print('After backward: model.W2 dtype', model.W2.dtype)\n",
    "print('After backward: model.W2 opt.param_groups[0][\\'params\\'][0] dtype', opt.param_groups[0]['params'][0].data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "------------------------------\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.bfloat16\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 2., 3.], dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.4414,  0.4785, -0.1357]], dtype=torch.bfloat16, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.5312], dtype=torch.bfloat16, requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! INCORRECT! Optimizer Copy is changed to 16-bit as well\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SimpleLinear3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(3,1)\n",
    "        self.W2 = nn.Parameter(torch.tensor([1,2,3], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.mean(self.W(X) + self.W2*3)\n",
    "\n",
    "model = SimpleLinear3()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for p in model.parameters():\n",
    "    print(p.dtype)\n",
    "print('------------------------------')\n",
    "model.bfloat16()\n",
    "for p in model.parameters():\n",
    "    print(p.dtype)\n",
    "print('------------------------------')\n",
    "opt.param_groups[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
