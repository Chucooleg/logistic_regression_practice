{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/notes/cuda.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Make sure devices can be discovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.7.7 (default, May  7 2020, 21:25:33) \n",
      "[GCC 7.3.0]\n",
      "__pyTorch VERSION: 1.5.1\n",
      "__CUDA VERSION\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "__CUDNN VERSION: 7603\n",
      "__Number CUDA Devices: 2\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  2\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "# https://discuss.pytorch.org/t/i-have-3-gpu-why-torch-cuda-device-count-only-return-1/7245/4\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 02:14:14 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   42C    P8    27W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   34C    P8    33W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multiple devices & How to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Simplest way to put tensors on different devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0') # same as cuda0 = torch.device('cuda:0') or cuda = torch.device('cuda')\n",
    "cuda1 = torch.device('cuda:1') # same as cuda1 = torch.device('cuda:1')\n",
    "cuda2 = torch.device('cuda:2') # no such device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assigned to device 0\n",
    "x = torch.tensor([1., 2.], device=cuda0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], device='cuda:1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assigned to device 1\n",
    "x = torch.tensor([1., 2.], device=cuda1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-89d5e0867bc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# will raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal"
     ]
    }
   ],
   "source": [
    "# will raise error\n",
    "x = torch.tensor([1., 2.], device=cuda2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Device Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "# use device 0 scope\n",
    "with torch.cuda.device(0):\n",
    "  # will assign to device 0 because we are passing cuda0, not because of scope\n",
    "  x1 = torch.tensor([1., 2.], device=cuda0)\n",
    "\n",
    "  # will assign to device 0 because we are passing cuda0, not because of scope\n",
    "  x2 = torch.tensor([1., 2.]).to(device=cuda0)\n",
    "\n",
    "  # will assign to device 0 because we are within device 0 scope\n",
    "  x3 = torch.tensor([1., 2.]).cuda()\n",
    "\n",
    "  # will assign to device 0 because we are passing cuda0, not because of scope\n",
    "  x4 = torch.tensor([1., 2.]).cuda(cuda0)\n",
    "\n",
    "print(x1.device)\n",
    "print(x2.device)\n",
    "print(x3.device)\n",
    "print(x4.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "# use device 1 scope\n",
    "with torch.cuda.device(1):\n",
    "  # will assign to device 0 because we are passing cuda0, not because of scope\n",
    "  x1 = torch.tensor([1., 2.], device=cuda0)\n",
    "\n",
    "  # will assign to device 0 because we are passing cuda0, not because of scope\n",
    "  x2 = torch.tensor([1., 2.]).to(device=cuda0)\n",
    "\n",
    "  # will assign to device 1 because we are within device 1 scope\n",
    "  x3 = torch.tensor([1., 2.]).cuda()\n",
    "\n",
    "  # will assign to device 0 because we are passing cuda0, not because of scope\n",
    "  x4 = torch.tensor([1., 2.]).cuda(cuda0)\n",
    "\n",
    "print(x1.device)\n",
    "print(x2.device)\n",
    "print(x3.device)\n",
    "print(x4.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Copy from device 0 to device 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], device='cuda:1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to(device=cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Put tensor and delete tensors on devices, free up CPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Restart Kernel, put one tensor on device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0') # same as cuda0 = torch.device('cuda:0') or cuda = torch.device('cuda')\n",
    "cuda1 = torch.device('cuda:1') # same as cuda1 = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(300000000, dtype=torch.int8).cuda(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 02:52:30 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   56C    P0    57W / 149W |    542MiB / 11441MiB |     18%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   35C    P8    34W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     90803      C   ...nvs/playground/bin/python      539MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/12\n",
    "\n",
    "def print_cuda_memory_allocation_for_device(device_to_print_stats):\n",
    "\n",
    "    print(torch.cuda.memory_allocated(device_to_print_stats))\n",
    "    # > 0\n",
    "    print(torch.cuda.memory_reserved(device_to_print_stats))\n",
    "    # > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000256\n",
      "301989888\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_allocation_for_device(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_allocation_for_device(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "301989888\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_allocation_for_device(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_allocation_for_device(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 02:52:31 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   56C    P0    56W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   35C    P8    34W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     90803      C   ...nvs/playground/bin/python      539MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_allocation_for_device(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_allocation_for_device(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 02:52:32 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   56C    P0    57W / 149W |    254MiB / 11441MiB |     18%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   35C    P8    34W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     90803      C   ...nvs/playground/bin/python      251MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Restart Kernel, put one tensor on each device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 03:32:16 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   62C    P8    38W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   46C    P8    62W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0') # same as cuda0 = torch.device('cuda:0') or cuda = torch.device('cuda')\n",
    "cuda1 = torch.device('cuda:1') # same as cuda1 = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_allocated\n",
    "\n",
    "# device 0 - everything is zero\n",
    "print(torch.cuda.memory_summary(cuda0))\n",
    "# device 1 -- should give same result \n",
    "# print(torch.cuda.memory_summary(cuda1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(300000000, dtype=torch.int8).cuda(cuda0)\n",
    "y = torch.zeros(300000000, dtype=torch.int8).cuda(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 03:34:23 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   59C    P0    58W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    71W / 149W |    542MiB / 11441MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    103067      C   ...nvs/playground/bin/python      539MiB |\n",
      "|    1   N/A  N/A    103067      C   ...nvs/playground/bin/python      539MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cuda_memory_allocation_for_device(device_to_print_stats):\n",
    "\n",
    "    print(torch.cuda.memory_allocated(device_to_print_stats))\n",
    "    # > 0\n",
    "    print(torch.cuda.memory_reserved(device_to_print_stats))\n",
    "    # > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000256\n",
      "301989888\n"
     ]
    }
   ],
   "source": [
    "# both allocated and reserved are non-zero because of tensor x on device 0\n",
    "print_cuda_memory_allocation_for_device(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000256\n",
      "301989888\n"
     ]
    }
   ],
   "source": [
    "# both allocated and reserved are non-zero because of tensor y on device 1\n",
    "print_cuda_memory_allocation_for_device(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('active.all.allocated', 1),\n",
       "             ('active.all.current', 1),\n",
       "             ('active.all.freed', 0),\n",
       "             ('active.all.peak', 1),\n",
       "             ('active.large_pool.allocated', 1),\n",
       "             ('active.large_pool.current', 1),\n",
       "             ('active.large_pool.freed', 0),\n",
       "             ('active.large_pool.peak', 1),\n",
       "             ('active.small_pool.allocated', 0),\n",
       "             ('active.small_pool.current', 0),\n",
       "             ('active.small_pool.freed', 0),\n",
       "             ('active.small_pool.peak', 0),\n",
       "             ('active_bytes.all.allocated', 300000256),\n",
       "             ('active_bytes.all.current', 300000256),\n",
       "             ('active_bytes.all.freed', 0),\n",
       "             ('active_bytes.all.peak', 300000256),\n",
       "             ('active_bytes.large_pool.allocated', 300000256),\n",
       "             ('active_bytes.large_pool.current', 300000256),\n",
       "             ('active_bytes.large_pool.freed', 0),\n",
       "             ('active_bytes.large_pool.peak', 300000256),\n",
       "             ('active_bytes.small_pool.allocated', 0),\n",
       "             ('active_bytes.small_pool.current', 0),\n",
       "             ('active_bytes.small_pool.freed', 0),\n",
       "             ('active_bytes.small_pool.peak', 0),\n",
       "             ('allocated_bytes.all.allocated', 300000256),\n",
       "             ('allocated_bytes.all.current', 300000256),\n",
       "             ('allocated_bytes.all.freed', 0),\n",
       "             ('allocated_bytes.all.peak', 300000256),\n",
       "             ('allocated_bytes.large_pool.allocated', 300000256),\n",
       "             ('allocated_bytes.large_pool.current', 300000256),\n",
       "             ('allocated_bytes.large_pool.freed', 0),\n",
       "             ('allocated_bytes.large_pool.peak', 300000256),\n",
       "             ('allocated_bytes.small_pool.allocated', 0),\n",
       "             ('allocated_bytes.small_pool.current', 0),\n",
       "             ('allocated_bytes.small_pool.freed', 0),\n",
       "             ('allocated_bytes.small_pool.peak', 0),\n",
       "             ('allocation.all.allocated', 1),\n",
       "             ('allocation.all.current', 1),\n",
       "             ('allocation.all.freed', 0),\n",
       "             ('allocation.all.peak', 1),\n",
       "             ('allocation.large_pool.allocated', 1),\n",
       "             ('allocation.large_pool.current', 1),\n",
       "             ('allocation.large_pool.freed', 0),\n",
       "             ('allocation.large_pool.peak', 1),\n",
       "             ('allocation.small_pool.allocated', 0),\n",
       "             ('allocation.small_pool.current', 0),\n",
       "             ('allocation.small_pool.freed', 0),\n",
       "             ('allocation.small_pool.peak', 0),\n",
       "             ('inactive_split.all.allocated', 1),\n",
       "             ('inactive_split.all.current', 1),\n",
       "             ('inactive_split.all.freed', 0),\n",
       "             ('inactive_split.all.peak', 1),\n",
       "             ('inactive_split.large_pool.allocated', 1),\n",
       "             ('inactive_split.large_pool.current', 1),\n",
       "             ('inactive_split.large_pool.freed', 0),\n",
       "             ('inactive_split.large_pool.peak', 1),\n",
       "             ('inactive_split.small_pool.allocated', 0),\n",
       "             ('inactive_split.small_pool.current', 0),\n",
       "             ('inactive_split.small_pool.freed', 0),\n",
       "             ('inactive_split.small_pool.peak', 0),\n",
       "             ('inactive_split_bytes.all.allocated', 1989632),\n",
       "             ('inactive_split_bytes.all.current', 1989632),\n",
       "             ('inactive_split_bytes.all.freed', 0),\n",
       "             ('inactive_split_bytes.all.peak', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.allocated', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.current', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.freed', 0),\n",
       "             ('inactive_split_bytes.large_pool.peak', 1989632),\n",
       "             ('inactive_split_bytes.small_pool.allocated', 0),\n",
       "             ('inactive_split_bytes.small_pool.current', 0),\n",
       "             ('inactive_split_bytes.small_pool.freed', 0),\n",
       "             ('inactive_split_bytes.small_pool.peak', 0),\n",
       "             ('num_alloc_retries', 0),\n",
       "             ('num_ooms', 0),\n",
       "             ('reserved_bytes.all.allocated', 301989888),\n",
       "             ('reserved_bytes.all.current', 301989888),\n",
       "             ('reserved_bytes.all.freed', 0),\n",
       "             ('reserved_bytes.all.peak', 301989888),\n",
       "             ('reserved_bytes.large_pool.allocated', 301989888),\n",
       "             ('reserved_bytes.large_pool.current', 301989888),\n",
       "             ('reserved_bytes.large_pool.freed', 0),\n",
       "             ('reserved_bytes.large_pool.peak', 301989888),\n",
       "             ('reserved_bytes.small_pool.allocated', 0),\n",
       "             ('reserved_bytes.small_pool.current', 0),\n",
       "             ('reserved_bytes.small_pool.freed', 0),\n",
       "             ('reserved_bytes.small_pool.peak', 0),\n",
       "             ('segment.all.allocated', 1),\n",
       "             ('segment.all.current', 1),\n",
       "             ('segment.all.freed', 0),\n",
       "             ('segment.all.peak', 1),\n",
       "             ('segment.large_pool.allocated', 1),\n",
       "             ('segment.large_pool.current', 1),\n",
       "             ('segment.large_pool.freed', 0),\n",
       "             ('segment.large_pool.peak', 1),\n",
       "             ('segment.small_pool.allocated', 0),\n",
       "             ('segment.small_pool.current', 0),\n",
       "             ('segment.small_pool.freed', 0),\n",
       "             ('segment.small_pool.peak', 0)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check detailed memory stats\n",
    "# https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_stats\n",
    "\n",
    "# device 0 should give non-zero result because x is on device 0\n",
    "torch.cuda.memory_stats(cuda0)\n",
    "# device 1 -- should give same result because y is on device 0\n",
    "# torch.cuda.memory_stats(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from large pool |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from large pool |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  294912 KB |  294912 KB |  294912 KB |       0 B  |\n",
      "|       from large pool |  294912 KB |  294912 KB |  294912 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    1943 KB |    1943 KB |    1943 KB |       0 B  |\n",
      "|       from large pool |    1943 KB |    1943 KB |    1943 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/cuda.html#torch.cuda.memory_allocated\n",
    "\n",
    "# device 0 should give non-zero result because x is on device 0\n",
    "print(torch.cuda.memory_summary(cuda0))\n",
    "# device 1 -- should give same result because y is on device 1\n",
    "# print(torch.cuda.memory_summary(cuda1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Delete tensor on device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "301989888\n"
     ]
    }
   ],
   "source": [
    "# allocated is zero because x is deleted from device 0\n",
    "# reserved is still the same number because we haven't cleared x from device 0 's cache\n",
    "print_cuda_memory_allocation_for_device(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000256\n",
      "301989888\n"
     ]
    }
   ],
   "source": [
    "# both allocated and reserved are non-zero because of tensor y on device 1\n",
    "print_cuda_memory_allocation_for_device(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 03:34:24 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   59C    P0    58W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    71W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    103067      C   ...nvs/playground/bin/python      539MiB |\n",
      "|    1   N/A  N/A    103067      C   ...nvs/playground/bin/python      539MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# cache is not cleared\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('active.all.allocated', 1),\n",
       "             ('active.all.current', 0),\n",
       "             ('active.all.freed', 1),\n",
       "             ('active.all.peak', 1),\n",
       "             ('active.large_pool.allocated', 1),\n",
       "             ('active.large_pool.current', 0),\n",
       "             ('active.large_pool.freed', 1),\n",
       "             ('active.large_pool.peak', 1),\n",
       "             ('active.small_pool.allocated', 0),\n",
       "             ('active.small_pool.current', 0),\n",
       "             ('active.small_pool.freed', 0),\n",
       "             ('active.small_pool.peak', 0),\n",
       "             ('active_bytes.all.allocated', 300000256),\n",
       "             ('active_bytes.all.current', 0),\n",
       "             ('active_bytes.all.freed', 300000256),\n",
       "             ('active_bytes.all.peak', 300000256),\n",
       "             ('active_bytes.large_pool.allocated', 300000256),\n",
       "             ('active_bytes.large_pool.current', 0),\n",
       "             ('active_bytes.large_pool.freed', 300000256),\n",
       "             ('active_bytes.large_pool.peak', 300000256),\n",
       "             ('active_bytes.small_pool.allocated', 0),\n",
       "             ('active_bytes.small_pool.current', 0),\n",
       "             ('active_bytes.small_pool.freed', 0),\n",
       "             ('active_bytes.small_pool.peak', 0),\n",
       "             ('allocated_bytes.all.allocated', 300000256),\n",
       "             ('allocated_bytes.all.current', 0),\n",
       "             ('allocated_bytes.all.freed', 300000256),\n",
       "             ('allocated_bytes.all.peak', 300000256),\n",
       "             ('allocated_bytes.large_pool.allocated', 300000256),\n",
       "             ('allocated_bytes.large_pool.current', 0),\n",
       "             ('allocated_bytes.large_pool.freed', 300000256),\n",
       "             ('allocated_bytes.large_pool.peak', 300000256),\n",
       "             ('allocated_bytes.small_pool.allocated', 0),\n",
       "             ('allocated_bytes.small_pool.current', 0),\n",
       "             ('allocated_bytes.small_pool.freed', 0),\n",
       "             ('allocated_bytes.small_pool.peak', 0),\n",
       "             ('allocation.all.allocated', 1),\n",
       "             ('allocation.all.current', 0),\n",
       "             ('allocation.all.freed', 1),\n",
       "             ('allocation.all.peak', 1),\n",
       "             ('allocation.large_pool.allocated', 1),\n",
       "             ('allocation.large_pool.current', 0),\n",
       "             ('allocation.large_pool.freed', 1),\n",
       "             ('allocation.large_pool.peak', 1),\n",
       "             ('allocation.small_pool.allocated', 0),\n",
       "             ('allocation.small_pool.current', 0),\n",
       "             ('allocation.small_pool.freed', 0),\n",
       "             ('allocation.small_pool.peak', 0),\n",
       "             ('inactive_split.all.allocated', 1),\n",
       "             ('inactive_split.all.current', 0),\n",
       "             ('inactive_split.all.freed', 1),\n",
       "             ('inactive_split.all.peak', 1),\n",
       "             ('inactive_split.large_pool.allocated', 1),\n",
       "             ('inactive_split.large_pool.current', 0),\n",
       "             ('inactive_split.large_pool.freed', 1),\n",
       "             ('inactive_split.large_pool.peak', 1),\n",
       "             ('inactive_split.small_pool.allocated', 0),\n",
       "             ('inactive_split.small_pool.current', 0),\n",
       "             ('inactive_split.small_pool.freed', 0),\n",
       "             ('inactive_split.small_pool.peak', 0),\n",
       "             ('inactive_split_bytes.all.allocated', 1989632),\n",
       "             ('inactive_split_bytes.all.current', 0),\n",
       "             ('inactive_split_bytes.all.freed', 1989632),\n",
       "             ('inactive_split_bytes.all.peak', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.allocated', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.current', 0),\n",
       "             ('inactive_split_bytes.large_pool.freed', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.peak', 1989632),\n",
       "             ('inactive_split_bytes.small_pool.allocated', 0),\n",
       "             ('inactive_split_bytes.small_pool.current', 0),\n",
       "             ('inactive_split_bytes.small_pool.freed', 0),\n",
       "             ('inactive_split_bytes.small_pool.peak', 0),\n",
       "             ('num_alloc_retries', 0),\n",
       "             ('num_ooms', 0),\n",
       "             ('reserved_bytes.all.allocated', 301989888),\n",
       "             ('reserved_bytes.all.current', 301989888),\n",
       "             ('reserved_bytes.all.freed', 0),\n",
       "             ('reserved_bytes.all.peak', 301989888),\n",
       "             ('reserved_bytes.large_pool.allocated', 301989888),\n",
       "             ('reserved_bytes.large_pool.current', 301989888),\n",
       "             ('reserved_bytes.large_pool.freed', 0),\n",
       "             ('reserved_bytes.large_pool.peak', 301989888),\n",
       "             ('reserved_bytes.small_pool.allocated', 0),\n",
       "             ('reserved_bytes.small_pool.current', 0),\n",
       "             ('reserved_bytes.small_pool.freed', 0),\n",
       "             ('reserved_bytes.small_pool.peak', 0),\n",
       "             ('segment.all.allocated', 1),\n",
       "             ('segment.all.current', 1),\n",
       "             ('segment.all.freed', 0),\n",
       "             ('segment.all.peak', 1),\n",
       "             ('segment.large_pool.allocated', 1),\n",
       "             ('segment.large_pool.current', 1),\n",
       "             ('segment.large_pool.freed', 0),\n",
       "             ('segment.large_pool.peak', 1),\n",
       "             ('segment.small_pool.allocated', 0),\n",
       "             ('segment.small_pool.current', 0),\n",
       "             ('segment.small_pool.freed', 0),\n",
       "             ('segment.small_pool.peak', 0)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device 0 -- harder to read see table below\n",
    "torch.cuda.memory_stats(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('active.all.allocated', 1),\n",
       "             ('active.all.current', 1),\n",
       "             ('active.all.freed', 0),\n",
       "             ('active.all.peak', 1),\n",
       "             ('active.large_pool.allocated', 1),\n",
       "             ('active.large_pool.current', 1),\n",
       "             ('active.large_pool.freed', 0),\n",
       "             ('active.large_pool.peak', 1),\n",
       "             ('active.small_pool.allocated', 0),\n",
       "             ('active.small_pool.current', 0),\n",
       "             ('active.small_pool.freed', 0),\n",
       "             ('active.small_pool.peak', 0),\n",
       "             ('active_bytes.all.allocated', 300000256),\n",
       "             ('active_bytes.all.current', 300000256),\n",
       "             ('active_bytes.all.freed', 0),\n",
       "             ('active_bytes.all.peak', 300000256),\n",
       "             ('active_bytes.large_pool.allocated', 300000256),\n",
       "             ('active_bytes.large_pool.current', 300000256),\n",
       "             ('active_bytes.large_pool.freed', 0),\n",
       "             ('active_bytes.large_pool.peak', 300000256),\n",
       "             ('active_bytes.small_pool.allocated', 0),\n",
       "             ('active_bytes.small_pool.current', 0),\n",
       "             ('active_bytes.small_pool.freed', 0),\n",
       "             ('active_bytes.small_pool.peak', 0),\n",
       "             ('allocated_bytes.all.allocated', 300000256),\n",
       "             ('allocated_bytes.all.current', 300000256),\n",
       "             ('allocated_bytes.all.freed', 0),\n",
       "             ('allocated_bytes.all.peak', 300000256),\n",
       "             ('allocated_bytes.large_pool.allocated', 300000256),\n",
       "             ('allocated_bytes.large_pool.current', 300000256),\n",
       "             ('allocated_bytes.large_pool.freed', 0),\n",
       "             ('allocated_bytes.large_pool.peak', 300000256),\n",
       "             ('allocated_bytes.small_pool.allocated', 0),\n",
       "             ('allocated_bytes.small_pool.current', 0),\n",
       "             ('allocated_bytes.small_pool.freed', 0),\n",
       "             ('allocated_bytes.small_pool.peak', 0),\n",
       "             ('allocation.all.allocated', 1),\n",
       "             ('allocation.all.current', 1),\n",
       "             ('allocation.all.freed', 0),\n",
       "             ('allocation.all.peak', 1),\n",
       "             ('allocation.large_pool.allocated', 1),\n",
       "             ('allocation.large_pool.current', 1),\n",
       "             ('allocation.large_pool.freed', 0),\n",
       "             ('allocation.large_pool.peak', 1),\n",
       "             ('allocation.small_pool.allocated', 0),\n",
       "             ('allocation.small_pool.current', 0),\n",
       "             ('allocation.small_pool.freed', 0),\n",
       "             ('allocation.small_pool.peak', 0),\n",
       "             ('inactive_split.all.allocated', 1),\n",
       "             ('inactive_split.all.current', 1),\n",
       "             ('inactive_split.all.freed', 0),\n",
       "             ('inactive_split.all.peak', 1),\n",
       "             ('inactive_split.large_pool.allocated', 1),\n",
       "             ('inactive_split.large_pool.current', 1),\n",
       "             ('inactive_split.large_pool.freed', 0),\n",
       "             ('inactive_split.large_pool.peak', 1),\n",
       "             ('inactive_split.small_pool.allocated', 0),\n",
       "             ('inactive_split.small_pool.current', 0),\n",
       "             ('inactive_split.small_pool.freed', 0),\n",
       "             ('inactive_split.small_pool.peak', 0),\n",
       "             ('inactive_split_bytes.all.allocated', 1989632),\n",
       "             ('inactive_split_bytes.all.current', 1989632),\n",
       "             ('inactive_split_bytes.all.freed', 0),\n",
       "             ('inactive_split_bytes.all.peak', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.allocated', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.current', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.freed', 0),\n",
       "             ('inactive_split_bytes.large_pool.peak', 1989632),\n",
       "             ('inactive_split_bytes.small_pool.allocated', 0),\n",
       "             ('inactive_split_bytes.small_pool.current', 0),\n",
       "             ('inactive_split_bytes.small_pool.freed', 0),\n",
       "             ('inactive_split_bytes.small_pool.peak', 0),\n",
       "             ('num_alloc_retries', 0),\n",
       "             ('num_ooms', 0),\n",
       "             ('reserved_bytes.all.allocated', 301989888),\n",
       "             ('reserved_bytes.all.current', 301989888),\n",
       "             ('reserved_bytes.all.freed', 0),\n",
       "             ('reserved_bytes.all.peak', 301989888),\n",
       "             ('reserved_bytes.large_pool.allocated', 301989888),\n",
       "             ('reserved_bytes.large_pool.current', 301989888),\n",
       "             ('reserved_bytes.large_pool.freed', 0),\n",
       "             ('reserved_bytes.large_pool.peak', 301989888),\n",
       "             ('reserved_bytes.small_pool.allocated', 0),\n",
       "             ('reserved_bytes.small_pool.current', 0),\n",
       "             ('reserved_bytes.small_pool.freed', 0),\n",
       "             ('reserved_bytes.small_pool.peak', 0),\n",
       "             ('segment.all.allocated', 1),\n",
       "             ('segment.all.current', 1),\n",
       "             ('segment.all.freed', 0),\n",
       "             ('segment.all.peak', 1),\n",
       "             ('segment.large_pool.allocated', 1),\n",
       "             ('segment.large_pool.current', 1),\n",
       "             ('segment.large_pool.freed', 0),\n",
       "             ('segment.large_pool.peak', 1),\n",
       "             ('segment.small_pool.allocated', 0),\n",
       "             ('segment.small_pool.current', 0),\n",
       "             ('segment.small_pool.freed', 0),\n",
       "             ('segment.small_pool.peak', 0)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device 1 -- result same as before\n",
    "torch.cuda.memory_stats(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |  292969 KB |  292969 KB |  292969 KB |\n",
      "|       from large pool |       0 B  |  292969 KB |  292969 KB |  292969 KB |\n",
      "|       from small pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |  292969 KB |  292969 KB |  292969 KB |\n",
      "|       from large pool |       0 B  |  292969 KB |  292969 KB |  292969 KB |\n",
      "|       from small pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  294912 KB |  294912 KB |  294912 KB |       0 B  |\n",
      "|       from large pool |  294912 KB |  294912 KB |  294912 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |    1943 KB |    1943 KB |    1943 KB |\n",
      "|       from large pool |       0 B  |    1943 KB |    1943 KB |    1943 KB |\n",
      "|       from small pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       1    |       1    |       1    |\n",
      "|       from large pool |       0    |       1    |       1    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       1    |       1    |       1    |\n",
      "|       from large pool |       0    |       1    |       1    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       1    |       1    |       1    |\n",
      "|       from large pool |       0    |       1    |       1    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# noticed that Cur Usage, Active memory, Non-releasable memory all dropped to zero, moved to Tot Freed\n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from large pool |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from large pool |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  294912 KB |  294912 KB |  294912 KB |       0 B  |\n",
      "|       from large pool |  294912 KB |  294912 KB |  294912 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    1943 KB |    1943 KB |    1943 KB |       0 B  |\n",
      "|       from large pool |    1943 KB |    1943 KB |    1943 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# device 1 -- same as before\n",
    "print(torch.cuda.memory_summary(cuda1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Empty cache of device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# device 0\n",
    "# allocated is zero because x is deleted from device 0\n",
    "# reserved is zero because we cleared the cache\n",
    "print_cuda_memory_allocation_for_device(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000256\n",
      "301989888\n"
     ]
    }
   ],
   "source": [
    "# device 1 - same as before, tensor y is still on device 1\n",
    "print_cuda_memory_allocation_for_device(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 03:35:41 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   61C    P0    58W / 149W |    254MiB / 11441MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    72W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    103067      C   ...nvs/playground/bin/python      251MiB |\n",
      "|    1   N/A  N/A    103067      C   ...nvs/playground/bin/python      539MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# memory on device 0 is not completely zero but should be fine, those 254MiB can be reoccupied\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('active.all.allocated', 1),\n",
       "             ('active.all.current', 0),\n",
       "             ('active.all.freed', 1),\n",
       "             ('active.all.peak', 1),\n",
       "             ('active.large_pool.allocated', 1),\n",
       "             ('active.large_pool.current', 0),\n",
       "             ('active.large_pool.freed', 1),\n",
       "             ('active.large_pool.peak', 1),\n",
       "             ('active.small_pool.allocated', 0),\n",
       "             ('active.small_pool.current', 0),\n",
       "             ('active.small_pool.freed', 0),\n",
       "             ('active.small_pool.peak', 0),\n",
       "             ('active_bytes.all.allocated', 300000256),\n",
       "             ('active_bytes.all.current', 0),\n",
       "             ('active_bytes.all.freed', 300000256),\n",
       "             ('active_bytes.all.peak', 300000256),\n",
       "             ('active_bytes.large_pool.allocated', 300000256),\n",
       "             ('active_bytes.large_pool.current', 0),\n",
       "             ('active_bytes.large_pool.freed', 300000256),\n",
       "             ('active_bytes.large_pool.peak', 300000256),\n",
       "             ('active_bytes.small_pool.allocated', 0),\n",
       "             ('active_bytes.small_pool.current', 0),\n",
       "             ('active_bytes.small_pool.freed', 0),\n",
       "             ('active_bytes.small_pool.peak', 0),\n",
       "             ('allocated_bytes.all.allocated', 300000256),\n",
       "             ('allocated_bytes.all.current', 0),\n",
       "             ('allocated_bytes.all.freed', 300000256),\n",
       "             ('allocated_bytes.all.peak', 300000256),\n",
       "             ('allocated_bytes.large_pool.allocated', 300000256),\n",
       "             ('allocated_bytes.large_pool.current', 0),\n",
       "             ('allocated_bytes.large_pool.freed', 300000256),\n",
       "             ('allocated_bytes.large_pool.peak', 300000256),\n",
       "             ('allocated_bytes.small_pool.allocated', 0),\n",
       "             ('allocated_bytes.small_pool.current', 0),\n",
       "             ('allocated_bytes.small_pool.freed', 0),\n",
       "             ('allocated_bytes.small_pool.peak', 0),\n",
       "             ('allocation.all.allocated', 1),\n",
       "             ('allocation.all.current', 0),\n",
       "             ('allocation.all.freed', 1),\n",
       "             ('allocation.all.peak', 1),\n",
       "             ('allocation.large_pool.allocated', 1),\n",
       "             ('allocation.large_pool.current', 0),\n",
       "             ('allocation.large_pool.freed', 1),\n",
       "             ('allocation.large_pool.peak', 1),\n",
       "             ('allocation.small_pool.allocated', 0),\n",
       "             ('allocation.small_pool.current', 0),\n",
       "             ('allocation.small_pool.freed', 0),\n",
       "             ('allocation.small_pool.peak', 0),\n",
       "             ('inactive_split.all.allocated', 1),\n",
       "             ('inactive_split.all.current', 0),\n",
       "             ('inactive_split.all.freed', 1),\n",
       "             ('inactive_split.all.peak', 1),\n",
       "             ('inactive_split.large_pool.allocated', 1),\n",
       "             ('inactive_split.large_pool.current', 0),\n",
       "             ('inactive_split.large_pool.freed', 1),\n",
       "             ('inactive_split.large_pool.peak', 1),\n",
       "             ('inactive_split.small_pool.allocated', 0),\n",
       "             ('inactive_split.small_pool.current', 0),\n",
       "             ('inactive_split.small_pool.freed', 0),\n",
       "             ('inactive_split.small_pool.peak', 0),\n",
       "             ('inactive_split_bytes.all.allocated', 1989632),\n",
       "             ('inactive_split_bytes.all.current', 0),\n",
       "             ('inactive_split_bytes.all.freed', 1989632),\n",
       "             ('inactive_split_bytes.all.peak', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.allocated', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.current', 0),\n",
       "             ('inactive_split_bytes.large_pool.freed', 1989632),\n",
       "             ('inactive_split_bytes.large_pool.peak', 1989632),\n",
       "             ('inactive_split_bytes.small_pool.allocated', 0),\n",
       "             ('inactive_split_bytes.small_pool.current', 0),\n",
       "             ('inactive_split_bytes.small_pool.freed', 0),\n",
       "             ('inactive_split_bytes.small_pool.peak', 0),\n",
       "             ('num_alloc_retries', 0),\n",
       "             ('num_ooms', 0),\n",
       "             ('reserved_bytes.all.allocated', 301989888),\n",
       "             ('reserved_bytes.all.current', 0),\n",
       "             ('reserved_bytes.all.freed', 301989888),\n",
       "             ('reserved_bytes.all.peak', 301989888),\n",
       "             ('reserved_bytes.large_pool.allocated', 301989888),\n",
       "             ('reserved_bytes.large_pool.current', 0),\n",
       "             ('reserved_bytes.large_pool.freed', 301989888),\n",
       "             ('reserved_bytes.large_pool.peak', 301989888),\n",
       "             ('reserved_bytes.small_pool.allocated', 0),\n",
       "             ('reserved_bytes.small_pool.current', 0),\n",
       "             ('reserved_bytes.small_pool.freed', 0),\n",
       "             ('reserved_bytes.small_pool.peak', 0),\n",
       "             ('segment.all.allocated', 1),\n",
       "             ('segment.all.current', 0),\n",
       "             ('segment.all.freed', 1),\n",
       "             ('segment.all.peak', 1),\n",
       "             ('segment.large_pool.allocated', 1),\n",
       "             ('segment.large_pool.current', 0),\n",
       "             ('segment.large_pool.freed', 1),\n",
       "             ('segment.large_pool.peak', 1),\n",
       "             ('segment.small_pool.allocated', 0),\n",
       "             ('segment.small_pool.current', 0),\n",
       "             ('segment.small_pool.freed', 0),\n",
       "             ('segment.small_pool.peak', 0)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device 0 \n",
    "torch.cuda.memory_stats(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |  292969 KB |  292969 KB |  292969 KB |\n",
      "|       from large pool |       0 B  |  292969 KB |  292969 KB |  292969 KB |\n",
      "|       from small pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |  292969 KB |  292969 KB |  292969 KB |\n",
      "|       from large pool |       0 B  |  292969 KB |  292969 KB |  292969 KB |\n",
      "|       from small pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |  294912 KB |  294912 KB |  294912 KB |\n",
      "|       from large pool |       0 B  |  294912 KB |  294912 KB |  294912 KB |\n",
      "|       from small pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |    1943 KB |    1943 KB |    1943 KB |\n",
      "|       from large pool |       0 B  |    1943 KB |    1943 KB |    1943 KB |\n",
      "|       from small pool |       0 B  |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       1    |       1    |       1    |\n",
      "|       from large pool |       0    |       1    |       1    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       1    |       1    |       1    |\n",
      "|       from large pool |       0    |       1    |       1    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       1    |       1    |       1    |\n",
      "|       from large pool |       0    |       1    |       1    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       1    |       1    |       1    |\n",
      "|       from large pool |       0    |       1    |       1    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notice that GPU reserved memory is cleared after we empty the cache\n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Put new tensor on device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(300000000, dtype=torch.int8).cuda(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000256\n",
      "301989888\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_allocation_for_device(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000256\n",
      "301989888\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_allocation_for_device(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 03:36:55 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   61C    P0    58W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    71W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    103067      C   ...nvs/playground/bin/python      539MiB |\n",
      "|    1   N/A  N/A    103067      C   ...nvs/playground/bin/python      539MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# memory on device 0 is 542MiB again. It looks like those 254MiB is reoccupied again\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  292969 KB |  292969 KB |  585938 KB |  292969 KB |\n",
      "|       from large pool |  292969 KB |  292969 KB |  585938 KB |  292969 KB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  292969 KB |  292969 KB |  585938 KB |  292969 KB |\n",
      "|       from large pool |  292969 KB |  292969 KB |  585938 KB |  292969 KB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  294912 KB |  294912 KB |  589824 KB |  294912 KB |\n",
      "|       from large pool |  294912 KB |  294912 KB |  589824 KB |  294912 KB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    1943 KB |    1943 KB |    3886 KB |    1943 KB |\n",
      "|       from large pool |    1943 KB |    1943 KB |    3886 KB |    1943 KB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       1    |       1    |       2    |       1    |\n",
      "|       from large pool |       1    |       1    |       2    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       1    |       1    |       2    |       1    |\n",
      "|       from large pool |       1    |       1    |       2    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       2    |       1    |\n",
      "|       from large pool |       1    |       1    |       2    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |       1    |       2    |       1    |\n",
      "|       from large pool |       1    |       1    |       2    |       1    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same stats as we first put tensor x on device 0\n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What if I don't empty cache? (Restart Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0') # same as cuda0 = torch.device('cuda:0') or cuda = torch.device('cuda')\n",
    "cuda1 = torch.device('cuda:1') # same as cuda1 = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(300000000, dtype=torch.int8).cuda(cuda0)\n",
    "y = torch.zeros(300000000, dtype=torch.int8).cuda(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 03:46:22 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   60C    P0    58W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    71W / 149W |    542MiB / 11441MiB |     30%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    103598      C   ...nvs/playground/bin/python      539MiB |\n",
      "|    1   N/A  N/A    103598      C   ...nvs/playground/bin/python      539MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# show up as 542MiB\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from large pool |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from large pool |  292969 KB |  292969 KB |  292969 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  294912 KB |  294912 KB |  294912 KB |       0 B  |\n",
      "|       from large pool |  294912 KB |  294912 KB |  294912 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    1943 KB |    1943 KB |    1943 KB |       0 B  |\n",
      "|       from large pool |    1943 KB |    1943 KB |    1943 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |       1    |       1    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same stats as we first put tensor x on device 0\n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x = torch.zeros(300000000, dtype=torch.int8).cuda(cuda0)\n",
    "    y = torch.zeros(300000000, dtype=torch.int8).cuda(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 03:46:46 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   61C    P0    58W / 149W |    830MiB / 11441MiB |     29%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    71W / 149W |    830MiB / 11441MiB |     49%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    103598      C   ...nvs/playground/bin/python      827MiB |\n",
      "|    1   N/A  N/A    103598      C   ...nvs/playground/bin/python      827MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# increased to 830MiB\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  292969 KB |  585938 KB |    3147 MB |    2861 MB |\n",
      "|       from large pool |  292969 KB |  585938 KB |    3147 MB |    2861 MB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  292969 KB |  585938 KB |    3147 MB |    2861 MB |\n",
      "|       from large pool |  292969 KB |  585938 KB |    3147 MB |    2861 MB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  589824 KB |  589824 KB |  589824 KB |       0 B  |\n",
      "|       from large pool |  589824 KB |  589824 KB |  589824 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    1943 KB |    3886 KB |   21373 KB |   19430 KB |\n",
      "|       from large pool |    1943 KB |    3886 KB |   21373 KB |   19430 KB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       1    |       2    |      11    |      10    |\n",
      "|       from large pool |       1    |       2    |      11    |      10    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       1    |       2    |      11    |      10    |\n",
      "|       from large pool |       1    |       2    |      11    |      10    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       2    |       2    |       2    |       0    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |       2    |      11    |      10    |\n",
      "|       from large pool |       1    |       2    |      11    |      10    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same stats as we first put tensor x on device 0\n",
    "# Total freed is 2861MB\n",
    "# each tensor is about 300MB\n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292.969"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KB in MB\n",
    "(292969 / 1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    x = torch.zeros(300000000, dtype=torch.int8).cuda(cuda0)\n",
    "    y = torch.zeros(300000000, dtype=torch.int8).cuda(cuda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 03:54:22 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   62C    P0    58W / 149W |    830MiB / 11441MiB |     29%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    71W / 149W |    830MiB / 11441MiB |     49%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    103598      C   ...nvs/playground/bin/python      827MiB |\n",
      "|    1   N/A  N/A    103598      C   ...nvs/playground/bin/python      827MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# remains at 830MiB\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  292969 KB |  585938 KB |   11730 MB |   11444 MB |\n",
      "|       from large pool |  292969 KB |  585938 KB |   11730 MB |   11444 MB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  292969 KB |  585938 KB |   11730 MB |   11444 MB |\n",
      "|       from large pool |  292969 KB |  585938 KB |   11730 MB |   11444 MB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  589824 KB |  589824 KB |  589824 KB |       0 B  |\n",
      "|       from large pool |  589824 KB |  589824 KB |  589824 KB |       0 B  |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    1943 KB |    3886 KB |   79663 KB |   77720 KB |\n",
      "|       from large pool |    1943 KB |    3886 KB |   79663 KB |   77720 KB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       1    |       2    |      41    |      40    |\n",
      "|       from large pool |       1    |       2    |      41    |      40    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       1    |       2    |      41    |      40    |\n",
      "|       from large pool |       1    |       2    |      41    |      40    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       2    |       2    |       2    |       0    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |       2    |      41    |      40    |\n",
      "|       from large pool |       1    |       2    |      41    |      40    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same stats as we first put tensor x on device 0\n",
    "# Total freed is 2861MB\n",
    "# each tensor is about 300MB\n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    x = torch.zeros(300000000, dtype=torch.int8).cuda(cuda0)\n",
    "    y = torch.zeros(300000000, dtype=torch.int8).cuda(cuda1)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 26 03:55:48 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   62C    P0    58W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   47C    P0    71W / 149W |    542MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    103598      C   ...nvs/playground/bin/python      539MiB |\n",
      "|    1   N/A  N/A    103598      C   ...nvs/playground/bin/python      539MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# remains at 830MiB\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  292969 KB |  585938 KB |   20599 MB |   20313 MB |\n",
      "|       from large pool |  292969 KB |  585938 KB |   20599 MB |   20313 MB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  292969 KB |  585938 KB |   20599 MB |   20313 MB |\n",
      "|       from large pool |  292969 KB |  585938 KB |   20599 MB |   20313 MB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  294912 KB |  589824 KB |    8928 MB |    8640 MB |\n",
      "|       from large pool |  294912 KB |  589824 KB |    8928 MB |    8640 MB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    1943 KB |    3886 KB |  139896 KB |  137953 KB |\n",
      "|       from large pool |    1943 KB |    3886 KB |  139896 KB |  137953 KB |\n",
      "|       from small pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       1    |       2    |      72    |      71    |\n",
      "|       from large pool |       1    |       2    |      72    |      71    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       1    |       2    |      72    |      71    |\n",
      "|       from large pool |       1    |       2    |      72    |      71    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       1    |       2    |      31    |      30    |\n",
      "|       from large pool |       1    |       2    |      31    |      30    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |       2    |      72    |      71    |\n",
      "|       from large pool |       1    |       2    |      72    |      71    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same stats as we first put tensor x on device 0\n",
    "# Total freed is 2861MB\n",
    "# each tensor is about 300MB\n",
    "print(torch.cuda.memory_summary(cuda0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
